{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#CODE TO INITIALISE TENSORFLOW\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "#set_session(tf.Session(config=config))\n",
    "print(\"Success!\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for i in range(4):\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[i],\n",
    "        tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024))\n",
    "\n",
    "\n",
    "#Prints the devices in use. The next time I open this, should be configured to only use GPU 3\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version is\n",
      "2.0.0\n",
      "Keras version is\n",
      "2.3.1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9999884668476972254\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16978756131605864287\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14540005492852338272\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4940025772609772804\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6771526126704929419\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:3\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9797640049213285738\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "Tensorflow version is\n",
      "2.0.0\n",
      "Keras version is\n",
      "2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f4dbc36a090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing TF and Keras\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "\n",
    "#FRANK# view versions\n",
    "print(\"Tensorflow version is\")\n",
    "print(tf.__version__)\n",
    "print(\"Keras version is\")\n",
    "print(keras.__version__)\n",
    "\n",
    "#FRANK# these lines limiting memory usages are for tf2, which requires cudatoolkit>10 and newer driver\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#print(gpus)\n",
    "#for i in range(4):   \n",
    "#    tf.config.experimental.set_virtual_device_configuration(gpus[i],\n",
    "#        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5860500000)])\n",
    "\n",
    "#config = tf.ConfigProto(log_device_placement=True)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "#sess = tf.Session(config=config)\n",
    "#set_session(sess)\n",
    "#print(\"Success!\")\n",
    "\n",
    "# Prints the devices in use. \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Tensorflow version is\")\n",
    "print(tf.__version__)\n",
    "print(\"Keras version is\")\n",
    "print(keras.__version__)\n",
    "\n",
    "# Use only GPU3\n",
    "tf.device('/device:XLA_GPU:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Importing Keras \n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "# Import Libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import matplotlib\n",
    "import time\n",
    "import pickle \n",
    "\n",
    "from itertools import groupby\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import pyjet\n",
    "from pyjet import cluster, DTYPE_PTEPM\n",
    "from pyjet.testdata import get_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "# Import local libraries\n",
    "from substructure import * # Jet substructure variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#IMPORTANT NOTE DON'T DELETE\n",
    "#The 1e-3 factor in my weights comes from the fact that Pythia reports cross-sections in millibarns\n",
    "\n",
    "#VARIABLES\n",
    "width = 40\n",
    "height = 40 # width, height of pictures   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     30,
     54,
     93,
     100,
     149,
     162,
     170,
     181,
     246,
     249,
     252,
     255,
     274,
     279,
     286
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reading in Josh's files; background is a single large file (pT cut must be 1 here for Josh's sample)\n",
    "# This read produces event_list (collection of raw vectors) and event images\n",
    "background_event_list,background_mass_list,background_image_list,num_background_files = \\\n",
    "    load_events(\"actual\", max_Files=1,path=\"/data1/users/jzlin/MLM/background_7413/\",\\\n",
    "                contains=\"_actual\",pt_cut=1, width, height)\n",
    "num_background_files = 15693\n",
    "signal_event_list,signal_mass_list,signal_image_list,num_signal_files = \\\n",
    "    load_events(\"actual\", max_Read = len(background_event_list),path=\"/data1/users/jzlin/MLM/heavy_signal/\",\\\n",
    "                contains=\"_signal\",pt_cut=1, width, height)\n",
    "\n",
    "# Check size of dataset\n",
    "print(len(background_mass_list),len(signal_mass_list))\n",
    "\n",
    "# Zero centering and normalizing\n",
    "background_image_list, signal_image_list = zero_center_and_normalize(background_image_list,signal_image_list)\n",
    "#background_mass_window = np.logical_and(np.array(background_mass_list) > 115,np.array(background_mass_list) < 135)\n",
    "\n",
    "# Cluster events_lists into jets. The results are named background/signal_event_list_clustered\n",
    "background_event_list_clustered = cluster_event(background_event_list)\n",
    "signal_event_list_clustered = cluster_event(signal_event_list)\n",
    "\n",
    "# Reclustering the events (i.e. clustering within events)\n",
    "background_reclustered = recluster_event(background_event_list_clustered)\n",
    "signal_reclustered = recluster_event(signal_event_list_clustered)\n",
    "\n",
    "# Produce jet images, the zero-center and normalize\n",
    "background_recluster_images = return_fine_image_list_reclustered(background_event_list,\n",
    "                                                           background_reclustered,0.8, width, height)\n",
    "signal_recluster_images = return_fine_image_list_reclustered(signal_event_list,\n",
    "                                                           signal_reclustered,0.8, width, height)\n",
    "\n",
    "background_recluster_images, signal_recluster_images = zero_center_and_normalize(background_recluster_images, signal_recluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight calculation for Josh's sample\n",
    "backgroundCross = 2.048e-06 # Cross-section of processes in millibarns, NOT USED\n",
    "\n",
    "actual_background_cross=2.84e-9 # In barns, used in background weight\n",
    "average_number_accepted=2162 # Used in background weight\n",
    "\n",
    "actual_signal_cross = np.average([1.738e-14,1.7277e-14]) # Used in signal weight\n",
    "signal_accepted = np.average([8708-189,8827-172]) # Used in signal weight \n",
    "\n",
    "background_weight = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "signal_weight = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(background_recluster_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# Function that splits whole event dataset into test, train and evaluation sets.\n",
    "# the *_cut outputs has a 115<mass<135 cut.\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "rsplit = np.array([0.5,0.75])\n",
    "\n",
    "def splitData(background_image_list,signal_image_list):\n",
    "    try:\n",
    "        input_shape = background_image_list[0].shape\n",
    "    except AttributeError:\n",
    "        input_shape = (0)\n",
    "        \n",
    "    b_split = np.split(background_image_list,(len(background_image_list)*rsplit).astype(int))\n",
    "    s_split = np.split(signal_image_list,(len(signal_image_list)*rsplit).astype(int))\n",
    "    bm_split = np.split(background_mass_list,(len(background_mass_list)*rsplit).astype(int))\n",
    "    sm_split = np.split(signal_mass_list,(len(signal_mass_list)*rsplit).astype(int))\n",
    "    \n",
    "    x_train = np.concatenate((b_split[0],s_split[0]))\n",
    "    y_train = np.array(np.concatenate((np.zeros(len(b_split[0])),np.ones(len(s_split[0])))))\n",
    "    mass_train = np.concatenate((bm_split[0],sm_split[0]))\n",
    "    \n",
    "    x_val = np.concatenate((b_split[1],s_split[1]))\n",
    "    y_val = np.array(np.concatenate((np.zeros(len(b_split[1])),np.ones(len(s_split[1])))))\n",
    "    mass_val = np.concatenate((bm_split[1],sm_split[1]))\n",
    "    \n",
    "    x_test = np.concatenate((b_split[2],s_split[2]))\n",
    "    y_test = np.array(np.concatenate((np.zeros(len(b_split[2])),np.ones(len(s_split[2])))))\n",
    "    mass_test = np.concatenate((bm_split[2],sm_split[2]))\n",
    "        \n",
    "    \n",
    "    x_train_cut = [] #Note that we *train* on this cut sample, but *test* on the whole thing I guess\n",
    "    y_train_cut = []\n",
    "    mass_train_cut = []\n",
    "    for x in reversed(range(len(x_train))):\n",
    "        if mass_train[x] < 135 and mass_train[x] > 115:\n",
    "            x_train_cut.append(x_train[x])\n",
    "            y_train_cut.append(y_train[x])\n",
    "            mass_train_cut.append(mass_train[x])\n",
    "    x_train_cut = np.array(x_train_cut)\n",
    "    y_train_cut = np.array(y_train_cut)\n",
    "    mass_train_cut = np.array(mass_train_cut)\n",
    "    \n",
    "    x_val_cut = [] \n",
    "    y_val_cut = []\n",
    "    mass_val_cut = []\n",
    "    for x in reversed(range(len(x_val))):\n",
    "        if mass_val[x] < 135 and mass_val[x] > 115:\n",
    "            x_val_cut.append(x_val[x])\n",
    "            y_val_cut.append(y_val[x])\n",
    "            mass_val_cut.append(mass_val[x])\n",
    "    x_val_cut = np.array(x_val_cut)\n",
    "    y_val_cut = np.array(y_val_cut)\n",
    "    mass_val_cut = np.array(mass_val_cut)\n",
    "    \n",
    "    x_test_cut = [] \n",
    "    y_test_cut = []\n",
    "    mass_test_cut = []\n",
    "    for x in reversed(range(len(x_test))):\n",
    "        if mass_test[x] < 135 and mass_test[x] > 115:\n",
    "            x_test_cut.append(x_test[x])\n",
    "            y_test_cut.append(y_test[x])\n",
    "            mass_test_cut.append(mass_test[x])\n",
    "    x_test_cut = np.array(x_test_cut)\n",
    "    y_test_cut = np.array(y_test_cut)\n",
    "    mass_test_cut = np.array(mass_test_cut)\n",
    "    \n",
    "    return(input_shape,\n",
    "           x_train,y_train,mass_train,\n",
    "           x_val,y_val,mass_val,\n",
    "           x_test,y_test,mass_test,\n",
    "           x_train_cut,y_train_cut,mass_train_cut,\n",
    "           x_val_cut,y_val_cut,mass_val_cut,\n",
    "           x_test_cut,y_test_cut,mass_test_cut,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split event images\n",
    "input_shape, \\\n",
    "    x_train,y_train,mass_train, \\\n",
    "    x_val,y_val,mass_val, \\\n",
    "    x_test,y_test,mass_test, \\\n",
    "    x_train_cut,y_train_cut,mass_train_cut, \\\n",
    "    x_val_cut,y_val_cut,mass_val_cut, \\\n",
    "    x_test_cut,y_test_cut,mass_test_cut = splitData(background_image_list,signal_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split jet images\n",
    "input_shape_r, \\\n",
    "    x_train_r,y_train_r,mass_train_r, \\\n",
    "    x_val_r,y_val_r,mass_val_r, \\\n",
    "    x_test_r,y_test_r,mass_test_r, \\\n",
    "    x_train_cut_r,y_train_cut_r,mass_train_cut_r, \\\n",
    "    x_val_cut_r,y_val_cut_r,mass_val_cut_r, \\\n",
    "    x_test_cut_r,y_test_cut_r,mass_test_cut_r = splitData(background_recluster_images,signal_recluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK print an event image\n",
    "print(np.shape(x_train[0]))\n",
    "plt.imshow(x_train_r[-1][0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_r[-1][1])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_r[-1][2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     38,
     49,
     89,
     103,
     117
    ]
   },
   "outputs": [],
   "source": [
    "def generate_real_SIC(expect,predict,masses,quality=1,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten(),masses)).transpose(), key=lambda x: x[1])),0)\n",
    "    efficiency = []; signal_eff = [];\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort),quality*10):\n",
    "        background_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==0],bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==1],bins=np.arange(50,197,7))\n",
    "        log_likelihood = 0; baseLL = 0; \n",
    "        signal_eff.append(np.sum(to_sort[:i+1,0])/total_signal)\n",
    "        test_LL_vs_SS = []\n",
    "        for signal_strength in np.arange(1,5,quality/1000):\n",
    "            log_likelihood = 0;\n",
    "            for k in range(len(bins)-1):\n",
    "                expected = background_weight*background_mass_binned[k]+signal_weight*signal_strength*signal_mass_binned[k]\n",
    "                observed = background_weight*background_mass_binned[k]+signal_weight*signal_mass_binned[k]\n",
    "                if (expected <= 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "            if signal_strength == 1:\n",
    "                baseLL = log_likelihood\n",
    "            test_LL_vs_SS.append(log_likelihood)\n",
    "            if log_likelihood < baseLL-1/2:\n",
    "                efficiency.append(1/(signal_strength-1))\n",
    "                break\n",
    "            if signal_strength > 3:\n",
    "                efficiency.append(1/2)\n",
    "                break\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"base efficiency : \" + str(float(efficiency[-1])))\n",
    "    efficiency = np.array(efficiency)/float(efficiency[-1])\n",
    "    max_eff = np.max(efficiency)\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def find_highest_SIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)): # generate a int range scanning over 95% of all samples??\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    return(max_eff)\n",
    "\n",
    "def find_highest_SIC_binned(expect,predict,masses):\n",
    "    bins = np.arange(50,197,7)\n",
    "    efficiency = []\n",
    "    def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "        log_likelihood = 0\n",
    "        background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "        for i in range(len(bins)-1):\n",
    "            expected = background_weight*background_mass_binned[i]+signal_weight*signal_strength*signal_mass_binned[i]\n",
    "            observed = background_weight*background_mass_binned[i]+signal_weight*signal_mass_binned[i]\n",
    "            if (expected <= 0):\n",
    "                return float(\"inf\")\n",
    "            log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "        return -log_likelihood\n",
    "    \n",
    "    sigmas = []\n",
    "    for i in np.arange(0,0.9,0.05):\n",
    "        #res = least_squares(lambda x : log_like(x,masses[np.logical_and(predict.flatten() >= i,expect == 0)],\n",
    "        #                                        masses[np.logical_and(predict.flatten() >= i,expect == 1)]),x0=1)\n",
    "\n",
    "        #i is the cut on the machine learning\n",
    "        kept_back = masses[np.logical_and(predict.flatten() >= i,expect == 0)]\n",
    "        kept_signal = masses[np.logical_and(predict.flatten() >= i,expect == 1)]\n",
    "        j_array = []\n",
    "        for j in np.arange(1,25,0.5):\n",
    "            #print log_like(j,kept_back,kept_signal)\n",
    "            if log_like(j,kept_back,kept_signal) > log_like(1,kept_back,kept_signal)+0.5:\n",
    "                j_array.append(j-1)\n",
    "                break\n",
    "            if j >20:\n",
    "                j_array.append(20)\n",
    "                break\n",
    "        print(j_array,i)\n",
    "        sigmas.append(1/np.min(j_array))\n",
    "        \n",
    "    max_eff = np.max(sigmas)\n",
    "    return(max_eff)\n",
    "\n",
    "def generateSIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []; signal_eff = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)):\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        signal_eff.append(signal_eff_temp)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "    log_likelihood = 0\n",
    "    background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "    signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weight*background_mass_binned[i]+signal_weight*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weight*background_mass_binned[i]+signal_weight*signal_mass_binned[i]\n",
    "        if (expected <= 0):\n",
    "            return float(\"inf\")\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return -log_likelihood\n",
    "\n",
    "def log_like(signal_strength):\n",
    "    log_likelihood = 0\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weight*background_mass_binned[i]+signal_weight*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weight*background_mass_binned[i]+signal_weight*signal_mass_binned[i]\n",
    "        #print(expected)\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     7,
     10,
     24
    ]
   },
   "outputs": [],
   "source": [
    "#FRANK# Returns a function that generates a padding layer\n",
    "#FRANK# x is the detector image, of following format:\n",
    "#FRANK# [index, image#(charged and neutral pt and multiplicity), 40, 40]\n",
    "def return_pad_me(padding):\n",
    "    def pad_me(x):\n",
    "        #FRANK# x[:,:,:y,:] slice x off from y at the given axis.\n",
    "        return(tf.concat((x,x[:,:,:padding,:]),2))\n",
    "    return(pad_me)\n",
    "\n",
    "def pad_out(padding,input_shape):\n",
    "    return input_shape\n",
    "\n",
    "class gen_call(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, test_data):\n",
    "        self.x, self.y = test_data\n",
    "    \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.highest_SIC_train = []\n",
    "        self.highest_SIC_test = []\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        self.highest_SIC_test.append(find_highest_SIC(self.y,y_pred))\n",
    "        print(str(self.highest_SIC_test[-1]) + \" is how good\")\n",
    "\n",
    "def show_outputs(output):\n",
    "    #Assumes the output is in shape like (32,41,36)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for i in range(1,1+output.shape[0]):\n",
    "        fig.add_subplot(4,output.shape[0]/4,i)\n",
    "        plt.imshow(10*output[i-1,:,:])\n",
    "        plt.axis('off')\n",
    "    #plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# this method is the exact replica of find_highest_SIC() but operates on tensors.\n",
    "#FRANK# for some reason, pred has different format from expect. \n",
    "def highest_SIC_metric(y_true,y_pred):\n",
    "    print('highest_SIC_metric() is called')\n",
    "    y_true = tf.keras.backend.flatten(y_true) #FRANK# flattens to 1D\n",
    "    y_pred = tf.keras.backend.flatten(y_pred) #FRANK# flattens to 1D\n",
    "    \n",
    "    #stacked = tf.transpose(tf.stack((expect,predict)))\n",
    "    #to_sort = tf.reverse(sorted(stacked, key=lambda x: x[1]),0) #FRANK# sorted is wrong!!\n",
    "    total_sample = tf.cast(tf.size(y_pred), tf.float32)  # count total nums, then cast to float32 to avoid issue\n",
    "    total_signal = tf.cast(tf.reduce_sum(y_true), tf.float32)  # summing across all predicted (where sigs are 1's and bkgs are 0's) to get total num of signal. \n",
    "    total_background = tf.cast(tf.subtract(total_sample, total_signal), tf.float32)  # subtracting signal countss from total size to get background counts\n",
    "    \n",
    "    \n",
    "    # original mechanism:\n",
    "    # 1. sort by ML score\n",
    "    # 2. sum all actuals before an index\n",
    "    # 3. count all 0's before an index \n",
    "    \n",
    "    sorted_indices = tf.argsort(y_pred,axis=-1,direction='ASCENDING') # tf.argsort: Returns the indices of a tensor that give its sorted order along an axis.\n",
    "    sorted_sigs = tf.gather(y_pred,sorted_indices)\n",
    "    # return the indices of prediction in ascending order. By reading these indices, you can access corresponding expected y's.\n",
    "    ones = tf.fill(tf.shape(sorted_sigs), 1.0)\n",
    "    sorted_bkgs = tf.subtract(ones, sorted_sigs)\n",
    "    # make a sorted tensor where 1's are bkgs and 0's are sigs\n",
    "    \n",
    "    sig_cum_sums = tf.cast(tf.cumsum(sorted_sigs), tf.float32) # return the integrated signal number from 0 to each index\n",
    "    bkg_cum_sums = tf.cast(tf.cumsum(sorted_bkgs), tf.float32) # return the integrated bkg number from 0 to each index\n",
    "\n",
    "    sig_effs = tf.divide(sig_cum_sums, total_signal)\n",
    "    bkg_effs = tf.divide(bkg_cum_sums, total_background) #FRANK# total_background might be 0, causing bkg_effs=inf\n",
    "    effs = tf.divide(sig_effs, tf.sqrt(bkg_effs))\n",
    "        \n",
    "    return tf.reduce_max(effs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "# Event image model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "event_image_cnn = Sequential()\n",
    "event_image_cnn.add(Lambda(return_pad_me(4),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first'))\n",
    "event_image_cnn.add(Lambda(return_pad_me(1),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "event_image_cnn.add(Lambda(return_pad_me(4),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(Conv2D(64, (5, 5), \n",
    "                 activation='relu',\n",
    "                 data_format='channels_first'))\n",
    "event_image_cnn.add(Lambda(return_pad_me(1),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "event_image_cnn.add(Flatten())\n",
    "event_image_cnn.add(Dense(1000, activation='relu'))\n",
    "event_image_cnn.add(Dense(1, activation='sigmoid'))\n",
    "event_image_cnn.summary()\n",
    "\n",
    "#model_opt = keras.optimizers.Adadelta(lr=2.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model_opt = keras.optimizers.Adadelta() #FRANK# decrease learning rate?\n",
    "\n",
    "event_image_cnn.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=model_opt,\n",
    "              metrics=['accuracy', highest_SIC_metric])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Jet image model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "jet_image_cnn = Sequential()\n",
    "jet_image_cnn.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first',input_shape=input_shape))\n",
    "#jet_image_cnn.add(Dropout(0.5))\n",
    "jet_image_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "jet_image_cnn.add(Conv2D(64, (5, 5), activation='relu',data_format='channels_first'))\n",
    "#jet_image_cnn.add(Dropout(0.5))\n",
    "jet_image_cnn.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "jet_image_cnn.add(Flatten())\n",
    "jet_image_cnn.add(Dense(1000, activation='relu'))\n",
    "#jet_image_cnn.add(Dropout(0.2))\n",
    "jet_image_cnn.add(Dense(1, activation='sigmoid'))\n",
    "#jet_image_cnn.summary()\n",
    "\n",
    "model_opt = keras.optimizers.Adadelta()\n",
    "\n",
    "jet_image_cnn.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=model_opt,\n",
    "              metrics=['accuracy', 'highest_SIC_test'])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Combined model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#FRANK# input shape is calculated during splitting:\n",
    "#FRANK# splitData(background_image_list,signal_image_list)\n",
    "#FRANK# how is jet image taken?\n",
    "event_image_branch = Sequential()\n",
    "\n",
    "#FRANK# Lambda is from Keras.\n",
    "event_image_branch.add(Lambda(return_pad_me(5), #FRANK# tf.concat((x,x[:,:,:5,:]),2). Concatenate means pasting together.\n",
    "                            #FRANK# this layer slices off an edge of the tensor and paste it on the other side.\n",
    "                 input_shape=input_shape))\n",
    "event_image_branch.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first'))\n",
    "event_image_branch.add(Lambda(return_pad_me(2)))\n",
    "event_image_branch.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "event_image_branch.add(Lambda(return_pad_me(5),\n",
    "                 input_shape=input_shape))\n",
    "event_image_branch.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#event_image_branch.add(Dropout(0.5))\n",
    "event_image_branch.add(Lambda(return_pad_me(2)))\n",
    "event_image_branch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "event_image_branch.add(Flatten())\n",
    "event_image_branch.add(Dense(300,activation='relu'))\n",
    "#event_image_branch.add(Dropout(0.5))\n",
    "\n",
    "jet_image_branch = Sequential()\n",
    "jet_image_branch.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                      kernel_initializer='random_uniform',input_shape=input_shape_r, data_format=\"channels_first\")) #FRANK# problemmatic. see debug note #1\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "jet_image_branch.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "jet_image_branch.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "jet_image_branch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "jet_image_branch.add(Flatten())\n",
    "jet_image_branch.add(Dense(300, activation='relu'))\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "#FRANK# Merge layer is no longer supported.  \n",
    "#combined_model = Sequential()\n",
    "#combined_model.add(Merge([event_image_branch, jet_image_branch], mode = 'concat'))\n",
    "#combined_model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#FRANK# these are not models, but tensors. Model.input or Model.output are tensor pointers I think.\n",
    "combined_model_tensor = Concatenate(axis=-1)([event_image_branch.output, jet_image_branch.output])   \n",
    "combined_model_tensor = Dense(1,activation='sigmoid')(combined_model_tensor)\n",
    "#FRANK# what you need to do is merging output of 300 and 300 into one.\n",
    "\n",
    "#combined_model.summary()\n",
    "combined_model = Model([event_image_branch.input,jet_image_branch.input], combined_model_tensor)\n",
    "combined_model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', highest_SIC_metric])\n",
    "\n",
    "# Visualizing event image model\n",
    "plot_model(combined_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#FRANK# Sample training call testing metric\n",
    "epochs = 100\n",
    "print(y_train)\n",
    "with tf.device('/device:XLA_GPU:3'):\n",
    "    history_event_only = event_image_cnn.fit(x_train, y_train,\n",
    "        batch_size=1024,#batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val,y_val),\n",
    "        verbose=1, shuffle=True, callbacks=[EarlyStopping(monitor='highest_SIC_test', patience=2), gen_call((x_val_cut,y_val_cut))])\n",
    "\n",
    "#FRANK# for event image SIC: predicting\n",
    "with tf.device('/device:XLA_GPU:3'):\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    \n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# testing metric\n",
    "print(np.array(y_pred.flatten()))\n",
    "print(np.array(y_test))\n",
    "print(highest_SIC_metric(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# training history plots\n",
    "def loss_history_plot(training_history):\n",
    "    print(training_history.history.keys())\n",
    "    plt.plot(training_history.history['loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print groomed mass histogram\n",
    "plt.rcParams['figure.figsize'] = [7.5,6]\n",
    "\n",
    "hist_bkg, bin_edges_bkg = np.histogram(background_mass_list, bins=21, range = (50, 197))\n",
    "bin_centers_bkg = (bin_edges_bkg[:-1] + bin_edges_bkg[1:]) / 2\n",
    "hist_sig, bin_edges_bkg = np.histogram(signal_mass_list, bins=21, range = (50, 197))\n",
    "\n",
    "plt.step(bin_centers_bkg, hist_bkg*background_weight, label='bkgd')\n",
    "plt.step(bin_centers_bkg, hist_sig*signal_weight, label='sig')\n",
    "plt.legend()\n",
    "plt.ylim(top=10000)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(right=197)  # adjust the right leaving left unchanged\n",
    "plt.xlim(left=50) \n",
    "plt.show()\n",
    "#log_like(1,background_mass_list=background_mass_list,signal_mass_list=signal_mass_list)\n",
    "\n",
    "# Calculating beta3\n",
    "background_beta3 = find_new_var_beta_3(background_reclustered,background_event_list_clustered,pt_cut = 1)\n",
    "signal_beta3 = find_new_var_beta_3(signal_reclustered,signal_event_list_clustered,pt_cut = 1)\n",
    "\n",
    "# Beta-3 histogram\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 4]\n",
    "beta3_min = min([min(background_beta3),min(signal_beta3)])\n",
    "beta3_max = max([max(background_beta3),max(signal_beta3)])\n",
    "print('beta3 range is '+str((beta3_min, beta3_max)))\n",
    "plt.hist(background_beta3, label='bkgd', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_beta3, label='sig', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Beta-3\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(background_mass_list, label='bkgd', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_mass_list, label='sig', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Trimmed Mass\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Beta-3 plots\n",
    "plot_simple_ROC_SIC(signal_beta3, background_beta3, step = 0.001):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the predicted results into sample and background using known answers.\n",
    "# Here we assume all backgrounds are put before signals in all datasets. \n",
    "#TODO# Check before use!\n",
    "\n",
    "def split_sig_bkgd(predicted, expected)\n",
    "    split_location = int(np.argwhere(expected==1.)[0])\n",
    "    #FRANK# signal predictions with event image model\n",
    "    sig = predicted[split_location:]\n",
    "    bkgd = predicted[:split_location]\n",
    "\n",
    "    return sig, bkgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# calculating true positive rates and false positive rates. \n",
    "#FRANK# TPR = TP/P\n",
    "#FRANK# FPR = FP/N\n",
    "#FRANK# note that bkgds have lower beta in general, so the cut should be \n",
    "#FRANK# beta3 > threshold.\n",
    "#FRANK# 'where' usage referred from \n",
    "#FRANK# https://stackoverflow.com/questions/12995937/count-all-values-in-a-matrix-greater-than-a-value\n",
    "\n",
    "def simple_ROC(sig, bkgd, step = 1):\n",
    "    thres_min = min([min(sig),min(bkgd)])\n",
    "    thres_max = max([max(sig),max(bkgd)])\n",
    "    search_range=(thres_min, thres_max)\n",
    "    sig_count = len(sig)\n",
    "    bkgd_count = len(bkgd)\n",
    "    sig_rates = []\n",
    "    bkgd_rates = []\n",
    "    thres_list = np.arange(search_range[0], search_range[1], step)\n",
    "    for thres in thres_list:\n",
    "        sig_selected = np.greater(sig, thres)\n",
    "        bkgd_selected = np.greater(bkgd, thres)\n",
    "        sig_rates.append(np.sum(sig_selected)/sig_count)\n",
    "        bkgd_rates.append(np.sum(bkgd_selected)/bkgd_count)\n",
    "    return sig_rates, bkgd_rates, thres_list\n",
    "\n",
    "def plot_simple_ROC_SIC(sig, bkgd, step = 0.001):\n",
    "    \n",
    "    # Calculate simple ROC curve\n",
    "    sig_rates, bkgd_rates, thres_list = simple_ROC(sig, bkgd, step)\n",
    "    \n",
    "\n",
    "    # Plotting ROC\n",
    "    plt.rcParams['figure.figsize'] = [6, 6]\n",
    "    plt.plot(sig_rates, bkgd_rates,label='bkgd',linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"signal rate\")\n",
    "    plt.ylabel(\"background rate\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting SIC (ε/√ε_b) \n",
    "    event_significances = sig_rates/np.sqrt(bkgd_rates)\n",
    "    plt.rcParams['figure.figsize'] = [5, 4]\n",
    "    plt.plot(sig_rates,event_significances,label='beta3',linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"signal rate\")\n",
    "    plt.ylabel(\"significance improvement\")\n",
    "    plt.ylim(top=2.5)\n",
    "    plt.ylim(bottom=1)\n",
    "    plt.xlim(right=1)  # adjust the right leaving left unchanged\n",
    "    plt.xlim(left=0.1) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here evaluate ML performance by e.g.\n",
    "#r_combine_y,r_combine_x = generate_real_SIC(y_test,new_model_combine.predict([x_test,x_test_r]),mass_test,quality=1)\n",
    "\n",
    "r_full_y,r_full_x = generateSIC(y_test,predicted, quality=100)\n",
    "#r_fine_y,r_fine_x = generate_real_SIC(y_test,model_fine.predict(x_test_r),mass_test,quality=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "#plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#FRANK# event image SIC\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "plt.plot(sig_rates,significances,label='Beta3',linewidth=1)\n",
    "#FRANK# beta-3 SIC\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Keras",
   "language": "python",
   "name": "python_3_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
