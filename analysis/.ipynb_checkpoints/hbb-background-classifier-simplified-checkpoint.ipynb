{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#CODE TO INITIALISE TENSORFLOW\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "#set_session(tf.Session(config=config))\n",
    "print(\"Success!\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for i in range(4):\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[i],\n",
    "        tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024))\n",
    "\n",
    "\n",
    "#Prints the devices in use. The next time I open this, should be configured to only use GPU 3\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing TF and Keras\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "\n",
    "#FRANK# view versions\n",
    "print(\"Tensorflow version is\")\n",
    "print(tf.__version__)\n",
    "print(\"Keras version is\")\n",
    "print(keras.__version__)\n",
    "\n",
    "#FRANK# these lines limiting memory usages are for tf2, which requires cudatoolkit>10 and newer driver\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#print(gpus)\n",
    "#for i in range(4):   \n",
    "#    tf.config.experimental.set_virtual_device_configuration(gpus[i],\n",
    "#        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5860500000)])\n",
    "\n",
    "#config = tf.ConfigProto(log_device_placement=True)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "#sess = tf.Session(config=config)\n",
    "#set_session(sess)\n",
    "#print(\"Success!\")\n",
    "\n",
    "# Prints the devices in use. \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Tensorflow version is\")\n",
    "print(tf.__version__)\n",
    "print(\"Keras version is\")\n",
    "print(keras.__version__)\n",
    "\n",
    "# Use only GPU3\n",
    "tf.device('/device:XLA_GPU:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Importing Keras \n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "# Import Libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import matplotlib\n",
    "import time\n",
    "import pickle \n",
    "\n",
    "from itertools import groupby\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import pyjet\n",
    "from pyjet import cluster, DTYPE_PTEPM\n",
    "from pyjet.testdata import get_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "# Import local libraries\n",
    "from substructure-variables.py import * # Jet substructure variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#IMPORTANT NOTE DON'T DELETE\n",
    "#The 1e-3 factor in my weights comes from the fact that Pythia reports cross-sections in millibarns\n",
    "\n",
    "#VARIABLES\n",
    "width = 40\n",
    "height = 40 #width, height of pictures\n",
    "\n",
    "#FRANK# why do we need cross sec?\n",
    "backgroundCross = 2.048e-06 #cross-section of processes in millibarns\n",
    "\n",
    "actual_background_cross=2.84e-9 #barns\n",
    "average_number_accepted=2162\n",
    "\n",
    "actual_signal_cross = np.average([1.738e-14,1.7277e-14])\n",
    "signal_accepted = np.average([8708-189,8827-172])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     30,
     54,
     93,
     100,
     149,
     162,
     170,
     181,
     246,
     249,
     252,
     255,
     274,
     279,
     286
    ]
   },
   "outputs": [],
   "source": [
    "# This function reads off the original CSV format: \n",
    "# pt, eta, phi, m, id, isCharged \\n\n",
    "# pt, eta, phi, m, id, isCharged \\n\n",
    "# ... (of all particles in one event)\n",
    "# trimmed mass of identified jet \\n\n",
    "# \\n\n",
    "# and produces a event_list\n",
    "def return_event_list(fileName,max_Read = float(\"inf\"),weighted=0,pt_cut=0):\n",
    "    \n",
    "    printed = 0\n",
    "    \n",
    "    event_list = [];mass_list = [];weight_list = [];\n",
    "    tmp_events = open(fileName).read().split(\"\\n\\n\")[:-1]\n",
    "    print(len(tmp_events))\n",
    "    for x in tmp_events:\n",
    "        try:\n",
    "            if len(event_list) == max_Read: #FRANK# limit event size. If violated, interrupt the entire method\n",
    "                return(event_list,mass_list)\n",
    "            if weighted == 0:\n",
    "                                \n",
    "                #FRANK# rfind finds the last occurance\n",
    "                #FRANK# [a:b] extract everything b/w a and bth elem, including ath and excluding bth\n",
    "                #FRANK# why isn't the cut applied to weight and mass list? #ISSUE#\n",
    "                \n",
    "                mass_list.append(float(x[x.rfind(\"\\n\")+1:-1]))\n",
    "                to_cut = np.array(np.genfromtxt(x[:x.rfind(\"\\n\")].splitlines(), delimiter=\",\"))\n",
    "                event_list.append(to_cut[[x[0] > pt_cut for x in to_cut]])\n",
    "            else:\n",
    "                weight_list.append(float(x[x.rfind(\"\\n\")+1:]))\n",
    "                mass_list.append(float(x[x.rfind(\"\\n\",0,x.rfind(\"\\n\")-1)+1:x.rfind(\"\\n\")+1]))\n",
    "                to_cut = np.genfromtxt(x[:x.rfind(\"\\n\")].splitlines(),delimiter=\",\")\n",
    "                event_list.append(to_cut[[x[0] > pt_cut for x in to_cut]])\n",
    "        except:\n",
    "            print('We failed to turn your CSV into an np array. Make sure you have the correct format. ')\n",
    "            print( sys.exc_info()[0])\n",
    "            return\n",
    "    if weighted == 0:\n",
    "        #print(mass_list[0])\n",
    "        return(event_list,mass_list)\n",
    "    else:\n",
    "        return(event_list,mass_list,weight_list)\n",
    "\n",
    "#FRANK# Converting event_list to event image\n",
    "def return_image_list(event_list):\n",
    "    image_list = []\n",
    "    image_0 = np.zeros((width,height)) #Charged pt #FRANK# I think it's labeled wrong here. Would it matter?\n",
    "    image_1 = np.zeros((width,height)) #Neutral pt\n",
    "    image_2 = np.zeros((width,height)) #Charged multiplicity\n",
    "    \n",
    "    #FRANK# pt, eta, phi, m, id, isCharged \\n\n",
    "    #FRANK# 0   1    2    3  4   5\n",
    "    for z in range(len(event_list)):\n",
    "        image_0 = np.zeros((width,height));image_1 = np.zeros((width,height));image_2 = np.zeros((width,height))\n",
    "        for x in range(len(event_list[z])):\n",
    "            phi_index = math.floor(width*event_list[z][x,2]//(2*math.pi)+width//2)\n",
    "            eta_index = math.floor(height*event_list[z][x,1]//10+height/2) #FRANK# // is integer divide\n",
    "            eta_index = min(eta_index,height-1)\n",
    "            eta_index = max(0,eta_index)\n",
    "            phi_index = int(phi_index);eta_index = int(eta_index)\n",
    "            if (event_list[z][x,5] == 0):  #FRANK# neutral\n",
    "                image_0[phi_index,eta_index] = image_0[phi_index,eta_index] + event_list[z][x,0]\n",
    "            elif (event_list[z][x,5] == 1):  #FRANK# charged\n",
    "                image_1[phi_index,eta_index] = image_1[phi_index,eta_index] + event_list[z][x,0]\n",
    "                image_2[phi_index,eta_index] = image_2[phi_index,eta_index] + 1\n",
    "        image_0 = np.divide(image_0,np.sum(image_0))\n",
    "        image_1 = np.divide(image_1,np.sum(image_1))\n",
    "        image_2 = np.divide(image_2,np.sum(image_2))\n",
    "        image_list.append(np.array([image_0,image_1,image_2]))\n",
    "    return(image_list)\n",
    "\n",
    "# This method loads event files to produce:\n",
    "# event_list, mass_list, image_list, files_Read and weight_list\n",
    "def load_events(event_type,debug = 0, max_Read = float(\"inf\"), max_Files = float(\"inf\"), weighted=0, \\\n",
    "                path = \"/data1/users/jzlin/NLO/\", contains = \"philback\", pt_cut = 0):\n",
    "    print(\"Loading events for \" + event_type)\n",
    "    reading_event_list,reading_mass_list = [],[]\n",
    "    reading_image_list = []\n",
    "    reading_weight_list = []\n",
    "    files_Read = 0\n",
    "    print('list of files is'+ str(os.listdir(path)))\n",
    "    for i in os.listdir(path):\n",
    "        print('test: i is: '+ str(i))\n",
    "        print('total path is: '+os.path.join(path,i))\n",
    "        \n",
    "        if (files_Read == max_Files):\n",
    "            break\n",
    "        if len(reading_event_list) >= max_Read:\n",
    "            return(reading_event_list,reading_mass_list,reading_image_list,files_Read)\n",
    "        if 'swp' in i:\n",
    "            continue\n",
    "        if os.path.isfile(os.path.join(path,i)) and (event_type+contains) in i:\n",
    "            if debug==1:\n",
    "                print(i)\n",
    "                print(os.path.join(path,i))\n",
    "            if weighted==0:\n",
    "                print(i)\n",
    "                temp_event_list,temp_mass_list = return_event_list(os.path.join(path,i),pt_cut=pt_cut)\n",
    "            else:\n",
    "                temp_event_list,temp_mass_list,temp_weight_list = return_event_list(os.path.join(path,i),\n",
    "                                                                                    weighted=1,pt_cut=pt_cut)\n",
    "                reading_weight_list = reading_weight_list = temp_weight_list\n",
    "            temp_image_list = return_image_list(temp_event_list)\n",
    "            if (len(temp_image_list) != len(temp_mass_list)):\n",
    "                print(\"Something has gone wrong when reading the file\")\n",
    "                print(os.path.join(path,i))\n",
    "            reading_event_list = reading_event_list + temp_event_list\n",
    "            reading_mass_list = reading_mass_list + temp_mass_list\n",
    "            reading_image_list = reading_image_list + temp_image_list\n",
    "            files_Read = files_Read + 1\n",
    "            print(\"Read \" + str(files_Read) + \" number of files\\r\")\n",
    "    if weighted==0:\n",
    "        return(reading_event_list,reading_mass_list,reading_image_list,files_Read)\n",
    "    else:\n",
    "        return(reading_event_list,reading_mass_list,reading_image_list,files_Read,reading_weight_list)\n",
    "\n",
    "# This is not used. \n",
    "def return_fine_image_list(event_list, event_list_clustered, granularity, which_jet = 0):\n",
    "    image_list = []\n",
    "    image_0 = np.zeros((width,height)) #Charged pt\n",
    "    image_1 = np.zeros((width,height)) #Neutral pt\n",
    "    image_2 = np.zeros((width,height)) #Charged multiplicity\n",
    "\n",
    "    for z in range(len(event_list)):\n",
    "        image_0 = np.zeros((width,height))\n",
    "        image_1 = np.zeros((width,height))\n",
    "        image_2 = np.zeros((width,height))\n",
    "        for x in range(len(event_list[z])):\n",
    "            \n",
    "            try:\n",
    "                phi_index = (event_list[z][x,2]-event_list_clustered[z][which_jet].phi)\n",
    "            except:\n",
    "                print(z)\n",
    "            #At this point, phi_index is just delta_phi, which could be anywhere from -2pi to 2pi\n",
    "            if (phi_index % (2*math.pi) >= (width//2)*granularity) and (phi_index % (2*math.pi) <= 2*math.pi-(width//2)*granularity):\n",
    "                continue\n",
    "                #This gets rid of the delta phi's that are far away from the jet\n",
    "            phi_index = phi_index % (2*math.pi)\n",
    "            if phi_index > math.pi:\n",
    "                 phi_index = phi_index - 2*math.pi   \n",
    "            phi_index = int(math.floor(phi_index/granularity)) #should be good now\n",
    "            if (phi_index > (width//2)) or (phi_index < -(width//2)):\n",
    "                print(phi_index)\n",
    "            phi_index = phi_index + (width//2)\n",
    "            \n",
    "\n",
    "            eta_index = int(math.floor((event_list[z][x,1]-event_list_clustered[z][which_jet].eta)/granularity) + height//2)\n",
    "            if eta_index >= height:\n",
    "                continue\n",
    "            if eta_index < 0:\n",
    "                continue\n",
    "            \n",
    "            #finally, lets fill\n",
    "            if (event_list[z][x,5] == 0):\n",
    "                image_0[phi_index,eta_index] = image_0[phi_index,eta_index] + event_list[z][x,0]\n",
    "            elif (event_list[z][x,5] == 1):\n",
    "                image_1[phi_index,eta_index] = image_1[phi_index,eta_index] + event_list[z][x,0]\n",
    "                image_2[phi_index,eta_index] = image_2[phi_index,eta_index] + 1\n",
    "\n",
    "        #Now, lets go through and normalise to 255\n",
    "        image_0 = np.divide(image_0,np.sum(image_0))\n",
    "        image_1 = np.divide(image_1,np.sum(image_1))\n",
    "        image_2 = np.divide(image_2,np.sum(image_2))\n",
    "        image_list.append(np.array([image_0,image_1,image_2]))\n",
    "    return(image_list)\n",
    "\n",
    "def cluster_event(event_list):\n",
    "    event_list_clustered = []\n",
    "    for x in range(len(event_list)):\n",
    "        to_Cluster = np.array([event_list[x][:,0],event_list[x][:,1],event_list[x][:,2],event_list[x][:,3]])\n",
    "        to_Cluster = np.swapaxes(to_Cluster,0,1)\n",
    "        to_Cluster = np.core.records.fromarrays(to_Cluster.transpose(), \n",
    "                                             names='pT, eta, phi, mass',\n",
    "                                             formats = 'f8, f8, f8,f8')\n",
    "        sequence_Cluster = cluster(to_Cluster, R = 0.8,p = -1)\n",
    "        jets_Cluster = sequence_Cluster.inclusive_jets()\n",
    "        event_list_clustered.append(jets_Cluster)\n",
    "    return(event_list_clustered)\n",
    "\n",
    "def recluster_event(cluster_list):\n",
    "    reclustered_list= []\n",
    "    for i in range(len(cluster_list)):\n",
    "        sequence_Cluster = cluster((cluster_list[i][0]), R=0.2,p=-1)\n",
    "        jets_Cluster = sequence_Cluster.inclusive_jets()\n",
    "        reclustered_list.append(jets_Cluster)\n",
    "    return(reclustered_list)\n",
    "\n",
    "def return_fine_image_list_reclustered(event_list, event_list_clustered, radius, which_jet = 0,verbose = False):\n",
    "    image_list = []\n",
    "    image_0 = np.zeros((width,height)) #Neutral pt\n",
    "    image_1 = np.zeros((width,height)) #Charged pt\n",
    "    image_2 = np.zeros((width,height)) #Charged multiplicity\n",
    "    \n",
    "    no_two = 0\n",
    "\n",
    "    for z in range(len(event_list)):\n",
    "        image_0 = np.zeros((width,height))\n",
    "        image_1 = np.zeros((width,height))\n",
    "        image_2 = np.zeros((width,height))\n",
    "        \n",
    "        if (len(event_list_clustered[z]) > 1):\n",
    "            #First, let's find the direction of the second-hardest jet relative to the first-hardest subjet\n",
    "            phi_dir = -(dphi(event_list_clustered[z][1].phi,event_list_clustered[z][0].phi))\n",
    "            eta_dir = -(event_list_clustered[z][1].eta - event_list_clustered[z][0].eta)\n",
    "            #Norm difference:\n",
    "            norm_dir = np.linalg.norm([phi_dir,eta_dir])\n",
    "            #This is now the y-hat direction. so we can actually find the unit vector:\n",
    "            y_hat = np.divide([phi_dir,eta_dir],np.linalg.norm([phi_dir,eta_dir]))\n",
    "            #and we can find the x_hat direction as well\n",
    "            x_hat = np.array([y_hat[1],-y_hat[0]]) \n",
    "        else:\n",
    "            no_two = no_two + 1\n",
    "            #continue\n",
    "            \n",
    "        if verbose==True:\n",
    "            print(x_hat,y_hat,norm_dir)\n",
    "            \n",
    "        \n",
    "        for x in range(len(event_list[z])):\n",
    "            if (len(event_list_clustered[z]) == 1):\n",
    "                #In the case that the reclustering only found one hard jet (that seems kind of bad, but hey)\n",
    "                #no_two = no_two+1\n",
    "                new_coord = [dphi(event_list[z][x,2],event_list_clustered[z][0].phi),event_list[z][x,1]-event_list_clustered[z][0].eta]\n",
    "                indxs = [math.floor(width*new_coord[0]/(radius*1.5))+width//2,math.floor(height*(new_coord[1])/(radius*1.5))+height//2]\n",
    "            else:\n",
    "                #Now, we want to express an incoming particle in this new basis:\n",
    "                part_coord = [dphi(event_list[z][x,2],event_list_clustered[z][0].phi),event_list[z][x,1]-event_list_clustered[z][0].eta]\n",
    "                new_coord = np.dot(np.array([x_hat,y_hat]),part_coord)\n",
    "                #Now, we want to cast these new coordinates into our array\n",
    "                indxs = [math.floor(width*new_coord[0]/(radius*1.5))+width//2,math.floor(height*(new_coord[1]+norm_dir/1.5)/(radius*1.5))+height//2]\n",
    "                \n",
    "            if indxs[0] >= width or indxs[1] >= height or indxs[0] <= 0 or indxs[1] <= 0:\n",
    "                continue\n",
    "            phi_index = int(indxs[0]); eta_index = int(indxs[1])\n",
    "            #finally, lets fill\n",
    "            if (event_list[z][x,5] == 0):\n",
    "                image_0[phi_index,eta_index] = image_0[phi_index,eta_index] + event_list[z][x,0]\n",
    "            elif (event_list[z][x,5] == 1):\n",
    "                image_1[phi_index,eta_index] = image_1[phi_index,eta_index] + event_list[z][x,0]\n",
    "                image_2[phi_index,eta_index] = image_2[phi_index,eta_index] + 1\n",
    "\n",
    "        #Now, lets go through and normalise to 255\n",
    "        if (np.sum(image_0) == 0 or np.sum(image_1) == 0 or np.sum(image_2) == 0):\n",
    "            image_list.append(np.array([image_0,image_1,image_2]))\n",
    "            continue\n",
    "        image_0 = np.divide(image_0,np.sum(image_0))\n",
    "        image_1 = np.divide(image_1,np.sum(image_1))\n",
    "        image_2 = np.divide(image_2,np.sum(image_2))\n",
    "        image_list.append(np.array([image_0,image_1,image_2]))\n",
    "    print(\"no two \" + str(no_two))\n",
    "    return(image_list)\n",
    "\n",
    "# Correct phi range\n",
    "def fix_phi(phi):\n",
    "    while phi > math.pi:\n",
    "        phi = phi - 2*math.pi\n",
    "    while phi < -math.pi:\n",
    "        phi = phi + 2*math.pi\n",
    "    return phi\n",
    "\n",
    "# Returns the difference in phi between phi, and phi_center\n",
    "# as a float between (-PI, PI)\n",
    "def dphi(phi,phi_c):\n",
    "    \n",
    "    dphi_temp = phi - phi_c\n",
    "    while dphi_temp > np.pi:\n",
    "        dphi_temp = dphi_temp - 2*np.pi\n",
    "    while dphi_temp < -np.pi:\n",
    "        dphi_temp = dphi_temp + 2*np.pi\n",
    "    return (dphi_temp)\n",
    "\n",
    "# Rapidity\n",
    "\n",
    "def y(p):\n",
    "    return ((1/2)*math.log((p.e+p.pz)/(p.e-p.pz)))\n",
    "\n",
    "def R(con1,con2):\n",
    "    return (((con1.eta-con2.eta)**2+dphi(con1.phi,con2.phi)**2)**(1/2))\n",
    "\n",
    "def R_y(con1,con2):\n",
    "    return (((y(con1)-y(con2))**2+dphi(con1.phi,con2.phi)**2)**(1/2))\n",
    "\n",
    "def N_2(jcon):\n",
    "    #Takes jcon, jet constituents\n",
    "    p_x_total = np.sum([con.px for con in jcon])\n",
    "    p_y_total = np.sum([con.py for con in jcon])\n",
    "    p_total = (p_x_total**2+p_y_total**2)**(1/2)\n",
    "    \n",
    "    v_1e2 = 0\n",
    "    for i in range(len(jcon)):\n",
    "        for j in range(i+1,len(jcon)):\n",
    "            v_1e2 = v_1e2+ jcon[i].pt*jcon[j].pt*R(jcon[i],jcon[j])/(p_total**2)\n",
    "    v_2e3 = 0\n",
    "    for i in range(len(jcon)):\n",
    "        for j in range(i+1,len(jcon)):\n",
    "            for k in range(j+1,len(jcon)):\n",
    "                v_2e3 = v_2e3 + jcon[i].pt*jcon[j].pt*jcon[j].pt*min(R(jcon[i],jcon[j])*R(jcon[i],jcon[k]),\n",
    "                                                                     R(jcon[j],jcon[k])*R(jcon[i],jcon[j]),\n",
    "                                                        R(jcon[i],jcon[k])*R(jcon[j],jcon[k]))/(p_total**3)\n",
    "    return v_2e3/(v_1e2**2)\n",
    "\n",
    "# Softdrop \n",
    "\n",
    "class myJet(object):\n",
    "    def __init__(self,px,py,pz):\n",
    "        self.px = px; self.py = py; self.pz = pz; self.pt = (px**2+py**2)**(1/2)\n",
    "        self.phi = math.atan2(py,px); self.eta = -math.log(math.tan(math.atan2(self.pt,self.pz)/2));\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "    def add_child(self,obj):\n",
    "        self.children.append(obj)\n",
    "\n",
    "def softdrop(jcon,z=0.1,debug = 0):\n",
    "    #Takes the constituents of a jet, and softdrops it.\n",
    "    #First, we need to step through the jet and build the tree of clustering\n",
    "    #Since we are reclustering the whole thing; just take R = 1; i.e. dont need to think about it\n",
    "    def distance(con1,con2):\n",
    "        return R(con1,con2)**2\n",
    "    pseudojets = []\n",
    "    nodes = []\n",
    "    for con in jcon:\n",
    "        x = Node([con,1])\n",
    "        #pseudojets.append(x)\n",
    "        nodes.append(x) #1 means its still a pseudojet; i.e. not been clustered\n",
    "    def how_many_pseudo(nodes):\n",
    "        how_many = 0\n",
    "        for node in nodes:\n",
    "            if node.data[1] == 1:\n",
    "                how_many = how_many + 1\n",
    "        return how_many\n",
    "    if debug == 1:\n",
    "        print(\"len(nodes) : \" + str(len(nodes)))\n",
    "        #print(nodes)\n",
    "    rep = 0\n",
    "    while how_many_pseudo(nodes) > 1:\n",
    "        #print(how_many_pseudo(nodes))\n",
    "        min_distance = float(\"inf\")\n",
    "        min_index = [0,0]\n",
    "        for i in range(0,len(nodes)):\n",
    "            if nodes[i].data[1] == 0: #Its already part of something else\n",
    "                continue\n",
    "            for j in range(i+1,len(nodes)):\n",
    "                if nodes[j].data[1] == 0:\n",
    "                    continue\n",
    "                if distance(nodes[i].data[0],nodes[j].data[0]) < min_distance:\n",
    "                    min_index[0] = i; min_index[1] = j; \n",
    "                    min_distance = distance(nodes[i].data[0],nodes[j].data[0])\n",
    "        i = min_index[0];j=min_index[1];\n",
    "        new_node = Node([myJet(nodes[i].data[0].px+nodes[j].data[0].px,\n",
    "                           nodes[i].data[0].py+nodes[j].data[0].py,\n",
    "                           nodes[i].data[0].pz+nodes[j].data[0].pz),1])\n",
    "        new_node.add_child(nodes[i])\n",
    "        new_node.add_child(nodes[j])\n",
    "        nodes.append(new_node)\n",
    "        nodes[i].data[1] = 0\n",
    "        nodes[j].data[1] = 0\n",
    "    \n",
    "    #print(nodes)\n",
    "\n",
    "    to_check = [] #nodes to check\n",
    "    for i in range(len(nodes)):\n",
    "        if nodes[i].data[1] == 1:\n",
    "            to_check.append(nodes[i])\n",
    "    softcon = []\n",
    "    while len(to_check) > 0:\n",
    "        #print(to_check)\n",
    "        our_childs = to_check[0].children\n",
    "        #print(our_childs)\n",
    "        if len(our_childs) == 0:\n",
    "            softcon.append(to_check[0].data[0])\n",
    "            to_check.pop(0)\n",
    "            continue\n",
    "        if min(our_childs[0].data[0].pt,our_childs[1].data[0].pt)/  \\\n",
    "                        (our_childs[0].data[0].pt+our_childs[1].data[0].pt) > z:\n",
    "            to_check.append(our_childs[0])\n",
    "            to_check.append(our_childs[1])\n",
    "        elif our_childs[0].data[0].pt > our_childs[1].data[0].pt:\n",
    "            to_check.append(our_childs[0])\n",
    "        else:\n",
    "            to_check.append(our_childs[1])\n",
    "        to_check.pop(0)\n",
    "    return softcon\n",
    "\n",
    "# Normalize and zero-center any image\n",
    "def zero_center_and_normalize(background_images, signal_images):\n",
    "    tmp_av = np.average(np.concatenate((background_images, signal_images)), axis=0)\n",
    "    tmp_sd = np.std(np.concatenate((background_images, signal_images)), axis=0)\n",
    "    for i in range(len(background_images)):\n",
    "        background_images[i] = np.divide((background_images[i] - tmp_av), (tmp_sd+1e-5)) #perhaps add some r to temp_sd to suppress noise\n",
    "    for i in range(len(signal_images)):\n",
    "        signal_images[i] = np.divide((signal_images[i] - tmp_av), (tmp_sd+1e-5))#/tmp_sd\n",
    "    return background_images, signal_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reading in Josh's files; background is a single large file (pT cut must be 1 here for Josh's sample)\n",
    "# This read produces event_list (collection of raw vectors) and event images\n",
    "background_event_list,background_mass_list,background_image_list,num_background_files = \\\n",
    "    load_events(\"actual\", max_Files=num_background_files,path=\"/data1/users/jzlin/MLM/background_7413/\",\\\n",
    "                contains=\"_actual\",pt_cut=1)\n",
    "num_background_files = 15693\n",
    "signal_event_list,signal_mass_list,signal_image_list,num_signal_files = \\\n",
    "    load_events(\"actual\", max_Read = len(background_event_list),path=\"/data1/users/jzlin/MLM/heavy_signal/\",\\\n",
    "                contains=\"_signal\",pt_cut=1)\n",
    "\n",
    "# Check size of dataset\n",
    "print(len(background_mass_list),len(signal_mass_list))\n",
    "\n",
    "# Zero centering and normalizing\n",
    "background_image_list, signal_image_list = zero_center_and_normalize(background_image_list,signal_image_list)\n",
    "background_mass_window = np.logical_and(np.array(background_mass_list) > 115,np.array(background_mass_list) < 135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights\n",
    "background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "signal_weights = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster events_lists into jets. The results are named background/signal_event_list_clustered\n",
    "background_event_list_clustered = cluster_event(background_event_list)\n",
    "signal_event_list_clustered = cluster_event(signal_event_list)\n",
    "\n",
    "# Reclustering the events (i.e. clustering within events)\n",
    "background_reclustered = recluster_event(background_event_list_clustered)\n",
    "signal_reclustered = recluster_event(signal_event_list_clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce jet images, the zero-center and normalize\n",
    "background_recluster_images = return_fine_image_list_reclustered(background_event_list,\n",
    "                                                           background_reclustered,0.8)\n",
    "signal_recluster_images = return_fine_image_list_reclustered(signal_event_list,\n",
    "                                                           signal_reclustered,0.8)\n",
    "\n",
    "background_recluster_images, signal_recluster_images = zero_center_and_normalize(signal_recluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(background_recluster_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# Function that splits whole event dataset into test, train and evaluation sets.\n",
    "# the *_cut outputs has a 115<mass<135 cut.\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "rsplit = np.array([0.5,0.75])\n",
    "\n",
    "def splitData(background_image_list,signal_image_list):\n",
    "    try:\n",
    "        input_shape = background_image_list[0].shape\n",
    "    except AttributeError:\n",
    "        input_shape = (0)\n",
    "        \n",
    "    b_split = np.split(background_image_list,(len(background_image_list)*rsplit).astype(int))\n",
    "    s_split = np.split(signal_image_list,(len(signal_image_list)*rsplit).astype(int))\n",
    "    bm_split = np.split(background_mass_list,(len(background_mass_list)*rsplit).astype(int))\n",
    "    sm_split = np.split(signal_mass_list,(len(signal_mass_list)*rsplit).astype(int))\n",
    "    \n",
    "    x_train = np.concatenate((b_split[0],s_split[0]))\n",
    "    y_train = np.array(np.concatenate((np.zeros(len(b_split[0])),np.ones(len(s_split[0])))))\n",
    "    mass_train = np.concatenate((bm_split[0],sm_split[0]))\n",
    "    \n",
    "    x_val = np.concatenate((b_split[1],s_split[1]))\n",
    "    y_val = np.array(np.concatenate((np.zeros(len(b_split[1])),np.ones(len(s_split[1])))))\n",
    "    mass_val = np.concatenate((bm_split[1],sm_split[1]))\n",
    "    \n",
    "    x_test = np.concatenate((b_split[2],s_split[2]))\n",
    "    y_test = np.array(np.concatenate((np.zeros(len(b_split[2])),np.ones(len(s_split[2])))))\n",
    "    mass_test = np.concatenate((bm_split[2],sm_split[2]))\n",
    "        \n",
    "    \n",
    "    x_train_cut = [] #Note that we *train* on this cut sample, but *test* on the whole thing I guess\n",
    "    y_train_cut = []\n",
    "    mass_train_cut = []\n",
    "    for x in reversed(range(len(x_train))):\n",
    "        if mass_train[x] < 135 and mass_train[x] > 115:\n",
    "            x_train_cut.append(x_train[x])\n",
    "            y_train_cut.append(y_train[x])\n",
    "            mass_train_cut.append(mass_train[x])\n",
    "    x_train_cut = np.array(x_train_cut)\n",
    "    y_train_cut = np.array(y_train_cut)\n",
    "    mass_train_cut = np.array(mass_train_cut)\n",
    "    \n",
    "    x_val_cut = [] \n",
    "    y_val_cut = []\n",
    "    mass_val_cut = []\n",
    "    for x in reversed(range(len(x_val))):\n",
    "        if mass_val[x] < 135 and mass_val[x] > 115:\n",
    "            x_val_cut.append(x_val[x])\n",
    "            y_val_cut.append(y_val[x])\n",
    "            mass_val_cut.append(mass_val[x])\n",
    "    x_val_cut = np.array(x_val_cut)\n",
    "    y_val_cut = np.array(y_val_cut)\n",
    "    mass_val_cut = np.array(mass_val_cut)\n",
    "    \n",
    "    x_test_cut = [] \n",
    "    y_test_cut = []\n",
    "    mass_test_cut = []\n",
    "    for x in reversed(range(len(x_test))):\n",
    "        if mass_test[x] < 135 and mass_test[x] > 115:\n",
    "            x_test_cut.append(x_test[x])\n",
    "            y_test_cut.append(y_test[x])\n",
    "            mass_test_cut.append(mass_test[x])\n",
    "    x_test_cut = np.array(x_test_cut)\n",
    "    y_test_cut = np.array(y_test_cut)\n",
    "    mass_test_cut = np.array(mass_test_cut)\n",
    "    \n",
    "    return(input_shape,\n",
    "           x_train,y_train,mass_train,\n",
    "           x_val,y_val,mass_val,\n",
    "           x_test,y_test,mass_test,\n",
    "           x_train_cut,y_train_cut,mass_train_cut,\n",
    "           x_val_cut,y_val_cut,mass_val_cut,\n",
    "           x_test_cut,y_test_cut,mass_test_cut,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split event images\n",
    "input_shape, \\\n",
    "    x_train,y_train,mass_train, \\\n",
    "    x_val,y_val,mass_val, \\\n",
    "    x_test,y_test,mass_test, \\\n",
    "    x_train_cut,y_train_cut,mass_train_cut, \\\n",
    "    x_val_cut,y_val_cut,mass_val_cut, \\\n",
    "    x_test_cut,y_test_cut,mass_test_cut = splitData(background_image_list,signal_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split jet images\n",
    "input_shape_r, \\\n",
    "    x_train_r,y_train_r,mass_train_r, \\\n",
    "    x_val_r,y_val_r,mass_val_r, \\\n",
    "    x_test_r,y_test_r,mass_test_r, \\\n",
    "    x_train_cut_r,y_train_cut_r,mass_train_cut_r, \\\n",
    "    x_val_cut_r,y_val_cut_r,mass_val_cut_r, \\\n",
    "    x_test_cut_r,y_test_cut_r,mass_test_cut_r = splitData(background_recluster_images,signal_recluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK print an event image\n",
    "print(np.shape(x_train[0]))\n",
    "plt.imshow(x_train_r[-1][0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_r[-1][1])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_r[-1][2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     38,
     49,
     89,
     103,
     117
    ]
   },
   "outputs": [],
   "source": [
    "def generate_real_SIC(expect,predict,masses,quality=1,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten(),masses)).transpose(), key=lambda x: x[1])),0)\n",
    "    background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "    signal_weights = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "    efficiency = []; signal_eff = [];\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort),quality*10):\n",
    "        background_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==0],bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==1],bins=np.arange(50,197,7))\n",
    "        log_likelihood = 0; baseLL = 0; \n",
    "        signal_eff.append(np.sum(to_sort[:i+1,0])/total_signal)\n",
    "        test_LL_vs_SS = []\n",
    "        for signal_strength in np.arange(1,5,quality/1000):\n",
    "            log_likelihood = 0;\n",
    "            for k in range(len(bins)-1):\n",
    "                expected = background_weights*background_mass_binned[k]+signal_weights*signal_strength*signal_mass_binned[k]\n",
    "                observed = background_weights*background_mass_binned[k]+signal_weights*signal_mass_binned[k]\n",
    "                if (expected <= 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "            if signal_strength == 1:\n",
    "                baseLL = log_likelihood\n",
    "            test_LL_vs_SS.append(log_likelihood)\n",
    "            if log_likelihood < baseLL-1/2:\n",
    "                efficiency.append(1/(signal_strength-1))\n",
    "                break\n",
    "            if signal_strength > 3:\n",
    "                efficiency.append(1/2)\n",
    "                break\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"base efficiency : \" + str(float(efficiency[-1])))\n",
    "    efficiency = np.array(efficiency)/float(efficiency[-1])\n",
    "    max_eff = np.max(efficiency)\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def find_highest_SIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)): # generate a int range scanning over 95% of all samples??\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    return(max_eff)\n",
    "\n",
    "def find_highest_SIC_binned(expect,predict,masses):\n",
    "    bins = np.arange(50,197,7)\n",
    "    efficiency = []\n",
    "    def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "        log_likelihood = 0\n",
    "        background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "        background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "        signal_weights = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "        for i in range(len(bins)-1):\n",
    "            expected = background_weights*background_mass_binned[i]+signal_weights*signal_strength*signal_mass_binned[i]\n",
    "            observed = background_weights*background_mass_binned[i]+signal_weights*signal_mass_binned[i]\n",
    "            if (expected <= 0):\n",
    "                return float(\"inf\")\n",
    "            log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "        return -log_likelihood\n",
    "    \n",
    "    sigmas = []\n",
    "    for i in np.arange(0,0.9,0.05):\n",
    "        #res = least_squares(lambda x : log_like(x,masses[np.logical_and(predict.flatten() >= i,expect == 0)],\n",
    "        #                                        masses[np.logical_and(predict.flatten() >= i,expect == 1)]),x0=1)\n",
    "\n",
    "        #i is the cut on the machine learning\n",
    "        kept_back = masses[np.logical_and(predict.flatten() >= i,expect == 0)]\n",
    "        kept_signal = masses[np.logical_and(predict.flatten() >= i,expect == 1)]\n",
    "        j_array = []\n",
    "        for j in np.arange(1,25,0.5):\n",
    "            #print log_like(j,kept_back,kept_signal)\n",
    "            if log_like(j,kept_back,kept_signal) > log_like(1,kept_back,kept_signal)+0.5:\n",
    "                j_array.append(j-1)\n",
    "                break\n",
    "            if j >20:\n",
    "                j_array.append(20)\n",
    "                break\n",
    "        print(j_array,i)\n",
    "        sigmas.append(1/np.min(j_array))\n",
    "        \n",
    "    max_eff = np.max(sigmas)\n",
    "    return(max_eff)\n",
    "\n",
    "def generateSIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []; signal_eff = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)):\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        signal_eff.append(signal_eff_temp)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "    log_likelihood = 0\n",
    "    background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "    signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "    background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "    signal_weights = 3.15*actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weights*background_mass_binned[i]+signal_weights*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weights*background_mass_binned[i]+signal_weights*signal_mass_binned[i]\n",
    "        if (expected <= 0):\n",
    "            return float(\"inf\")\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return -log_likelihood\n",
    "\n",
    "def log_like(signal_strength):\n",
    "    log_likelihood = 0\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weights*background_mass_binned[i]+signal_weights*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weights*background_mass_binned[i]+signal_weights*signal_mass_binned[i]\n",
    "        #print(expected)\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     7,
     10,
     24
    ]
   },
   "outputs": [],
   "source": [
    "#FRANK# Returns a function that generates a padding layer\n",
    "#FRANK# x is the detector image, of following format:\n",
    "#FRANK# [index, image#(charged and neutral pt and multiplicity), 40, 40]\n",
    "def return_pad_me(padding):\n",
    "    def pad_me(x):\n",
    "        #FRANK# x[:,:,:y,:] slice x off from y at the given axis.\n",
    "        return(tf.concat((x,x[:,:,:padding,:]),2))\n",
    "    return(pad_me)\n",
    "\n",
    "def pad_out(padding,input_shape):\n",
    "    return input_shape\n",
    "\n",
    "class gen_call(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, test_data):\n",
    "        self.x, self.y = test_data\n",
    "    \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.highest_SIC_train = []\n",
    "        self.highest_SIC_test = []\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        self.highest_SIC_test.append(find_highest_SIC(self.y,y_pred))\n",
    "        print(str(self.highest_SIC_test[-1]) + \" is how good\")\n",
    "\n",
    "def show_outputs(output):\n",
    "    #Assumes the output is in shape like (32,41,36)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for i in range(1,1+output.shape[0]):\n",
    "        fig.add_subplot(4,output.shape[0]/4,i)\n",
    "        plt.imshow(10*output[i-1,:,:])\n",
    "        plt.axis('off')\n",
    "    #plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# this method is the exact replica of find_highest_SIC() but operates on tensors.\n",
    "#FRANK# for some reason, pred has different format from expect. \n",
    "def highest_SIC_metric(y_true,y_pred):\n",
    "    print('highest_SIC_metric() is called')\n",
    "    y_true = tf.keras.backend.flatten(y_true) #FRANK# flattens to 1D\n",
    "    y_pred = tf.keras.backend.flatten(y_pred) #FRANK# flattens to 1D\n",
    "    \n",
    "    #stacked = tf.transpose(tf.stack((expect,predict)))\n",
    "    #to_sort = tf.reverse(sorted(stacked, key=lambda x: x[1]),0) #FRANK# sorted is wrong!!\n",
    "    total_sample = tf.cast(tf.size(y_pred), tf.float32)  # count total nums, then cast to float32 to avoid issue\n",
    "    total_signal = tf.cast(tf.reduce_sum(y_true), tf.float32)  # summing across all predicted (where sigs are 1's and bkgs are 0's) to get total num of signal. \n",
    "    total_background = tf.cast(tf.subtract(total_sample, total_signal), tf.float32)  # subtracting signal countss from total size to get background counts\n",
    "    \n",
    "    \n",
    "    # original mechanism:\n",
    "    # 1. sort by ML score\n",
    "    # 2. sum all actuals before an index\n",
    "    # 3. count all 0's before an index \n",
    "    \n",
    "    sorted_indices = tf.argsort(y_pred,axis=-1,direction='ASCENDING') # tf.argsort: Returns the indices of a tensor that give its sorted order along an axis.\n",
    "    sorted_sigs = tf.gather(y_pred,sorted_indices)\n",
    "    # return the indices of prediction in ascending order. By reading these indices, you can access corresponding expected y's.\n",
    "    ones = tf.fill(tf.shape(sorted_sigs), 1.0)\n",
    "    sorted_bkgs = tf.subtract(ones, sorted_sigs)\n",
    "    # make a sorted tensor where 1's are bkgs and 0's are sigs\n",
    "    \n",
    "    sig_cum_sums = tf.cast(tf.cumsum(sorted_sigs), tf.float32) # return the integrated signal number from 0 to each index\n",
    "    bkg_cum_sums = tf.cast(tf.cumsum(sorted_bkgs), tf.float32) # return the integrated bkg number from 0 to each index\n",
    "\n",
    "    sig_effs = tf.divide(sig_cum_sums, total_signal)\n",
    "    bkg_effs = tf.divide(bkg_cum_sums, total_background) #FRANK# total_background might be 0, causing bkg_effs=inf\n",
    "    effs = tf.divide(sig_effs, tf.sqrt(bkg_effs))\n",
    "        \n",
    "    return tf.reduce_max(effs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "# Event image model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "event_image_cnn = Sequential()\n",
    "event_image_cnn.add(Lambda(return_pad_me(4),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first'))\n",
    "event_image_cnn.add(Lambda(return_pad_me(1),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "event_image_cnn.add(Lambda(return_pad_me(4),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(Conv2D(64, (5, 5), \n",
    "                 activation='relu',\n",
    "                 data_format='channels_first'))\n",
    "event_image_cnn.add(Lambda(return_pad_me(1),\n",
    "                 input_shape=input_shape))\n",
    "event_image_cnn.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "event_image_cnn.add(Flatten())\n",
    "event_image_cnn.add(Dense(1000, activation='relu'))\n",
    "event_image_cnn.add(Dense(1, activation='sigmoid'))\n",
    "event_image_cnn.summary()\n",
    "\n",
    "#model_opt = keras.optimizers.Adadelta(lr=2.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model_opt = keras.optimizers.Adadelta() #FRANK# decrease learning rate\n",
    "\n",
    "event_image_cnn.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=model_opt,\n",
    "              metrics=['accuracy', highest_SIC_metric])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Jet image model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "jet_image_cnn = Sequential()\n",
    "jet_image_cnn.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first',input_shape=input_shape))\n",
    "#jet_image_cnn.add(Dropout(0.5))\n",
    "jet_image_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "jet_image_cnn.add(Conv2D(64, (5, 5), activation='relu',data_format='channels_first'))\n",
    "#jet_image_cnn.add(Dropout(0.5))\n",
    "jet_image_cnn.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "jet_image_cnn.add(Flatten())\n",
    "jet_image_cnn.add(Dense(1000, activation='relu'))\n",
    "#jet_image_cnn.add(Dropout(0.2))\n",
    "jet_image_cnn.add(Dense(1, activation='sigmoid'))\n",
    "#jet_image_cnn.summary()\n",
    "\n",
    "model_opt = keras.optimizers.Adadelta()\n",
    "\n",
    "jet_image_cnn.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=model_opt,\n",
    "              metrics=['accuracy', 'highest_SIC_test'])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Combined model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#FRANK# input shape is calculated during splitting:\n",
    "#FRANK# splitData(background_image_list,signal_image_list)\n",
    "#FRANK# how is jet image taken?\n",
    "event_image_branch = Sequential()\n",
    "\n",
    "#FRANK# Lambda is from Keras.\n",
    "event_image_branch.add(Lambda(return_pad_me(5), #FRANK# tf.concat((x,x[:,:,:5,:]),2). Concatenate means pasting together.\n",
    "                            #FRANK# this layer slices off an edge of the tensor and paste it on the other side.\n",
    "                 input_shape=input_shape))\n",
    "event_image_branch.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first'))\n",
    "event_image_branch.add(Lambda(return_pad_me(2)))\n",
    "event_image_branch.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "event_image_branch.add(Lambda(return_pad_me(5),\n",
    "                 input_shape=input_shape))\n",
    "event_image_branch.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#event_image_branch.add(Dropout(0.5))\n",
    "event_image_branch.add(Lambda(return_pad_me(2)))\n",
    "event_image_branch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "event_image_branch.add(Flatten())\n",
    "event_image_branch.add(Dense(300,activation='relu'))\n",
    "#event_image_branch.add(Dropout(0.5))\n",
    "\n",
    "jet_image_branch = Sequential()\n",
    "jet_image_branch.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                      kernel_initializer='random_uniform',input_shape=input_shape_r, data_format=\"channels_first\")) #FRANK# problemmatic. see debug note #1\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "jet_image_branch.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "jet_image_branch.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "jet_image_branch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "jet_image_branch.add(Flatten())\n",
    "jet_image_branch.add(Dense(300, activation='relu'))\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "#FRANK# Merge layer is no longer supported.  \n",
    "#combined_model = Sequential()\n",
    "#combined_model.add(Merge([event_image_branch, jet_image_branch], mode = 'concat'))\n",
    "#combined_model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#FRANK# these are not models, but tensors. Model.input or Model.output are tensor pointers I think.\n",
    "combined_model_tensor = Concatenate(axis=-1)([event_image_branch.output, jet_image_branch.output])   \n",
    "combined_model_tensor = Dense(1,activation='sigmoid')(combined_model_tensor)\n",
    "#FRANK# what you need to do is merging output of 300 and 300 into one.\n",
    "\n",
    "#combined_model.summary()\n",
    "combined_model = Model([event_image_branch.input,jet_image_branch.input], combined_model_tensor)\n",
    "combined_model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', highest_SIC_metric])\n",
    "\n",
    "# Visualizing event image model\n",
    "plot_model(combined_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#FRANK# Sample training call testing metric\n",
    "epochs = 100\n",
    "print(y_train)\n",
    "with tf.device('/device:XLA_GPU:3'):\n",
    "    history_event_only = event_image_cnn.fit(x_train, y_train,\n",
    "        batch_size=1024,#batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val,y_val),\n",
    "        verbose=1, shuffle=True, callbacks=[EarlyStopping(monitor='highest_SIC_test', patience=2), gen_call((x_val_cut,y_val_cut))])\n",
    "\n",
    "#FRANK# for event image SIC: predicting\n",
    "with tf.device('/device:XLA_GPU:3'):\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    \n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# testing metric\n",
    "print(np.array(y_pred.flatten()))\n",
    "print(np.array(y_test))\n",
    "print(highest_SIC_metric(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# training history plots\n",
    "def loss_history_plot(training_history):\n",
    "    print(training_history.history.keys())\n",
    "    plt.plot(training_history.history['loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print groomed mass histogram\n",
    "plt.rcParams['figure.figsize'] = [7.5,6]\n",
    "background_weight = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "signal_weight = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "background_weights = np.full(21, background_weight, dtype=None, order='C')\n",
    "signal_weights = np.full(21, signal_weight, dtype=None, order='C')\n",
    "\n",
    "hist_bkg, bin_edges_bkg = np.histogram(background_mass_list, bins=21, range = (50, 197))\n",
    "bin_centers_bkg = (bin_edges_bkg[:-1] + bin_edges_bkg[1:]) / 2\n",
    "hist_sig, bin_edges_bkg = np.histogram(signal_mass_list, bins=21, range = (50, 197))\n",
    "\n",
    "plt.step(bin_centers_bkg, hist_bkg*background_weight, label='bkgd')\n",
    "plt.step(bin_centers_bkg, hist_sig*signal_weight, label='sig')\n",
    "plt.legend()\n",
    "plt.ylim(top=10000)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(right=197)  # adjust the right leaving left unchanged\n",
    "plt.xlim(left=50) \n",
    "plt.show()\n",
    "#log_like(1,background_mass_list=background_mass_list,signal_mass_list=signal_mass_list)\n",
    "\n",
    "# Calculating beta3\n",
    "background_beta3 = find_new_var_beta_3(background_reclustered,background_event_list_clustered,pt_cut = 1)\n",
    "signal_beta3 = find_new_var_beta_3(signal_reclustered,signal_event_list_clustered,pt_cut = 1)\n",
    "\n",
    "# Beta-3 histogram\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 4]\n",
    "beta3_min = min([min(background_beta3),min(signal_beta3)])\n",
    "beta3_max = max([max(background_beta3),max(signal_beta3)])\n",
    "print('beta3 range is '+str((beta3_min, beta3_max)))\n",
    "plt.hist(background_beta3, label='bkgd', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_beta3, label='sig', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Beta-3\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(background_mass_list, label='bkgd', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_mass_list, label='sig', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Trimmed Mass\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Beta-3 plots\n",
    "plot_simple_ROC_SIC(signal_beta3, background_beta3, step = 0.001):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the predicted results into sample and background using known answers.\n",
    "# Here we assume all backgrounds are put before signals in all datasets. \n",
    "#TODO# Check before use!\n",
    "\n",
    "def split_sig_bkgd(predicted, expected)\n",
    "    split_location = int(np.argwhere(expected==1.)[0])\n",
    "    #FRANK# signal predictions with event image model\n",
    "    sig = predicted[split_location:]\n",
    "    bkgd = predicted[:split_location]\n",
    "\n",
    "    return sig, bkgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# calculating true positive rates and false positive rates. \n",
    "#FRANK# TPR = TP/P\n",
    "#FRANK# FPR = FP/N\n",
    "#FRANK# note that bkgds have lower beta in general, so the cut should be \n",
    "#FRANK# beta3 > threshold.\n",
    "#FRANK# 'where' usage referred from \n",
    "#FRANK# https://stackoverflow.com/questions/12995937/count-all-values-in-a-matrix-greater-than-a-value\n",
    "\n",
    "def simple_ROC(sig, bkgd, step = 1):\n",
    "    thres_min = min([min(sig),min(bkgd)])\n",
    "    thres_max = max([max(sig),max(bkgd)])\n",
    "    search_range=(thres_min, thres_max)\n",
    "    sig_count = len(sig)\n",
    "    bkgd_count = len(bkgd)\n",
    "    sig_rates = []\n",
    "    bkgd_rates = []\n",
    "    thres_list = np.arange(search_range[0], search_range[1], step)\n",
    "    for thres in thres_list:\n",
    "        sig_selected = np.greater(sig, thres)\n",
    "        bkgd_selected = np.greater(bkgd, thres)\n",
    "        sig_rates.append(np.sum(sig_selected)/sig_count)\n",
    "        bkgd_rates.append(np.sum(bkgd_selected)/bkgd_count)\n",
    "    return sig_rates, bkgd_rates, thres_list\n",
    "\n",
    "def plot_simple_ROC_SIC(sig, bkgd, step = 0.001):\n",
    "    \n",
    "    # Calculate simple ROC curve\n",
    "    sig_rates, bkgd_rates, thres_list = simple_ROC(sig, bkgd, step)\n",
    "    \n",
    "\n",
    "    # Plotting ROC\n",
    "    plt.rcParams['figure.figsize'] = [6, 6]\n",
    "    plt.plot(sig_rates, bkgd_rates,label='bkgd',linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"signal rate\")\n",
    "    plt.ylabel(\"background rate\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting SIC (ε/√ε_b) \n",
    "    event_significances = sig_rates/np.sqrt(bkgd_rates)\n",
    "    plt.rcParams['figure.figsize'] = [5, 4]\n",
    "    plt.plot(sig_rates,event_significances,label='beta3',linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"signal rate\")\n",
    "    plt.ylabel(\"significance improvement\")\n",
    "    plt.ylim(top=2.5)\n",
    "    plt.ylim(bottom=1)\n",
    "    plt.xlim(right=1)  # adjust the right leaving left unchanged\n",
    "    plt.xlim(left=0.1) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here evaluate ML performance by e.g.\n",
    "#r_combine_y,r_combine_x = generate_real_SIC(y_test,new_model_combine.predict([x_test,x_test_r]),mass_test,quality=1)\n",
    "\n",
    "r_full_y,r_full_x = generateSIC(y_test,predicted, quality=100)\n",
    "#r_fine_y,r_fine_x = generate_real_SIC(y_test,model_fine.predict(x_test_r),mass_test,quality=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "#plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#FRANK# event image SIC\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "plt.plot(sig_rates,significances,label='Beta3',linewidth=1)\n",
    "#FRANK# beta-3 SIC\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Keras",
   "language": "python",
   "name": "python_3_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
