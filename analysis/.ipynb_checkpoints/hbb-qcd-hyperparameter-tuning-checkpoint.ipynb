{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "This program loads in .npy files produced by file-processing.ipynb and performs CNN classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing TF and Keras\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "# Import ray tune (hyperparameter tuning)\n",
    "import ray\n",
    "from ray import tune        \n",
    "from ray.tune.util import pin_in_object_store #, get_pinned_object\n",
    "\n",
    "# Clear logs from previous runs\n",
    "!rm -rf ./tensorboard-logs/\n",
    "\n",
    "# Notes ############################################################################\n",
    "#\n",
    "# 1.\n",
    "#     Normal tf import is disabled to solve ray.tune pickling error\n",
    "#     import tensorflow as tf \n",
    "# 2.\n",
    "#     To automatically view training result with tensorboard, use \n",
    "#         tensorboard --logdir ~/ray_results\n",
    "# 3.\n",
    "#     Prints the devices in use, deleted since global tf import is disabled\n",
    "#     from tensorflow.python.client import device_lib\n",
    "#         print(device_lib.list_local_devices())\n",
    "#         print(\"Tensorflow version is\")\n",
    "#         print(tf.__version__)\n",
    "#         print(\"Keras version is\")\n",
    "#         print(keras.__version__)\n",
    "# 4.\n",
    "#     To use only GPU3, run commands using:\n",
    "#         with tf.device('/device:XLA_GPU:3'):\n",
    "# 5.\n",
    "#     To use tensorboard, enable extension and add logging folder production:\n",
    "#         import datetime\n",
    "#         %load_ext tensorboard\n",
    "#         log_dir=\"tensorboard-logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# Importing Keras \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.io_utils import HDF5Matrix # NECESSARY\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Concatenate\n",
    "print('Keras version: ' + keras.__version__)\n",
    "\n",
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import matplotlib\n",
    "import time\n",
    "import pickle \n",
    "\n",
    "# Import HDF5 to use disk for large dataset\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "# Import local libraries\n",
    "from substructure import * # Jet substructure variables\n",
    "import save_and_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     38,
     49,
     89,
     103,
     117
    ]
   },
   "outputs": [],
   "source": [
    "def generate_real_SIC(expect,predict,masses,quality=1,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten(),masses)).transpose(), key=lambda x: x[1])),0)\n",
    "    efficiency = []; signal_eff = [];\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort),quality*10):\n",
    "        background_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==0],bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==1],bins=np.arange(50,197,7))\n",
    "        log_likelihood = 0; baseLL = 0; \n",
    "        signal_eff.append(np.sum(to_sort[:i+1,0])/total_signal)\n",
    "        test_LL_vs_SS = []\n",
    "        for signal_strength in np.arange(1,5,quality/1000):\n",
    "            log_likelihood = 0;\n",
    "            for k in range(len(bins)-1):\n",
    "                expected = background_weight*background_mass_binned[k]+signal_weight*signal_strength*signal_mass_binned[k]\n",
    "                observed = background_weight*background_mass_binned[k]+signal_weight*signal_mass_binned[k]\n",
    "                if (expected <= 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "            if signal_strength == 1:\n",
    "                baseLL = log_likelihood\n",
    "            test_LL_vs_SS.append(log_likelihood)\n",
    "            if log_likelihood < baseLL-1/2:\n",
    "                efficiency.append(1/(signal_strength-1))\n",
    "                break\n",
    "            if signal_strength > 3:\n",
    "                efficiency.append(1/2)\n",
    "                break\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"base efficiency : \" + str(float(efficiency[-1])))\n",
    "    efficiency = np.array(efficiency)/float(efficiency[-1])\n",
    "    max_eff = np.max(efficiency)\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def find_highest_SIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)): # generate a int range scanning over 95% of all samples??\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    return(max_eff)\n",
    "\n",
    "def find_highest_SIC_binned(expect,predict,masses):\n",
    "    bins = np.arange(50,197,7)\n",
    "    efficiency = []\n",
    "    def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "        log_likelihood = 0\n",
    "        background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "        for i in range(len(bins)-1):\n",
    "            expected = background_weight*background_mass_binned[i]+signal_weight*signal_strength*signal_mass_binned[i]\n",
    "            observed = background_weight*background_mass_binned[i]+signal_weight*signal_mass_binned[i]\n",
    "            if (expected <= 0):\n",
    "                return float(\"inf\")\n",
    "            log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "        return -log_likelihood\n",
    "    \n",
    "    sigmas = []\n",
    "    for i in np.arange(0,0.9,0.05):\n",
    "        #res = least_squares(lambda x : log_like(x,masses[np.logical_and(predict.flatten() >= i,expect == 0)],\n",
    "        #                                        masses[np.logical_and(predict.flatten() >= i,expect == 1)]),x0=1)\n",
    "\n",
    "        #i is the cut on the machine learning\n",
    "        kept_back = masses[np.logical_and(predict.flatten() >= i,expect == 0)]\n",
    "        kept_signal = masses[np.logical_and(predict.flatten() >= i,expect == 1)]\n",
    "        j_array = []\n",
    "        for j in np.arange(1,25,0.5):\n",
    "            #print log_like(j,kept_back,kept_signal)\n",
    "            if log_like(j,kept_back,kept_signal) > log_like(1,kept_back,kept_signal)+0.5:\n",
    "                j_array.append(j-1)\n",
    "                break\n",
    "            if j >20:\n",
    "                j_array.append(20)\n",
    "                break\n",
    "        print(j_array,i)\n",
    "        sigmas.append(1/np.min(j_array))\n",
    "        \n",
    "    max_eff = np.max(sigmas)\n",
    "    return(max_eff)\n",
    "\n",
    "def generateSIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []; signal_eff = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)):\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        signal_eff.append(signal_eff_temp)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "    log_likelihood = 0\n",
    "    background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "    signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weight*background_mass_binned[i]+signal_weight*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weight*background_mass_binned[i]+signal_weight*signal_mass_binned[i]\n",
    "        if (expected <= 0):\n",
    "            return float(\"inf\")\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return -log_likelihood\n",
    "\n",
    "def log_like(signal_strength):\n",
    "    log_likelihood = 0\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weight*background_mass_binned[i]+signal_weight*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weight*background_mass_binned[i]+signal_weight*signal_mass_binned[i]\n",
    "        #print(expected)\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2,
     7,
     10,
     24
    ]
   },
   "outputs": [],
   "source": [
    "#FRANK# Returns a function that generates a padding layer\n",
    "#FRANK# x is the detector image, of following format:\n",
    "#FRANK# [index, image#(charged and neutral pt and multiplicity), 40, 40]\n",
    "def return_pad_me(padding):\n",
    "    def pad_me(x):\n",
    "        import tensorflow as tf # This solves 'TypeError: can't pickle _LazyLoader objects'\n",
    "        #FRANK# x[:,:,:y,:] slice x off from y at the given axis.\n",
    "        return(tf.concat((x,x[:,:,:padding,:]),2))\n",
    "    return(pad_me)\n",
    "\n",
    "def pad_out(padding,input_shape):\n",
    "    return input_shape\n",
    "\n",
    "class gen_call(Callback):\n",
    "    \n",
    "    def __init__(self, test_data):\n",
    "        self.x, self.y = test_data\n",
    "    \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.highest_SIC_train = []\n",
    "        self.highest_SIC_test = []\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        self.highest_SIC_test.append(find_highest_SIC(self.y,y_pred))\n",
    "        print(str(self.highest_SIC_test[-1]) + \" is how good\")\n",
    "\n",
    "def show_outputs(output):\n",
    "    #Assumes the output is in shape like (32,41,36)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for i in range(1,1+output.shape[0]):\n",
    "        fig.add_subplot(4,output.shape[0]/4,i)\n",
    "        plt.imshow(10*output[i-1,:,:])\n",
    "        plt.axis('off')\n",
    "    #plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# this method is the exact replica of find_highest_SIC() but operates on tensors.\n",
    "#FRANK# for some reason, pred has different format from expect. \n",
    "def highest_SIC_metric(y_true,y_pred):\n",
    "    import tensorflow as tf # This solves 'TypeError: can't pickle _LazyLoader objects'\n",
    "    print('highest_SIC_metric() is called')\n",
    "    y_true = tf.keras.backend.flatten(y_true) #FRANK# flattens to 1D\n",
    "    y_pred = tf.keras.backend.flatten(y_pred) #FRANK# flattens to 1D\n",
    "    \n",
    "    #stacked = tf.transpose(tf.stack((expect,predict)))\n",
    "    #to_sort = tf.reverse(sorted(stacked, key=lambda x: x[1]),0) #FRANK# sorted is wrong!!\n",
    "    total_sample = tf.cast(tf.size(y_pred), tf.float32)  # count total nums, then cast to float32 to avoid issue\n",
    "    total_signal = tf.cast(tf.reduce_sum(y_true), tf.float32)  # summing across all predicted (where sigs are 1's and bkgs are 0's) to get total num of signal. \n",
    "    total_background = tf.cast(tf.subtract(total_sample, total_signal), tf.float32)  # subtracting signal countss from total size to get background counts\n",
    "    \n",
    "    \n",
    "    # original mechanism:\n",
    "    # 1. sort by ML score\n",
    "    # 2. sum all actuals before an index\n",
    "    # 3. count all 0's before an index \n",
    "    \n",
    "    sorted_indices = tf.argsort(y_pred,axis=-1,direction='ASCENDING') # tf.argsort: Returns the indices of a tensor that give its sorted order along an axis.\n",
    "    sorted_sigs = tf.gather(y_pred,sorted_indices)\n",
    "    # return the indices of prediction in ascending order. By reading these indices, you can access corresponding expected y's.\n",
    "    ones = tf.fill(tf.shape(sorted_sigs), 1.0)\n",
    "    sorted_bkgs = tf.subtract(ones, sorted_sigs)\n",
    "    # make a sorted tensor where 1's are bkgs and 0's are sigs\n",
    "    \n",
    "    sig_cum_sums = tf.cast(tf.cumsum(sorted_sigs), tf.float32) # return the integrated signal number from 0 to each index\n",
    "    bkg_cum_sums = tf.cast(tf.cumsum(sorted_bkgs), tf.float32) # return the integrated bkg number from 0 to each index\n",
    "\n",
    "    sig_effs = tf.divide(sig_cum_sums, total_signal)\n",
    "    bkg_effs = tf.divide(bkg_cum_sums, total_background) #FRANK# total_background might be 0, causing bkg_effs=inf\n",
    "    effs = tf.divide(sig_effs, tf.sqrt(bkg_effs))\n",
    "        \n",
    "    return tf.reduce_max(effs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event image hyperparameter tuning\n",
    "\n",
    "This section tunes the learning rate, epoch # and batch size of event image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tune callback for Keras\n",
    "class TuneReporterCallback(Callback):\n",
    "\n",
    "    def __init__(self, logs={}):\n",
    "        self.iteration = 0\n",
    "        super(TuneReporterCallback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.iteration += 1\n",
    "        tune.track.log(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"), mean_loss=logs.get(\"loss\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "# First define the global parameter input_shape - dimension of event images. \n",
    "# It is supposed to be a tuple.\n",
    "# This parameter is stored in every event image hdf5 files. \n",
    "f = h5py.File('event-image-hdf5/train.hdf5', 'r')\n",
    "input_shape = np.zeros(3, dtype='int32')\n",
    "dset = f['input_shape']\n",
    "dset.read_direct(input_shape)\n",
    "input_shape=tuple(input_shape)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 22th version\n",
    "# Parameters to tune:\n",
    "#     Learning rate;\n",
    "#     Batch size;\n",
    "\n",
    "# Tensorboard callback is disabled since it is included in tune\n",
    "#    import tensorflow as tf # This solves 'TypeError: can't pickle _LazyLoader objects'\n",
    "#    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Create event image model automatically with tunable hyperparameter\n",
    "def create_event_image_model(lr):\n",
    "    import tensorflow as tf # This solves 'TypeError: can't pickle _LazyLoader objects'\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # Event image model\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    event_image_cnn = Sequential()\n",
    "    event_image_cnn.add(Lambda(return_pad_me(4),\n",
    "                     input_shape=input_shape))\n",
    "    event_image_cnn.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                     activation='relu',\n",
    "                    data_format='channels_first'))\n",
    "    event_image_cnn.add(Lambda(return_pad_me(1),\n",
    "                     input_shape=input_shape))\n",
    "    event_image_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "    event_image_cnn.add(Lambda(return_pad_me(4),\n",
    "                     input_shape=input_shape))\n",
    "    event_image_cnn.add(Conv2D(64, (5, 5), \n",
    "                     activation='relu',\n",
    "                     data_format='channels_first'))\n",
    "    event_image_cnn.add(Lambda(return_pad_me(1),\n",
    "                     input_shape=input_shape))\n",
    "    event_image_cnn.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "    event_image_cnn.add(Flatten())\n",
    "    event_image_cnn.add(Dense(1000, activation='relu'))\n",
    "    event_image_cnn.add(Dense(1, activation='sigmoid'))\n",
    "    event_image_cnn.summary()\n",
    "\n",
    "    #model_opt = keras.optimizers.Adadelta(lr=2.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model_opt = keras.optimizers.Adadelta(lr=lr) # learning rate and decay rates are tunable\n",
    "\n",
    "    event_image_cnn.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=model_opt,\n",
    "                  metrics=['accuracy', highest_SIC_metric])\n",
    "    return event_image_cnn\n",
    "\n",
    "# Create training call for Tune using EVENT IMAGE\n",
    "# config should contain keys:\n",
    "#     'lr': learning rate\n",
    "#     'epoch': epoch number\n",
    "#     'batch': batch size\n",
    "def tune_event_image(config):\n",
    "    model = create_event_image_model(lr=config['lr'])\n",
    "    # This saves the top model. `accuracy` is only available in TF2.0.\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        \"model.h5\", monitor='accuracy', save_best_only=True) #, save_freq=2)\n",
    "    \n",
    "    folder_id = str(numpy.random.randint())\n",
    "    \n",
    "    import tensorflow as tf # This solves 'TypeError: can't pickle _LazyLoader objects'\n",
    "    # Train the model\n",
    "    with tf.device('/device:XLA_GPU:3'):        \n",
    "        x_train = HDF5Matrix('/home/ffu/higgs-classifier/analysis/event-image-hdf5/'++'/train.hdf5', 'x_train')\n",
    "        y_train = HDF5Matrix('/home/ffu/higgs-classifier/analysis/event-image-hdf5/'++'/train.hdf5', 'y_train')\n",
    "        x_test = HDF5Matrix('/home/ffu/higgs-classifier/analysis/event-image-hdf5/'++'/test.hdf5', 'x_test')\n",
    "        y_test = HDF5Matrix('/home/ffu/higgs-classifier/analysis/event-image-hdf5/'++'/test.hdf5', 'y_test')\n",
    "        model.fit(\n",
    "            x_train, \n",
    "            y_train, \n",
    "            validation_data=(\n",
    "                x_test, \n",
    "                y_test),\n",
    "            verbose=0, batch_size=config['batch'], epochs=config['epoch'], callbacks=[checkpoint_callback, TuneReporterCallback()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyperparameter space for random search\n",
    "hyperparameter_space = {\n",
    "    'lr': tune.sample_from(lambda spec: 10**np.random.uniform(low=-5.0, high=-1.0)), # range should be 0.00001 to 0.1, log-like.\n",
    "    'batch': tune.sample_from(lambda spec: int(np.random.uniform(low=50, high=1000))), # range should be 50 to 1000?\n",
    "    'epoch': tune.sample_from(lambda spec: int(np.random.uniform(low=100, high=500))) # range should be 100 to 500?\n",
    "}\n",
    "\n",
    "# Config trial number\n",
    "num_samples = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 53.4/94.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/17.48 GiB heap, 0.0/6.01 GiB objects<br>Result logdir: /home/ffu/ray_results/tune_event_image<br>Number of trials: 50 (50 ERROR)<br>Table truncated to 20 rows. 30 trials (30 ERROR) not shown.<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th><th>epoch  </th><th>batch  </th><th>lr  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_event_image_6ead02bc</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6eaca3c6</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6eac3f3a</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6eab8298</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6eab0b7e</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea98b82</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea9296c</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea854f6</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea7fbdc</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea79e58</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea56278</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea44032</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea3e916</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea38af2</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea32cd8</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea2cf54</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea268b6</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea20dbc</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea1b182</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "<tr><td>tune_event_image_6ea13414</td><td>ERROR   </td><td>     </td><td>       </td><td>       </td><td>    </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 50<br>Table truncated to 20 rows (30 overflow)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_event_image_6e94db10</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e94db10_2020-01-23_16-35-34rvrq80fg/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e953dda</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e953dda_2020-01-23_16-35-340gtfzke7/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e959e88</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e959e88_2020-01-23_16-35-34ifb824rh/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e96324e</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e96324e_2020-01-23_16-35-34yx8nc_9h/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e968eec</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e968eec_2020-01-23_16-35-34owcue536/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e96ea90</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e96ea90_2020-01-23_16-35-34yligi0kq/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e975322</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e975322_2020-01-23_16-35-34bbjo13fa/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e97ad22</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e97ad22_2020-01-23_16-35-343pocbm2j/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e980b78</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e980b78_2020-01-23_16-35-34qa7wwoe5/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e986d98</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e986d98_2020-01-23_16-35-3420giqn2v/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e98ccc0</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e98ccc0_2020-01-23_16-35-34uz6mdohp/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9936ce</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9936ce_2020-01-23_16-35-34erurnnne/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9994ca</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9994ca_2020-01-23_16-35-34m07q1qi_/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9aa4b4</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9aa4b4_2020-01-23_16-35-34xbcruum7/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9b0ad0</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9b0ad0_2020-01-23_16-35-34dege0xme/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9b6a7a</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9b6a7a_2020-01-23_16-35-34lumq2tll/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9bc9fc</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9bc9fc_2020-01-23_16-35-3817cyywk8/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9c4a26</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9c4a26_2020-01-23_16-35-38p6q9bnt1/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9ca6e2</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9ca6e2_2020-01-23_16-35-387_sxsack/error.txt</td></tr>\n",
       "<tr><td>tune_event_image_6e9d029a</td><td style=\"text-align: right;\">           1</td><td>/home/ffu/ray_results/tune_event_image/tune_event_image_6e9d029a_2020-01-23_16-35-381yra9tjn/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [tune_event_image_6e94db10, tune_event_image_6e953dda, tune_event_image_6e959e88, tune_event_image_6e96324e, tune_event_image_6e968eec, tune_event_image_6e96ea90, tune_event_image_6e975322, tune_event_image_6e97ad22, tune_event_image_6e980b78, tune_event_image_6e986d98, tune_event_image_6e98ccc0, tune_event_image_6e9936ce, tune_event_image_6e9994ca, tune_event_image_6e9aa4b4, tune_event_image_6e9b0ad0, tune_event_image_6e9b6a7a, tune_event_image_6e9bc9fc, tune_event_image_6e9c4a26, tune_event_image_6e9ca6e2, tune_event_image_6e9d029a, tune_event_image_6e9d5db2, tune_event_image_6e9dbec4, tune_event_image_6e9e26b6, tune_event_image_6e9e8548, tune_event_image_6e9ee722, tune_event_image_6e9f429e, tune_event_image_6e9fa5c2, tune_event_image_6ea00206, tune_event_image_6ea05fbc, tune_event_image_6ea0c9b6, tune_event_image_6ea13414, tune_event_image_6ea1b182, tune_event_image_6ea20dbc, tune_event_image_6ea268b6, tune_event_image_6ea2cf54, tune_event_image_6ea32cd8, tune_event_image_6ea38af2, tune_event_image_6ea3e916, tune_event_image_6ea44032, tune_event_image_6ea56278, tune_event_image_6ea79e58, tune_event_image_6ea7fbdc, tune_event_image_6ea854f6, tune_event_image_6ea9296c, tune_event_image_6ea98b82, tune_event_image_6eab0b7e, tune_event_image_6eab8298, tune_event_image_6eac3f3a, tune_event_image_6eaca3c6, tune_event_image_6ead02bc])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bf58693f0163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     num_samples=num_samples)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/python_3_keras/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [tune_event_image_6e94db10, tune_event_image_6e953dda, tune_event_image_6e959e88, tune_event_image_6e96324e, tune_event_image_6e968eec, tune_event_image_6e96ea90, tune_event_image_6e975322, tune_event_image_6e97ad22, tune_event_image_6e980b78, tune_event_image_6e986d98, tune_event_image_6e98ccc0, tune_event_image_6e9936ce, tune_event_image_6e9994ca, tune_event_image_6e9aa4b4, tune_event_image_6e9b0ad0, tune_event_image_6e9b6a7a, tune_event_image_6e9bc9fc, tune_event_image_6e9c4a26, tune_event_image_6e9ca6e2, tune_event_image_6e9d029a, tune_event_image_6e9d5db2, tune_event_image_6e9dbec4, tune_event_image_6e9e26b6, tune_event_image_6e9e8548, tune_event_image_6e9ee722, tune_event_image_6e9f429e, tune_event_image_6e9fa5c2, tune_event_image_6ea00206, tune_event_image_6ea05fbc, tune_event_image_6ea0c9b6, tune_event_image_6ea13414, tune_event_image_6ea1b182, tune_event_image_6ea20dbc, tune_event_image_6ea268b6, tune_event_image_6ea2cf54, tune_event_image_6ea32cd8, tune_event_image_6ea38af2, tune_event_image_6ea3e916, tune_event_image_6ea44032, tune_event_image_6ea56278, tune_event_image_6ea79e58, tune_event_image_6ea7fbdc, tune_event_image_6ea854f6, tune_event_image_6ea9296c, tune_event_image_6ea98b82, tune_event_image_6eab0b7e, tune_event_image_6eab8298, tune_event_image_6eac3f3a, tune_event_image_6eaca3c6, tune_event_image_6ead02bc])"
     ]
    }
   ],
   "source": [
    "# Initialize Ray with memory size 3000000000, since train and test's total size is 2655952088\n",
    "np.random.seed(5) # Set random seed\n",
    "ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "ray.init(num_gpus=1)  # memory=3000000000, object_store_memory=3000000000,\n",
    "! rm -rf ~/ray_results/tune_event_image # Clean previous logs\n",
    "\n",
    "# Pinning object to resolve 'Connection reset by peer'. Not used after switching to HDF5. \n",
    "# x_train = pin_in_object_store(x_train)\n",
    "# y_train = pin_in_object_store(y_train)\n",
    "# x_test = pin_in_object_store(x_test)\n",
    "# y_test = pin_in_object_store(y_test)\n",
    "\n",
    "# loading dataset in the form of hdf5 dataset. \n",
    "# MIGHT NOT WORK WITH TUNE's MULTITHREADING FEATURE. TESTING AS OF 3:27, Jan 23\n",
    "\n",
    "# Run tuning\n",
    "# run with GPU3 is disabled\n",
    "# with tf.device('/device:XLA_GPU:3'):\n",
    "analysis = tune.run(\n",
    "    tune_event_image, \n",
    "    verbose=1, \n",
    "    config=hyperparameter_space,\n",
    "    num_samples=num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models (Example, please do not use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Jet image model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "jet_image_cnn = Sequential()\n",
    "jet_image_cnn.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first',input_shape=input_shape))\n",
    "#jet_image_cnn.add(Dropout(0.5))\n",
    "jet_image_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "jet_image_cnn.add(Conv2D(64, (5, 5), activation='relu',data_format='channels_first'))\n",
    "#jet_image_cnn.add(Dropout(0.5))\n",
    "jet_image_cnn.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "jet_image_cnn.add(Flatten())\n",
    "jet_image_cnn.add(Dense(1000, activation='relu'))\n",
    "#jet_image_cnn.add(Dropout(0.2))\n",
    "jet_image_cnn.add(Dense(1, activation='sigmoid'))\n",
    "#jet_image_cnn.summary()\n",
    "\n",
    "model_opt = keras.optimizers.Adadelta()\n",
    "\n",
    "jet_image_cnn.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=model_opt,\n",
    "              metrics=['accuracy', 'highest_SIC_test'])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Combined model\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#FRANK# input shape is calculated during splitting:\n",
    "#FRANK# splitData(background_image_list,signal_image_list)\n",
    "#FRANK# how is jet image taken?\n",
    "event_image_branch = Sequential()\n",
    "\n",
    "#FRANK# Lambda is from Keras.\n",
    "event_image_branch.add(Lambda(return_pad_me(5), #FRANK# tf.concat((x,x[:,:,:5,:]),2). Concatenate means pasting together.\n",
    "                            #FRANK# this layer slices off an edge of the tensor and paste it on the other side.\n",
    "                 input_shape=input_shape))\n",
    "event_image_branch.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first'))\n",
    "event_image_branch.add(Lambda(return_pad_me(2)))\n",
    "event_image_branch.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "event_image_branch.add(Lambda(return_pad_me(5),\n",
    "                 input_shape=input_shape))\n",
    "event_image_branch.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#event_image_branch.add(Dropout(0.5))\n",
    "event_image_branch.add(Lambda(return_pad_me(2)))\n",
    "event_image_branch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "event_image_branch.add(Flatten())\n",
    "event_image_branch.add(Dense(300,activation='relu'))\n",
    "#event_image_branch.add(Dropout(0.5))\n",
    "\n",
    "jet_image_branch = Sequential()\n",
    "jet_image_branch.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                      kernel_initializer='random_uniform',input_shape=input_shape_r, data_format=\"channels_first\")) #FRANK# problemmatic. see debug note #1\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "jet_image_branch.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "jet_image_branch.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "jet_image_branch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "jet_image_branch.add(Flatten())\n",
    "jet_image_branch.add(Dense(300, activation='relu'))\n",
    "#jet_image_branch.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "#FRANK# Merge layer is no longer supported.  \n",
    "#combined_model = Sequential()\n",
    "#combined_model.add(Merge([event_image_branch, jet_image_branch], mode = 'concat'))\n",
    "#combined_model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#FRANK# these are not models, but tensors. Model.input or Model.output are tensor pointers I think.\n",
    "combined_model_tensor = Concatenate(axis=-1)([event_image_branch.output, jet_image_branch.output])   \n",
    "combined_model_tensor = Dense(1,activation='sigmoid')(combined_model_tensor)\n",
    "#FRANK# what you need to do is merging output of 300 and 300 into one.\n",
    "\n",
    "#combined_model.summary()\n",
    "combined_model = Model([event_image_branch.input,jet_image_branch.input], combined_model_tensor)\n",
    "combined_model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', highest_SIC_metric])\n",
    "\n",
    "# Visualizing event image model\n",
    "plot_model(combined_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Tensorboard (run without \"%\" in terminal)\n",
    "%tensorboard --logdir tensorboard-logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#FRANK# Sample training call testing metric\n",
    "epochs = 100\n",
    "print(y_train)\n",
    "with tf.device('/device:XLA_GPU:3'):\n",
    "    history_event_only = event_image_cnn.fit(\n",
    "        x_train, \n",
    "        y_train,\n",
    "        batch_size=1024,#batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val,y_val),\n",
    "        verbose=1, shuffle=True, callbacks=[\n",
    "            tensorboard_callback, \n",
    "            EarlyStopping(monitor='highest_SIC_test', patience=2), \n",
    "            gen_call((x_val_cut,y_val_cut))])\n",
    "\n",
    "#FRANK# for event image SIC: predicting\n",
    "with tf.device('/device:XLA_GPU:3'):\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    \n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# testing metric\n",
    "print(np.array(y_pred.flatten()))\n",
    "print(np.array(y_test))\n",
    "print(highest_SIC_metric(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# training history plots\n",
    "def loss_history_plot(training_history):\n",
    "    print(training_history.history.keys())\n",
    "    plt.plot(training_history.history['loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print groomed mass histogram\n",
    "plt.rcParams['figure.figsize'] = [7.5,6]\n",
    "\n",
    "hist_bkg, bin_edges_bkg = np.histogram(background_mass_list, bins=21, range = (50, 197))\n",
    "bin_centers_bkg = (bin_edges_bkg[:-1] + bin_edges_bkg[1:]) / 2\n",
    "hist_sig, bin_edges_bkg = np.histogram(signal_mass_list, bins=21, range = (50, 197))\n",
    "\n",
    "plt.step(bin_centers_bkg, hist_bkg*background_weight, label='bkgd')\n",
    "plt.step(bin_centers_bkg, hist_sig*signal_weight, label='sig')\n",
    "plt.legend()\n",
    "plt.ylim(top=10000)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(right=197)  # adjust the right leaving left unchanged\n",
    "plt.xlim(left=50) \n",
    "plt.show()\n",
    "#log_like(1,background_mass_list=background_mass_list,signal_mass_list=signal_mass_list)\n",
    "\n",
    "# Calculating beta3\n",
    "background_beta3 = find_new_var_beta_3(background_reclustered,background_event_list_clustered,pt_cut = 1)\n",
    "signal_beta3 = find_new_var_beta_3(signal_reclustered,signal_event_list_clustered,pt_cut = 1)\n",
    "\n",
    "# Beta-3 histogram\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 4]\n",
    "beta3_min = min([min(background_beta3),min(signal_beta3)])\n",
    "beta3_max = max([max(background_beta3),max(signal_beta3)])\n",
    "print('beta3 range is '+str((beta3_min, beta3_max)))\n",
    "plt.hist(background_beta3, label='bkgd', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_beta3, label='sig', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Beta-3\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(background_mass_list, label='bkgd', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_mass_list, label='sig', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Trimmed Mass\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Beta-3 plots\n",
    "plot_simple_ROC_SIC(signal_beta3, background_beta3, step = 0.001):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the predicted results into sample and background using known answers.\n",
    "# Here we assume all backgrounds are put before signals in all datasets. \n",
    "#TODO# Check before use!\n",
    "\n",
    "def split_sig_bkgd(predicted, expected)\n",
    "    split_location = int(np.argwhere(expected==1.)[0])\n",
    "    #FRANK# signal predictions with event image model\n",
    "    sig = predicted[split_location:]\n",
    "    bkgd = predicted[:split_location]\n",
    "\n",
    "    return sig, bkgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# calculating true positive rates and false positive rates. \n",
    "#FRANK# TPR = TP/P\n",
    "#FRANK# FPR = FP/N\n",
    "#FRANK# note that bkgds have lower beta in general, so the cut should be \n",
    "#FRANK# beta3 > threshold.\n",
    "#FRANK# 'where' usage referred from \n",
    "#FRANK# https://stackoverflow.com/questions/12995937/count-all-values-in-a-matrix-greater-than-a-value\n",
    "\n",
    "def simple_ROC(sig, bkgd, step = 1):\n",
    "    thres_min = min([min(sig),min(bkgd)])\n",
    "    thres_max = max([max(sig),max(bkgd)])\n",
    "    search_range=(thres_min, thres_max)\n",
    "    sig_count = len(sig)\n",
    "    bkgd_count = len(bkgd)\n",
    "    sig_rates = []\n",
    "    bkgd_rates = []\n",
    "    thres_list = np.arange(search_range[0], search_range[1], step)\n",
    "    for thres in thres_list:\n",
    "        sig_selected = np.greater(sig, thres)\n",
    "        bkgd_selected = np.greater(bkgd, thres)\n",
    "        sig_rates.append(np.sum(sig_selected)/sig_count)\n",
    "        bkgd_rates.append(np.sum(bkgd_selected)/bkgd_count)\n",
    "    return sig_rates, bkgd_rates, thres_list\n",
    "\n",
    "def plot_simple_ROC_SIC(sig, bkgd, step = 0.001):\n",
    "    \n",
    "    # Calculate simple ROC curve\n",
    "    sig_rates, bkgd_rates, thres_list = simple_ROC(sig, bkgd, step)\n",
    "    \n",
    "\n",
    "    # Plotting ROC\n",
    "    plt.rcParams['figure.figsize'] = [6, 6]\n",
    "    plt.plot(sig_rates, bkgd_rates,label='bkgd',linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"signal rate\")\n",
    "    plt.ylabel(\"background rate\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting SIC (/_b) \n",
    "    event_significances = sig_rates/np.sqrt(bkgd_rates)\n",
    "    plt.rcParams['figure.figsize'] = [5, 4]\n",
    "    plt.plot(sig_rates,event_significances,label='beta3',linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"signal rate\")\n",
    "    plt.ylabel(\"significance improvement\")\n",
    "    plt.ylim(top=2.5)\n",
    "    plt.ylim(bottom=1)\n",
    "    plt.xlim(right=1)  # adjust the right leaving left unchanged\n",
    "    plt.xlim(left=0.1) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here evaluate ML performance by e.g.\n",
    "#r_combine_y,r_combine_x = generate_real_SIC(y_test,new_model_combine.predict([x_test,x_test_r]),mass_test,quality=1)\n",
    "\n",
    "r_full_y,r_full_x = generateSIC(y_test,predicted, quality=100)\n",
    "#r_fine_y,r_fine_x = generate_real_SIC(y_test,model_fine.predict(x_test_r),mass_test,quality=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "#plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#FRANK# event image SIC\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "plt.plot(sig_rates,significances,label='Beta3',linewidth=1)\n",
    "#FRANK# beta-3 SIC\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Keras",
   "language": "python",
   "name": "python_3_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
