{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#CODE TO INITIALISE TENSORFLOW\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "#set_session(tf.Session(config=config))\n",
    "print(\"Success!\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for i in range(4):\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[i],\n",
    "        tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024))\n",
    "\n",
    "\n",
    "#Prints the devices in use. The next time I open this, should be configured to only use GPU 3\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version is\n",
      "2.0.0\n",
      "Keras version is\n",
      "2.3.1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15428302617090920831\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1534414862458633519\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4763337767908064604\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8257738171104679438\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3129559460842053361\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:3\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2514836943740671368\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "Tensorflow version is\n",
      "2.0.0\n",
      "Keras version is\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "#CODE TO INITIALISE TENSORFLOW\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "\n",
    "#FRANK# view versions\n",
    "print(\"Tensorflow version is\")\n",
    "print(tf.__version__)\n",
    "print(\"Keras version is\")\n",
    "print(keras.__version__)\n",
    "\n",
    "#FRANK# these lines limiting memory usages are for tf2, which requires cudatoolkit>10 and newer driver\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#print(gpus)\n",
    "#for i in range(4):   \n",
    "#    tf.config.experimental.set_virtual_device_configuration(gpus[i],\n",
    "#        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5860500000)])\n",
    "\n",
    "#config = tf.ConfigProto(log_device_placement=True)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "#sess = tf.Session(config=config)\n",
    "#set_session(sess)\n",
    "#print(\"Success!\")\n",
    "\n",
    "import pickle\n",
    "#Prints the devices in use. The next time I open this, should be configured to only use GPU 3\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "tf.device('/device:GPU:3')\n",
    "print(\"Tensorflow version is\")\n",
    "print(tf.__version__)\n",
    "print(\"Keras version is\")\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#IMPORTS \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "#FRANK# Merge is no longer a valid layer! replacing with Concatenate instead.,\n",
    "#from keras.layers import MergeConcatenate\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import matplotlib\n",
    "import time\n",
    "\n",
    "from itertools import groupby\n",
    "#FRANK# StringIO is no longer suppeorted in Python3. using io instead.#\n",
    "#from StringIO import StringIO\n",
    "from io import StringIO\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import pyjet\n",
    "from pyjet import cluster, DTYPE_PTEPM\n",
    "from pyjet.testdata import get_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "\n",
    "width = 40\n",
    "height = 40 #width, height of pictures\n",
    "\n",
    "#FRANK# why do we need cross sec?\n",
    "backgroundCross = 2.048e-06 #cross-section of processes in millibarns\n",
    "\n",
    "actual_background_cross=2.84e-9 #barns\n",
    "average_number_accepted=2162\n",
    "\n",
    "actual_signal_cross = np.average([1.738e-14,1.7277e-14])\n",
    "signal_accepted = np.average([8708-189,8827-172])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     2,
     30,
     54,
     93,
     100,
     149,
     162,
     170,
     181,
     246,
     249,
     252,
     255,
     274,
     279,
     286
    ]
   },
   "outputs": [],
   "source": [
    "#FUNCTIONS NEEDED TO READ AND PROCESS DATA\n",
    "\n",
    "#FRANK# This function reads off the original CSV format: \n",
    "#FRANK# pt, eta, phi, m, id, isCharged \\n\n",
    "#FRANK# pt, eta, phi, m, id, isCharged \\n\n",
    "#FRANK# ... (of all particles in one event)\n",
    "#FRANK# trimmed mass of identified jet \\n\n",
    "#FRANK# \\n\n",
    "#FRANK# and produces ?\n",
    "\n",
    "def return_event_list(fileName,max_Read = float(\"inf\"),weighted=0,pt_cut=0):\n",
    "    \n",
    "    printed = 0\n",
    "    \n",
    "    event_list = [];mass_list = [];weight_list = [];\n",
    "    tmp_events = open(fileName).read().split(\"\\n\\n\")[:-1]\n",
    "    print(len(tmp_events))\n",
    "    for x in tmp_events:\n",
    "        try:\n",
    "            if len(event_list) == max_Read: #FRANK# limit event size. If violated, interrupt the entire method\n",
    "                return(event_list,mass_list)\n",
    "            if weighted == 0:\n",
    "                                \n",
    "                #FRANK# rfind finds the last occurance\n",
    "                #FRANK# [a:b] extract everything b/w a and bth elem, including ath and excluding bth\n",
    "                #FRANK# why isn't the cut applied to weight and mass list? #ISSUE#\n",
    "                \n",
    "                mass_list.append(float(x[x.rfind(\"\\n\")+1:-1]))\n",
    "                to_cut = np.array(np.genfromtxt(x[:x.rfind(\"\\n\")].splitlines(), delimiter=\",\"))\n",
    "                event_list.append(to_cut[[x[0] > pt_cut for x in to_cut]])\n",
    "            else:\n",
    "                weight_list.append(float(x[x.rfind(\"\\n\")+1:]))\n",
    "                mass_list.append(float(x[x.rfind(\"\\n\",0,x.rfind(\"\\n\")-1)+1:x.rfind(\"\\n\")+1]))\n",
    "                to_cut = np.genfromtxt(x[:x.rfind(\"\\n\")].splitlines(),delimiter=\",\")\n",
    "                event_list.append(to_cut[[x[0] > pt_cut for x in to_cut]])\n",
    "        except:\n",
    "            print('We failed to turn your CSV into an np array. Make sure you have the correct format. ')\n",
    "            print( sys.exc_info()[0])\n",
    "            return\n",
    "    if weighted == 0:\n",
    "        #print(mass_list[0])\n",
    "        return(event_list,mass_list)\n",
    "    else:\n",
    "        return(event_list,mass_list,weight_list)\n",
    "\n",
    "#FRANK# Converting event_list to event image\n",
    "def return_image_list(event_list):\n",
    "    image_list = []\n",
    "    image_0 = np.zeros((width,height)) #Charged pt #FRANK# I think it's labeled wrong here. Would it matter?\n",
    "    image_1 = np.zeros((width,height)) #Neutral pt\n",
    "    image_2 = np.zeros((width,height)) #Charged multiplicity\n",
    "    \n",
    "    #FRANK# pt, eta, phi, m, id, isCharged \\n\n",
    "    #FRANK# 0   1    2    3  4   5\n",
    "    for z in range(len(event_list)):\n",
    "        image_0 = np.zeros((width,height));image_1 = np.zeros((width,height));image_2 = np.zeros((width,height))\n",
    "        for x in range(len(event_list[z])):\n",
    "            phi_index = math.floor(width*event_list[z][x,2]//(2*math.pi)+width//2)\n",
    "            eta_index = math.floor(height*event_list[z][x,1]//10+height/2) #FRANK# // is integer divide\n",
    "            eta_index = min(eta_index,height-1)\n",
    "            eta_index = max(0,eta_index)\n",
    "            phi_index = int(phi_index);eta_index = int(eta_index)\n",
    "            if (event_list[z][x,5] == 0):  #FRANK# neutral\n",
    "                image_0[phi_index,eta_index] = image_0[phi_index,eta_index] + event_list[z][x,0]\n",
    "            elif (event_list[z][x,5] == 1):  #FRANK# charged\n",
    "                image_1[phi_index,eta_index] = image_1[phi_index,eta_index] + event_list[z][x,0]\n",
    "                image_2[phi_index,eta_index] = image_2[phi_index,eta_index] + 1\n",
    "        image_0 = np.divide(image_0,np.sum(image_0))\n",
    "        image_1 = np.divide(image_1,np.sum(image_1))\n",
    "        image_2 = np.divide(image_2,np.sum(image_2))\n",
    "        image_list.append(np.array([image_0,image_1,image_2]))\n",
    "    return(image_list)\n",
    "\n",
    "def load_events(event_type,debug = 0, max_Read = float(\"inf\"), max_Files = float(\"inf\"), weighted=0, \\\n",
    "                path = \"/data1/users/jzlin/NLO/\", contains = \"philback\", pt_cut = 0):\n",
    "    print(\"Loading events for \" + event_type)\n",
    "    reading_event_list,reading_mass_list = [],[]\n",
    "    reading_image_list = []\n",
    "    reading_weight_list = []\n",
    "    files_Read = 0\n",
    "    print('list of files is'+ str(os.listdir(path)))\n",
    "    for i in os.listdir(path):\n",
    "        print('test: i is: '+ str(i))\n",
    "        print('total path is: '+os.path.join(path,i))\n",
    "        \n",
    "        if (files_Read == max_Files):\n",
    "            break\n",
    "        if len(reading_event_list) >= max_Read:\n",
    "            return(reading_event_list,reading_mass_list,reading_image_list,files_Read)\n",
    "        if 'swp' in i:\n",
    "            continue\n",
    "        if os.path.isfile(os.path.join(path,i)) and (event_type+contains) in i:\n",
    "            if debug==1:\n",
    "                print(i)\n",
    "                print(os.path.join(path,i))\n",
    "            if weighted==0:\n",
    "                print(i)\n",
    "                temp_event_list,temp_mass_list = return_event_list(os.path.join(path,i),pt_cut=pt_cut)\n",
    "            else:\n",
    "                temp_event_list,temp_mass_list,temp_weight_list = return_event_list(os.path.join(path,i),\n",
    "                                                                                    weighted=1,pt_cut=pt_cut)\n",
    "                reading_weight_list = reading_weight_list = temp_weight_list\n",
    "            temp_image_list = return_image_list(temp_event_list)\n",
    "            if (len(temp_image_list) != len(temp_mass_list)):\n",
    "                print(\"Something has gone wrong when reading the file\")\n",
    "                print(os.path.join(path,i))\n",
    "            reading_event_list = reading_event_list + temp_event_list\n",
    "            reading_mass_list = reading_mass_list + temp_mass_list\n",
    "            reading_image_list = reading_image_list + temp_image_list\n",
    "            files_Read = files_Read + 1\n",
    "            print(\"Read \" + str(files_Read) + \" number of files\\r\")\n",
    "    if weighted==0:\n",
    "        return(reading_event_list,reading_mass_list,reading_image_list,files_Read)\n",
    "    else:\n",
    "        return(reading_event_list,reading_mass_list,reading_image_list,files_Read,reading_weight_list)\n",
    "    \n",
    "def fix_phi(phi):\n",
    "    while phi > math.pi:\n",
    "        phi = phi - 2*math.pi\n",
    "    while phi < -math.pi:\n",
    "        phi = phi + 2*math.pi\n",
    "    return phi\n",
    "\n",
    "def return_fine_image_list(event_list, event_list_clustered, granularity, which_jet = 0):\n",
    "    image_list = []\n",
    "    image_0 = np.zeros((width,height)) #Charged pt\n",
    "    image_1 = np.zeros((width,height)) #Neutral pt\n",
    "    image_2 = np.zeros((width,height)) #Charged multiplicity\n",
    "\n",
    "    for z in range(len(event_list)):\n",
    "        image_0 = np.zeros((width,height))\n",
    "        image_1 = np.zeros((width,height))\n",
    "        image_2 = np.zeros((width,height))\n",
    "        for x in range(len(event_list[z])):\n",
    "            \n",
    "            try:\n",
    "                phi_index = (event_list[z][x,2]-event_list_clustered[z][which_jet].phi)\n",
    "            except:\n",
    "                print(z)\n",
    "            #At this point, phi_index is just delta_phi, which could be anywhere from -2pi to 2pi\n",
    "            if (phi_index % (2*math.pi) >= (width//2)*granularity) and (phi_index % (2*math.pi) <= 2*math.pi-(width//2)*granularity):\n",
    "                continue\n",
    "                #This gets rid of the delta phi's that are far away from the jet\n",
    "            phi_index = phi_index % (2*math.pi)\n",
    "            if phi_index > math.pi:\n",
    "                 phi_index = phi_index - 2*math.pi   \n",
    "            phi_index = int(math.floor(phi_index/granularity)) #should be good now\n",
    "            if (phi_index > (width//2)) or (phi_index < -(width//2)):\n",
    "                print(phi_index)\n",
    "            phi_index = phi_index + (width//2)\n",
    "            \n",
    "\n",
    "            eta_index = int(math.floor((event_list[z][x,1]-event_list_clustered[z][which_jet].eta)/granularity) + height//2)\n",
    "            if eta_index >= height:\n",
    "                continue\n",
    "            if eta_index < 0:\n",
    "                continue\n",
    "            \n",
    "            #finally, lets fill\n",
    "            if (event_list[z][x,5] == 0):\n",
    "                image_0[phi_index,eta_index] = image_0[phi_index,eta_index] + event_list[z][x,0]\n",
    "            elif (event_list[z][x,5] == 1):\n",
    "                image_1[phi_index,eta_index] = image_1[phi_index,eta_index] + event_list[z][x,0]\n",
    "                image_2[phi_index,eta_index] = image_2[phi_index,eta_index] + 1\n",
    "\n",
    "        #Now, lets go through and normalise to 255\n",
    "        image_0 = np.divide(image_0,np.sum(image_0))\n",
    "        image_1 = np.divide(image_1,np.sum(image_1))\n",
    "        image_2 = np.divide(image_2,np.sum(image_2))\n",
    "        image_list.append(np.array([image_0,image_1,image_2]))\n",
    "    return(image_list)\n",
    "\n",
    "def cluster_event(event_list):\n",
    "    event_list_clustered = []\n",
    "    for x in range(len(event_list)):\n",
    "        to_Cluster = np.array([event_list[x][:,0],event_list[x][:,1],event_list[x][:,2],event_list[x][:,3]])\n",
    "        to_Cluster = np.swapaxes(to_Cluster,0,1)\n",
    "        to_Cluster = np.core.records.fromarrays(to_Cluster.transpose(), \n",
    "                                             names='pT, eta, phi, mass',\n",
    "                                             formats = 'f8, f8, f8,f8')\n",
    "        sequence_Cluster = cluster(to_Cluster, R = 0.8,p = -1)\n",
    "        jets_Cluster = sequence_Cluster.inclusive_jets()\n",
    "        event_list_clustered.append(jets_Cluster)\n",
    "    return(event_list_clustered)\n",
    "\n",
    "def recluster_event(cluster_list):\n",
    "    reclustered_list= []\n",
    "    for i in range(len(cluster_list)):\n",
    "        sequence_Cluster = cluster((cluster_list[i][0]), R=0.2,p=-1)\n",
    "        jets_Cluster = sequence_Cluster.inclusive_jets()\n",
    "        reclustered_list.append(jets_Cluster)\n",
    "    return(reclustered_list)\n",
    "\n",
    "def dphi(phi,phi_c):\n",
    "    #Returns the difference in phi between phi, and phi_center\n",
    "    #This is returnede as a float between (-PI, PI)\n",
    "    \n",
    "    dphi_temp = phi - phi_c\n",
    "    while dphi_temp > np.pi:\n",
    "        dphi_temp = dphi_temp - 2*np.pi\n",
    "    while dphi_temp < -np.pi:\n",
    "        dphi_temp = dphi_temp + 2*np.pi\n",
    "    return (dphi_temp)\n",
    "\n",
    "def return_fine_image_list_reclustered(event_list, event_list_clustered, radius, which_jet = 0,verbose = False):\n",
    "    image_list = []\n",
    "    image_0 = np.zeros((width,height)) #Neutral pt\n",
    "    image_1 = np.zeros((width,height)) #Charged pt\n",
    "    image_2 = np.zeros((width,height)) #Charged multiplicity\n",
    "    \n",
    "    no_two = 0\n",
    "\n",
    "    for z in range(len(event_list)):\n",
    "        image_0 = np.zeros((width,height))\n",
    "        image_1 = np.zeros((width,height))\n",
    "        image_2 = np.zeros((width,height))\n",
    "        \n",
    "        if (len(event_list_clustered[z]) > 1):\n",
    "            #First, let's find the direction of the second-hardest jet relative to the first-hardest subjet\n",
    "            phi_dir = -(dphi(event_list_clustered[z][1].phi,event_list_clustered[z][0].phi))\n",
    "            eta_dir = -(event_list_clustered[z][1].eta - event_list_clustered[z][0].eta)\n",
    "            #Norm difference:\n",
    "            norm_dir = np.linalg.norm([phi_dir,eta_dir])\n",
    "            #This is now the y-hat direction. so we can actually find the unit vector:\n",
    "            y_hat = np.divide([phi_dir,eta_dir],np.linalg.norm([phi_dir,eta_dir]))\n",
    "            #and we can find the x_hat direction as well\n",
    "            x_hat = np.array([y_hat[1],-y_hat[0]]) \n",
    "        else:\n",
    "            no_two = no_two + 1\n",
    "            #continue\n",
    "            \n",
    "        if verbose==True:\n",
    "            print(x_hat,y_hat,norm_dir)\n",
    "            \n",
    "        \n",
    "        for x in range(len(event_list[z])):\n",
    "            if (len(event_list_clustered[z]) == 1):\n",
    "                #In the case that the reclustering only found one hard jet (that seems kind of bad, but hey)\n",
    "                #no_two = no_two+1\n",
    "                new_coord = [dphi(event_list[z][x,2],event_list_clustered[z][0].phi),event_list[z][x,1]-event_list_clustered[z][0].eta]\n",
    "                indxs = [math.floor(width*new_coord[0]/(radius*1.5))+width//2,math.floor(height*(new_coord[1])/(radius*1.5))+height//2]\n",
    "            else:\n",
    "                #Now, we want to express an incoming particle in this new basis:\n",
    "                part_coord = [dphi(event_list[z][x,2],event_list_clustered[z][0].phi),event_list[z][x,1]-event_list_clustered[z][0].eta]\n",
    "                new_coord = np.dot(np.array([x_hat,y_hat]),part_coord)\n",
    "                #Now, we want to cast these new coordinates into our array\n",
    "                indxs = [math.floor(width*new_coord[0]/(radius*1.5))+width//2,math.floor(height*(new_coord[1]+norm_dir/1.5)/(radius*1.5))+height//2]\n",
    "                \n",
    "            if indxs[0] >= width or indxs[1] >= height or indxs[0] <= 0 or indxs[1] <= 0:\n",
    "                continue\n",
    "            phi_index = int(indxs[0]); eta_index = int(indxs[1])\n",
    "            #finally, lets fill\n",
    "            if (event_list[z][x,5] == 0):\n",
    "                image_0[phi_index,eta_index] = image_0[phi_index,eta_index] + event_list[z][x,0]\n",
    "            elif (event_list[z][x,5] == 1):\n",
    "                image_1[phi_index,eta_index] = image_1[phi_index,eta_index] + event_list[z][x,0]\n",
    "                image_2[phi_index,eta_index] = image_2[phi_index,eta_index] + 1\n",
    "\n",
    "        #Now, lets go through and normalise to 255\n",
    "        if (np.sum(image_0) == 0 or np.sum(image_1) == 0 or np.sum(image_2) == 0):\n",
    "            image_list.append(np.array([image_0,image_1,image_2]))\n",
    "            continue\n",
    "        image_0 = np.divide(image_0,np.sum(image_0))\n",
    "        image_1 = np.divide(image_1,np.sum(image_1))\n",
    "        image_2 = np.divide(image_2,np.sum(image_2))\n",
    "        image_list.append(np.array([image_0,image_1,image_2]))\n",
    "    print(\"no two \" + str(no_two))\n",
    "    return(image_list)\n",
    "\n",
    "def y(p):\n",
    "    return ((1/2)*math.log((p.e+p.pz)/(p.e-p.pz)))\n",
    "\n",
    "def R(con1,con2):\n",
    "    return (((con1.eta-con2.eta)**2+dphi(con1.phi,con2.phi)**2)**(1/2))\n",
    "\n",
    "def R_y(con1,con2):\n",
    "    return (((y(con1)-y(con2))**2+dphi(con1.phi,con2.phi)**2)**(1/2))\n",
    "\n",
    "def N_2(jcon):\n",
    "    #Takes jcon, jet constituents\n",
    "    p_x_total = np.sum([con.px for con in jcon])\n",
    "    p_y_total = np.sum([con.py for con in jcon])\n",
    "    p_total = (p_x_total**2+p_y_total**2)**(1/2)\n",
    "    \n",
    "    v_1e2 = 0\n",
    "    for i in range(len(jcon)):\n",
    "        for j in range(i+1,len(jcon)):\n",
    "            v_1e2 = v_1e2+ jcon[i].pt*jcon[j].pt*R(jcon[i],jcon[j])/(p_total**2)\n",
    "    v_2e3 = 0\n",
    "    for i in range(len(jcon)):\n",
    "        for j in range(i+1,len(jcon)):\n",
    "            for k in range(j+1,len(jcon)):\n",
    "                v_2e3 = v_2e3 + jcon[i].pt*jcon[j].pt*jcon[j].pt*min(R(jcon[i],jcon[j])*R(jcon[i],jcon[k]),\n",
    "                                                                     R(jcon[j],jcon[k])*R(jcon[i],jcon[j]),\n",
    "                                                        R(jcon[i],jcon[k])*R(jcon[j],jcon[k]))/(p_total**3)\n",
    "    return v_2e3/(v_1e2**2)\n",
    "\n",
    "class myJet(object):\n",
    "    def __init__(self,px,py,pz):\n",
    "        self.px = px; self.py = py; self.pz = pz; self.pt = (px**2+py**2)**(1/2)\n",
    "        self.phi = math.atan2(py,px); self.eta = -math.log(math.tan(math.atan2(self.pt,self.pz)/2));\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "    def add_child(self,obj):\n",
    "        self.children.append(obj)\n",
    "\n",
    "def softdrop(jcon,z=0.1,debug = 0):\n",
    "    #Takes the constituents of a jet, and softdrops it.\n",
    "    #First, we need to step through the jet and build the tree of clustering\n",
    "    #Since we are reclustering the whole thing; just take R = 1; i.e. dont need to think about it\n",
    "    def distance(con1,con2):\n",
    "        return R(con1,con2)**2\n",
    "    pseudojets = []\n",
    "    nodes = []\n",
    "    for con in jcon:\n",
    "        x = Node([con,1])\n",
    "        #pseudojets.append(x)\n",
    "        nodes.append(x) #1 means its still a pseudojet; i.e. not been clustered\n",
    "    def how_many_pseudo(nodes):\n",
    "        how_many = 0\n",
    "        for node in nodes:\n",
    "            if node.data[1] == 1:\n",
    "                how_many = how_many + 1\n",
    "        return how_many\n",
    "    if debug == 1:\n",
    "        print(\"len(nodes) : \" + str(len(nodes)))\n",
    "        #print(nodes)\n",
    "    rep = 0\n",
    "    while how_many_pseudo(nodes) > 1:\n",
    "        #print(how_many_pseudo(nodes))\n",
    "        min_distance = float(\"inf\")\n",
    "        min_index = [0,0]\n",
    "        for i in range(0,len(nodes)):\n",
    "            if nodes[i].data[1] == 0: #Its already part of something else\n",
    "                continue\n",
    "            for j in range(i+1,len(nodes)):\n",
    "                if nodes[j].data[1] == 0:\n",
    "                    continue\n",
    "                if distance(nodes[i].data[0],nodes[j].data[0]) < min_distance:\n",
    "                    min_index[0] = i; min_index[1] = j; \n",
    "                    min_distance = distance(nodes[i].data[0],nodes[j].data[0])\n",
    "        i = min_index[0];j=min_index[1];\n",
    "        new_node = Node([myJet(nodes[i].data[0].px+nodes[j].data[0].px,\n",
    "                           nodes[i].data[0].py+nodes[j].data[0].py,\n",
    "                           nodes[i].data[0].pz+nodes[j].data[0].pz),1])\n",
    "        new_node.add_child(nodes[i])\n",
    "        new_node.add_child(nodes[j])\n",
    "        nodes.append(new_node)\n",
    "        nodes[i].data[1] = 0\n",
    "        nodes[j].data[1] = 0\n",
    "    \n",
    "    #print(nodes)\n",
    "\n",
    "    to_check = [] #nodes to check\n",
    "    for i in range(len(nodes)):\n",
    "        if nodes[i].data[1] == 1:\n",
    "            to_check.append(nodes[i])\n",
    "    softcon = []\n",
    "    while len(to_check) > 0:\n",
    "        #print(to_check)\n",
    "        our_childs = to_check[0].children\n",
    "        #print(our_childs)\n",
    "        if len(our_childs) == 0:\n",
    "            softcon.append(to_check[0].data[0])\n",
    "            to_check.pop(0)\n",
    "            continue\n",
    "        if min(our_childs[0].data[0].pt,our_childs[1].data[0].pt)/  \\\n",
    "                        (our_childs[0].data[0].pt+our_childs[1].data[0].pt) > z:\n",
    "            to_check.append(our_childs[0])\n",
    "            to_check.append(our_childs[1])\n",
    "        elif our_childs[0].data[0].pt > our_childs[1].data[0].pt:\n",
    "            to_check.append(our_childs[0])\n",
    "        else:\n",
    "            to_check.append(our_childs[1])\n",
    "        to_check.pop(0)\n",
    "    return softcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading events for actual\n",
      "list of files is['old.csv', 'bigger_but_older.csv', 'actual_actualMLM_final15693.csv', 'big_but_old.csv', '10823.csv']\n",
      "test: i is: old.csv\n",
      "total path is: /data1/users/jzlin/MLM/background_7413/old.csv\n",
      "test: i is: bigger_but_older.csv\n",
      "total path is: /data1/users/jzlin/MLM/background_7413/bigger_but_older.csv\n",
      "test: i is: actual_actualMLM_final15693.csv\n",
      "total path is: /data1/users/jzlin/MLM/background_7413/actual_actualMLM_final15693.csv\n",
      "actual_actualMLM_final15693.csv\n",
      "46052\n",
      "Read 1 number of files\n",
      "test: i is: big_but_old.csv\n",
      "total path is: /data1/users/jzlin/MLM/background_7413/big_but_old.csv\n",
      "Loading events for actual\n",
      "list of files is['actual_signalMLM_run_03_0_seed_61690.csv', 'actual_signalMLM_run_03_3_seed_82962.csv', 'actual_signalMLM_run_03_3_seed_99614.csv', 'actual_signalMLM_run_03_0_seed_57602.csv', 'actual_signalMLM_run_03_3_seed_83577.csv', 'actual_signalMLM_run_03_3_seed_70357.csv', 'actual_signalMLM_run_03_1_seed_85118.csv', 'actual_signalMLM_run_03_7_seed_81419.csv', 'actual_signalMLM_run_03_1_seed_30995.csv', 'actual_signalMLM_run_03_6_seed_62807.csv', 'actual_signalMLM_run_03_2_seed_83303.csv', 'actual_signalMLM_run_03_1_seed_88402.csv', 'actual_signalMLM_run_03_2_seed_75869.csv', 'actual_signalMLM_run_03_7_seed_32689.csv', 'actual_signalMLM_run_03_5_seed_2948.csv', 'actual_signalMLM_run_03_5_seed_18274.csv', 'actual_signalMLM_run_03_0_seed_104125.csv', 'actual_signalMLM_run_03_4_seed_27979.csv', 'actual_signalMLM_run_03_4_seed_54042.csv', 'actual_signalMLM_run_03_7_seed_28221.csv', 'actual_signalMLM_run_03_5_seed_72273.csv', 'actual_signalMLM_run_03_2_seed_97124.csv', 'actual_signalMLM_run_03_7_seed_11600.csv', 'actual_signalMLM_run_03_6_seed_32139.csv', 'actual_signalMLM_run_03_5_seed_62784.csv', 'actual_signalMLM_run_03_1_seed_41638.csv', 'actual_signalMLM_run_03_3_seed_14638.csv', 'actual_signalMLM_run_03_1_seed_4770.csv', 'actual_signalMLM_run_03_4_seed_40531.csv', 'actual_signalMLM_run_03_2_seed_26797.csv', 'actual_signalMLM_run_03_5_seed_25302.csv', 'actual_signalMLM_run_03_6_seed_68183.csv', 'actual_signalMLM_run_03_0_seed_29687.csv', 'actual_signalMLM_run_03_0_seed_34052.csv', 'actual_signalMLM_run_03_6_seed_28822.csv', 'actual_signalMLM_run_03_6_seed_18760.csv', 'actual_signalMLM_run_03_7_seed_58753.csv', 'actual_signalMLM_run_03_4_seed_95050.csv', 'actual_signalMLM_run_03_4_seed_49131.csv', 'actual_signalMLM_run_03_2_seed_29150.csv']\n",
      "test: i is: actual_signalMLM_run_03_0_seed_61690.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_0_seed_61690.csv\n",
      "actual_signalMLM_run_03_0_seed_61690.csv\n",
      "1296\n",
      "Read 1 number of files\n",
      "test: i is: actual_signalMLM_run_03_3_seed_82962.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_3_seed_82962.csv\n",
      "actual_signalMLM_run_03_3_seed_82962.csv\n",
      "1241\n",
      "Read 2 number of files\n",
      "test: i is: actual_signalMLM_run_03_3_seed_99614.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_3_seed_99614.csv\n",
      "actual_signalMLM_run_03_3_seed_99614.csv\n",
      "1242\n",
      "Read 3 number of files\n",
      "test: i is: actual_signalMLM_run_03_0_seed_57602.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_0_seed_57602.csv\n",
      "actual_signalMLM_run_03_0_seed_57602.csv\n",
      "1311\n",
      "Read 4 number of files\n",
      "test: i is: actual_signalMLM_run_03_3_seed_83577.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_3_seed_83577.csv\n",
      "actual_signalMLM_run_03_3_seed_83577.csv\n",
      "1230\n",
      "Read 5 number of files\n",
      "test: i is: actual_signalMLM_run_03_3_seed_70357.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_3_seed_70357.csv\n",
      "actual_signalMLM_run_03_3_seed_70357.csv\n",
      "1227\n",
      "Read 6 number of files\n",
      "test: i is: actual_signalMLM_run_03_1_seed_85118.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_1_seed_85118.csv\n",
      "actual_signalMLM_run_03_1_seed_85118.csv\n",
      "1278\n",
      "Read 7 number of files\n",
      "test: i is: actual_signalMLM_run_03_7_seed_81419.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_7_seed_81419.csv\n",
      "actual_signalMLM_run_03_7_seed_81419.csv\n",
      "1220\n",
      "Read 8 number of files\n",
      "test: i is: actual_signalMLM_run_03_1_seed_30995.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_1_seed_30995.csv\n",
      "actual_signalMLM_run_03_1_seed_30995.csv\n",
      "1279\n",
      "Read 9 number of files\n",
      "test: i is: actual_signalMLM_run_03_6_seed_62807.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_6_seed_62807.csv\n",
      "actual_signalMLM_run_03_6_seed_62807.csv\n",
      "1290\n",
      "Read 10 number of files\n",
      "test: i is: actual_signalMLM_run_03_2_seed_83303.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_2_seed_83303.csv\n",
      "actual_signalMLM_run_03_2_seed_83303.csv\n",
      "1223\n",
      "Read 11 number of files\n",
      "test: i is: actual_signalMLM_run_03_1_seed_88402.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_1_seed_88402.csv\n",
      "actual_signalMLM_run_03_1_seed_88402.csv\n",
      "1240\n",
      "Read 12 number of files\n",
      "test: i is: actual_signalMLM_run_03_2_seed_75869.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_2_seed_75869.csv\n",
      "actual_signalMLM_run_03_2_seed_75869.csv\n",
      "1236\n",
      "Read 13 number of files\n",
      "test: i is: actual_signalMLM_run_03_7_seed_32689.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_7_seed_32689.csv\n",
      "actual_signalMLM_run_03_7_seed_32689.csv\n",
      "1273\n",
      "Read 14 number of files\n",
      "test: i is: actual_signalMLM_run_03_5_seed_2948.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_5_seed_2948.csv\n",
      "actual_signalMLM_run_03_5_seed_2948.csv\n",
      "1213\n",
      "Read 15 number of files\n",
      "test: i is: actual_signalMLM_run_03_5_seed_18274.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_5_seed_18274.csv\n",
      "actual_signalMLM_run_03_5_seed_18274.csv\n",
      "1238\n",
      "Read 16 number of files\n",
      "test: i is: actual_signalMLM_run_03_0_seed_104125.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_0_seed_104125.csv\n",
      "actual_signalMLM_run_03_0_seed_104125.csv\n",
      "1267\n",
      "Read 17 number of files\n",
      "test: i is: actual_signalMLM_run_03_4_seed_27979.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_4_seed_27979.csv\n",
      "actual_signalMLM_run_03_4_seed_27979.csv\n",
      "1240\n",
      "Read 18 number of files\n",
      "test: i is: actual_signalMLM_run_03_4_seed_54042.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_4_seed_54042.csv\n",
      "actual_signalMLM_run_03_4_seed_54042.csv\n",
      "1226\n",
      "Read 19 number of files\n",
      "test: i is: actual_signalMLM_run_03_7_seed_28221.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_7_seed_28221.csv\n",
      "actual_signalMLM_run_03_7_seed_28221.csv\n",
      "1257\n",
      "Read 20 number of files\n",
      "test: i is: actual_signalMLM_run_03_5_seed_72273.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_5_seed_72273.csv\n",
      "actual_signalMLM_run_03_5_seed_72273.csv\n",
      "1270\n",
      "Read 21 number of files\n",
      "test: i is: actual_signalMLM_run_03_2_seed_97124.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_2_seed_97124.csv\n",
      "actual_signalMLM_run_03_2_seed_97124.csv\n",
      "1252\n",
      "Read 22 number of files\n",
      "test: i is: actual_signalMLM_run_03_7_seed_11600.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_7_seed_11600.csv\n",
      "actual_signalMLM_run_03_7_seed_11600.csv\n",
      "1252\n",
      "Read 23 number of files\n",
      "test: i is: actual_signalMLM_run_03_6_seed_32139.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_6_seed_32139.csv\n",
      "actual_signalMLM_run_03_6_seed_32139.csv\n",
      "1251\n",
      "Read 24 number of files\n",
      "test: i is: actual_signalMLM_run_03_5_seed_62784.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_5_seed_62784.csv\n",
      "actual_signalMLM_run_03_5_seed_62784.csv\n",
      "1305\n",
      "Read 25 number of files\n",
      "test: i is: actual_signalMLM_run_03_1_seed_41638.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_1_seed_41638.csv\n",
      "actual_signalMLM_run_03_1_seed_41638.csv\n",
      "1262\n",
      "Read 26 number of files\n",
      "test: i is: actual_signalMLM_run_03_3_seed_14638.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_3_seed_14638.csv\n",
      "actual_signalMLM_run_03_3_seed_14638.csv\n",
      "1166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 27 number of files\n",
      "test: i is: actual_signalMLM_run_03_1_seed_4770.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_1_seed_4770.csv\n",
      "actual_signalMLM_run_03_1_seed_4770.csv\n",
      "1246\n",
      "Read 28 number of files\n",
      "test: i is: actual_signalMLM_run_03_4_seed_40531.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_4_seed_40531.csv\n",
      "actual_signalMLM_run_03_4_seed_40531.csv\n",
      "1203\n",
      "Read 29 number of files\n",
      "test: i is: actual_signalMLM_run_03_2_seed_26797.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_2_seed_26797.csv\n",
      "actual_signalMLM_run_03_2_seed_26797.csv\n",
      "1221\n",
      "Read 30 number of files\n",
      "test: i is: actual_signalMLM_run_03_5_seed_25302.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_5_seed_25302.csv\n",
      "actual_signalMLM_run_03_5_seed_25302.csv\n",
      "1236\n",
      "Read 31 number of files\n",
      "test: i is: actual_signalMLM_run_03_6_seed_68183.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_6_seed_68183.csv\n",
      "actual_signalMLM_run_03_6_seed_68183.csv\n",
      "1266\n",
      "Read 32 number of files\n",
      "test: i is: actual_signalMLM_run_03_0_seed_29687.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_0_seed_29687.csv\n",
      "actual_signalMLM_run_03_0_seed_29687.csv\n",
      "1294\n",
      "Read 33 number of files\n",
      "test: i is: actual_signalMLM_run_03_0_seed_34052.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_0_seed_34052.csv\n",
      "actual_signalMLM_run_03_0_seed_34052.csv\n",
      "1136\n",
      "Read 34 number of files\n",
      "test: i is: actual_signalMLM_run_03_6_seed_28822.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_6_seed_28822.csv\n",
      "actual_signalMLM_run_03_6_seed_28822.csv\n",
      "1224\n",
      "Read 35 number of files\n",
      "test: i is: actual_signalMLM_run_03_6_seed_18760.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_6_seed_18760.csv\n",
      "actual_signalMLM_run_03_6_seed_18760.csv\n",
      "1264\n",
      "Read 36 number of files\n",
      "test: i is: actual_signalMLM_run_03_7_seed_58753.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_7_seed_58753.csv\n",
      "actual_signalMLM_run_03_7_seed_58753.csv\n",
      "1274\n",
      "Read 37 number of files\n",
      "test: i is: actual_signalMLM_run_03_4_seed_95050.csv\n",
      "total path is: /data1/users/jzlin/MLM/heavy_signal/actual_signalMLM_run_03_4_seed_95050.csv\n",
      "46052 46149\n"
     ]
    }
   ],
   "source": [
    "#Reading in files; madgraph style, single large file (!!pT cut!!)\n",
    "num_signal_files = 0\n",
    "num_background_files = 1\n",
    "background_event_list,background_mass_list,background_image_list,num_background_files = \\\n",
    "    load_events(\"actual\", max_Files=num_background_files,path=\"/data1/users/jzlin/MLM/background_7413/\",\\\n",
    "                contains=\"_actual\",pt_cut=1)\n",
    "num_background_files = 15693\n",
    "\n",
    "signal_event_list,signal_mass_list,signal_image_list,num_signal_files = \\\n",
    "    load_events(\"actual\", max_Read = len(background_event_list),path=\"/data1/users/jzlin/MLM/heavy_signal/\",\\\n",
    "                contains=\"_signal\",pt_cut=1)\n",
    "\n",
    "#Check size of dataset\n",
    "print(len(background_mass_list),len(signal_mass_list))\n",
    "\n",
    "#Zero centering\n",
    "tmp_av = np.average(np.concatenate((background_image_list,signal_image_list)),axis=0)\n",
    "tmp_sd = np.std(np.concatenate((background_image_list,signal_image_list)),axis=0)\n",
    "save_av_img = tmp_av; save_sd_img = tmp_sd;\n",
    "for i in range(len(background_image_list)):\n",
    "    background_image_list[i] = np.divide((background_image_list[i] - tmp_av),(tmp_sd+1e-5)) #perhaps add some r to temp_sd to suppress noise\n",
    "for i in range(len(signal_image_list)):\n",
    "    signal_image_list[i] = np.divide((signal_image_list[i] - tmp_av),(tmp_sd+1e-5))#/tmp_sd\n",
    "    \n",
    "background_mass_window = np.logical_and(np.array(background_mass_list) > 115,np.array(background_mass_list) < 135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# calculate weights\n",
    "background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "signal_weights = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clusters events into jets into background/signal_event_list_clustered\n",
    "background_event_list_clustered = cluster_event(background_event_list)\n",
    "signal_event_list_clustered = cluster_event(signal_event_list)\n",
    "#Reclustering the events (i.e. clustering within events)\n",
    "background_reclustered = recluster_event(background_event_list_clustered)\n",
    "signal_reclustered = recluster_event(signal_event_list_clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_recluster_images = return_fine_image_list_reclustered(background_event_list,\n",
    "                                                           background_reclustered,0.8)\n",
    "signal_recluster_images = return_fine_image_list_reclustered(signal_event_list,\n",
    "                                                           signal_reclustered,0.8)\n",
    "\n",
    "background_recluster_images_nonstandard,signal_recluster_images_nonstandard = (np.copy(background_recluster_images),\n",
    "                                                                              np.copy(signal_recluster_images))\n",
    "tmp_av = np.average(np.concatenate((background_recluster_images,signal_recluster_images)),axis=0)\n",
    "tmp_sd = np.std(np.concatenate((background_recluster_images,signal_recluster_images)),axis=0)\n",
    "save_av_rc = tmp_av; save_sd_rc = tmp_sd;\n",
    "for i in range(len(background_recluster_images)):\n",
    "    background_recluster_images[i] = np.divide((background_recluster_images[i] - tmp_av),tmp_sd+1e-5) #perhaps add some r to temp_sd to suppress noise\n",
    "for i in range(len(signal_recluster_images)):\n",
    "    signal_recluster_images[i] = np.divide((signal_recluster_images[i] - tmp_av),tmp_sd+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(background_recluster_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "#Function that sets up whole event dataset into test and train datasets\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "rsplit = np.array([0.5,0.75])\n",
    "\n",
    "def splitData(background_image_list,signal_image_list):\n",
    "    try:\n",
    "        input_shape = background_image_list[0].shape\n",
    "    except AttributeError:\n",
    "        input_shape = (0)\n",
    "        \n",
    "    b_split = np.split(background_image_list,(len(background_image_list)*rsplit).astype(int))\n",
    "    s_split = np.split(signal_image_list,(len(signal_image_list)*rsplit).astype(int))\n",
    "    bm_split = np.split(background_mass_list,(len(background_mass_list)*rsplit).astype(int))\n",
    "    sm_split = np.split(signal_mass_list,(len(signal_mass_list)*rsplit).astype(int))\n",
    "    \n",
    "    x_train = np.concatenate((b_split[0],s_split[0]))\n",
    "    y_train = np.array(np.concatenate((np.zeros(len(b_split[0])),np.ones(len(s_split[0])))))\n",
    "    mass_train = np.concatenate((bm_split[0],sm_split[0]))\n",
    "    \n",
    "    x_val = np.concatenate((b_split[1],s_split[1]))\n",
    "    y_val = np.array(np.concatenate((np.zeros(len(b_split[1])),np.ones(len(s_split[1])))))\n",
    "    mass_val = np.concatenate((bm_split[1],sm_split[1]))\n",
    "    \n",
    "    x_test = np.concatenate((b_split[2],s_split[2]))\n",
    "    y_test = np.array(np.concatenate((np.zeros(len(b_split[2])),np.ones(len(s_split[2])))))\n",
    "    mass_test = np.concatenate((bm_split[2],sm_split[2]))\n",
    "        \n",
    "    \n",
    "    x_train_cut = [] #Note that we *train* on this cut sample, but *test* on the whole thing I guess\n",
    "    y_train_cut = []\n",
    "    mass_train_cut = []\n",
    "    for x in reversed(range(len(x_train))):\n",
    "        if mass_train[x] < 135 and mass_train[x] > 115:\n",
    "            x_train_cut.append(x_train[x])\n",
    "            y_train_cut.append(y_train[x])\n",
    "            mass_train_cut.append(mass_train[x])\n",
    "    x_train_cut = np.array(x_train_cut)\n",
    "    y_train_cut = np.array(y_train_cut)\n",
    "    mass_train_cut = np.array(mass_train_cut)\n",
    "    \n",
    "    x_val_cut = [] \n",
    "    y_val_cut = []\n",
    "    mass_val_cut = []\n",
    "    for x in reversed(range(len(x_val))):\n",
    "        if mass_val[x] < 135 and mass_val[x] > 115:\n",
    "            x_val_cut.append(x_val[x])\n",
    "            y_val_cut.append(y_val[x])\n",
    "            mass_val_cut.append(mass_val[x])\n",
    "    x_val_cut = np.array(x_val_cut)\n",
    "    y_val_cut = np.array(y_val_cut)\n",
    "    mass_val_cut = np.array(mass_val_cut)\n",
    "    \n",
    "    x_test_cut = [] \n",
    "    y_test_cut = []\n",
    "    mass_test_cut = []\n",
    "    for x in reversed(range(len(x_test))):\n",
    "        if mass_test[x] < 135 and mass_test[x] > 115:\n",
    "            x_test_cut.append(x_test[x])\n",
    "            y_test_cut.append(y_test[x])\n",
    "            mass_test_cut.append(mass_test[x])\n",
    "    x_test_cut = np.array(x_test_cut)\n",
    "    y_test_cut = np.array(y_test_cut)\n",
    "    mass_test_cut = np.array(mass_test_cut)\n",
    "    \n",
    "    return(input_shape,\n",
    "           x_train,y_train,mass_train,\n",
    "           x_val,y_val,mass_val,\n",
    "           x_test,y_test,mass_test,\n",
    "           x_train_cut,y_train_cut,mass_train_cut,\n",
    "           x_val_cut,y_val_cut,mass_val_cut,\n",
    "           x_test_cut,y_test_cut,mass_test_cut,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This actually splits the main data\n",
    "input_shape, \\\n",
    "    x_train,y_train,mass_train, \\\n",
    "    x_val,y_val,mass_val, \\\n",
    "    x_test,y_test,mass_test, \\\n",
    "    x_train_cut,y_train_cut,mass_train_cut, \\\n",
    "    x_val_cut,y_val_cut,mass_val_cut, \\\n",
    "    x_test_cut,y_test_cut,mass_test_cut = splitData(background_image_list,signal_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reclustered data\n",
    "input_shape_r, \\\n",
    "    x_train_r,y_train_r,mass_train_r, \\\n",
    "    x_val_r,y_val_r,mass_val_r, \\\n",
    "    x_test_r,y_test_r,mass_test_r, \\\n",
    "    x_train_cut_r,y_train_cut_r,mass_train_cut_r, \\\n",
    "    x_val_cut_r,y_val_cut_r,mass_val_cut_r, \\\n",
    "    x_test_cut_r,y_test_cut_r,mass_test_cut_r = splitData(background_recluster_images,signal_recluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 40, 40)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FRANK print an event image\n",
    "print(np.shape(x_train[0]))\n",
    "plt.imshow(x_train_r[-1][0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_r[-1][1])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_r[-1][2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     2,
     23,
     63,
     83,
     118,
     133,
     179,
     205,
     236,
     278,
     316,
     399,
     457
    ]
   },
   "outputs": [],
   "source": [
    "#Here are some substructure variables\n",
    "\n",
    "def find_new_var_3(reclustered,clustered,pt_cut = 1):\n",
    "    #The point is that many jets only have two subjets so the above is not really that efficient... hm\n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        net_R = 0\n",
    "        jet = clustered[sqn][0]\n",
    "        con = jet.constituents()\n",
    "        sub_1 = reclustered[sqn][0]\n",
    "        try:\n",
    "            sub_2 = reclustered[sqn][1]\n",
    "        except:\n",
    "            #print('ah')\n",
    "            sub_2 = sub_1\n",
    "        for i in range(len(con)):\n",
    "            if con[i].pt < pt_cut:\n",
    "                continue\n",
    "            #net_R = net_R + (con[i].pt/jet.pt)**(-1)*(R(sub_1,con[i])*R(sub_2,con[i]))\n",
    "            net_R = net_R + (R(sub_1,con[i])*R(sub_2,con[i]))\n",
    "        new_var.append(-net_R)\n",
    "    return new_var\n",
    "\n",
    "def find_new_var_3_norm(reclustered,clustered,pt_cut=1):\n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        net_R = 0\n",
    "        jet = clustered[sqn][0]\n",
    "        con = jet.constituents()\n",
    "        sub_1 = reclustered[sqn][0]\n",
    "        max_rad_1 = 0\n",
    "        max_rad_2 = 0\n",
    "        for i in range(0,len(con)):\n",
    "            if R(con[i],sub_1) > max_rad_1:\n",
    "                max_rad_1 = R(con[i],sub_1)\n",
    "        try:\n",
    "            sub_2 = reclustered[sqn][1]\n",
    "        except:\n",
    "            print('ah')\n",
    "            sub_2 = sub_1\n",
    "        for i in range(0,len(con)):\n",
    "                if R(con[i],sub_2) > max_rad_2:\n",
    "                    max_rad_2 = R(con[i],sub_2)\n",
    "\n",
    "        I = np.zeros((2,2))\n",
    "        basis_1 = np.array([jet.px,jet.py,jet.pz])/np.linalg.norm(np.array([jet.px,jet.py,jet.pz]))\n",
    "        basis_2 = np.array([-jet.py,jet.px,0])/np.linalg.norm(np.array([-jet.py,jet.px,0]))\n",
    "        basis_3 = np.cross(basis_1,basis_2)\n",
    "        transform = np.array([basis_1,basis_2,basis_3])\n",
    "        \n",
    "        norm_factor = 0\n",
    "        \n",
    "        for i in range(0,len(con)):\n",
    "            if con[i].pt < pt_cut:\n",
    "                continue\n",
    "            new_par = transform.dot(np.array([con[i].px,con[i].py,con[i].pz]))\n",
    "            rel_pt = (new_par[1]**2+new_par[2]**2)**(1/2)\n",
    "            norm_factor = norm_factor + rel_pt\n",
    "            net_R = net_R + (rel_pt)*(R(sub_1,con[i])*R(sub_2,con[i]))/(max_rad_1*max_rad_2)\n",
    "        net_R = net_R/norm_factor\n",
    "        new_var.append(net_R)\n",
    "    return new_var\n",
    "        \n",
    "def find_new_var_N_2(reclustered,clustered):\n",
    "    new_var = []\n",
    "    for i in clustered:\n",
    "        jcon = i[0].constituents()\n",
    "        #Takes jcon, jet constituents\n",
    "        p_total = np.sum([con.pt for con in jcon])\n",
    "        v_1e2 = 0\n",
    "        for i in range(len(jcon)):\n",
    "            for j in range(i+1,len(jcon)):\n",
    "                v_1e2 = v_1e2+ jcon[i].pt*jcon[j].pt*R(jcon[i],jcon[j])/(p_total**2)\n",
    "        #v_1e2 = np.sum([[(con1.pt/p_total)*(con2.pt/p_total)*R(con1,con2) for con1 in jcon] for con2 in jcon])/2\n",
    "        v_2e3 = 0\n",
    "        for i in range(len(jcon)):\n",
    "            for j in range(i+1,len(jcon)):\n",
    "                for k in range(j+1,len(jcon)):\n",
    "                    v_2e3 = v_2e3 + jcon[i].pt*jcon[j].pt*jcon[j].pt*min(R(jcon[i],jcon[j]),R(jcon[j],jcon[k]),\n",
    "                                                                            R(jcon[i],jcon[k]))/(p_total**3)\n",
    "        new_var.append(v_2e3/(v_1e2**2))\n",
    "    return(new_var)\n",
    "        \n",
    "def find_new_var_pf(reclustered,clustered):\n",
    "    #https://arxiv.org/pdf/0807.0234.pdf\n",
    "    #Planar Flow\n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        \n",
    "        I = np.zeros((2,2))\n",
    "        jet = clustered[sqn][0]\n",
    "        \n",
    "        basis_1 = np.array([jet.px,jet.py,jet.pz])/np.linalg.norm(np.array([jet.px,jet.py,jet.pz]))\n",
    "        basis_2 = np.array([-jet.py,jet.px,0])/np.linalg.norm(np.array([-jet.py,jet.px,0]))\n",
    "        basis_3 = np.cross(basis_1,basis_2)\n",
    "        \n",
    "        transform = np.array([basis_1,basis_2,basis_3])\n",
    "        printed = 0\n",
    "        for par in clustered[sqn][0].constituents():\n",
    "            #OK so we need to transfer into coordinates defined by the jet itself\n",
    "            #new_par = np.dot(transform,np.array(par.px,par.py,par.pz))\n",
    "            new_par = transform.dot(np.array([par.px,par.py,par.pz]))\n",
    "            \n",
    "            I = I + [[new_par[1]**2/par.e,new_par[1]*new_par[2]/par.e],[new_par[1]*new_par[2]/par.e,\n",
    "                                                                        new_par[2]**2/par.e]]\n",
    "        I = I/clustered[sqn][0].mass\n",
    "            #if printed == 0:\n",
    "                #print(par.e,new_par)\n",
    "                #print(new_par)\n",
    "                #print([par.px,par.py,par.pz])\n",
    "                #print(jet.px,jet.py,jet.pz)\n",
    "            #    printed = 0\n",
    "        #print(I)\n",
    "        #print(transform)\n",
    "        #new_var.append(4*(I[0,0]*I[1,1]-I[0,1]*I[1,0])/((I[0,0]+I[1,1])**2))\n",
    "        new_var.append(4*np.linalg.det(I)/(np.trace(I)**2))\n",
    "    return new_var\n",
    "\n",
    "def find_new_var_radius(reclustered,clustered):\n",
    "    #Finds the radius by looping over particles and finding the diameter and dividing by 2\n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        max_rad = 0\n",
    "        con = clustered[sqn][0].constituents()\n",
    "        for i in range(0,len(con)):\n",
    "            for j in range(i,len(con)):\n",
    "                if R(con[i],con[j]) > max_rad:\n",
    "                    max_rad = R(con[i],con[j])\n",
    "        new_var.append(max_rad/2)\n",
    "        #if new_var[-1] < 0.2:\n",
    "        #    print(con)\n",
    "    return new_var\n",
    "\n",
    "def find_new_var_int(reclustered,clustered):\n",
    "    #First, we want to start working in the transverse plane of the jet, with [jet, b-direction, other direction]\n",
    "    #as our axis. So:\n",
    "    new_var = []\n",
    "    alpha = 1\n",
    "    for sqn in range(len(reclustered)):\n",
    "        good = 0\n",
    "        jet = clustered[sqn][0]\n",
    "        b_jet_1 = reclustered[sqn][0]\n",
    "        \n",
    "        basis_1 = np.array([jet.px,jet.py,jet.pz])/np.linalg.norm(np.array([jet.px,jet.py,jet.pz]))\n",
    "        basis_2 = np.array([b_jet_1.px,b_jet_1.py,b_jet_1.pz])\n",
    "        basis_2 = basis_2 - np.dot(basis_2,basis_1)*basis_1\n",
    "        basis_2 = basis_2/np.linalg.norm(basis_2)\n",
    "        basis_3 = np.cross(basis_1,basis_2)\n",
    "        \n",
    "        transform = np.array([basis_1,basis_2,basis_3])\n",
    "        b_jet_1_newcoord = transform.dot(np.array([b_jet_1.px,b_jet_1.py,b_jet_1.pz]))\n",
    "        b_jet_1_newcoord = b_jet_1_newcoord/b_jet_1_newcoord[0]\n",
    "        #print(b_jet_1_newcoord)\n",
    "        \n",
    "        try:\n",
    "            b_jet_2 = reclustered[sqn][1]\n",
    "            b_jet_2_newcoord = transform.dot(np.array([b_jet_2.px,b_jet_2.py,b_jet_2.pz]))\n",
    "            b_jet_2_newcoord = b_jet_2_newcoord/b_jet_2_newcoord[0]\n",
    "        except:\n",
    "            b_jet_2_newcoord = - b_jet_1_newcoord\n",
    "        \n",
    "        num = 0\n",
    "        shift = (b_jet_1_newcoord+b_jet_2_newcoord)/2\n",
    "        for par in clustered[sqn][0].constituents():\n",
    "            new_par = transform.dot(np.array([par.px,par.py,par.pz]))\n",
    "            new_par = new_par/new_par[0]\n",
    "            new_par = new_par + shift\n",
    "            #good = good + (1-new_par[1]**2/b_jet_1_newcoord[1]**2)\n",
    "            #good = good + (-new_par[1]**3+b_jet_1_newcoord[1]**3)*(new_par[1]**3+b_jet_1_newcoord[1]**3)/b_jet_1_newcoord[1]**6\n",
    "            #good = good + np.e**(-(new_par[1]/b_jet_1_newcoord[1])**2-(new_par[2]/b_jet_1_newcoord[1])**2)\n",
    "            #good = good + 1/((new_par[1]/b_jet_1_newcoord[1])**2+1)\n",
    "            #good = good + (abs(new_par[1]) < alpha*b_jet_1_newcoord[1])*(abs(new_par[2]) < alpha*b_jet_1_newcoord[1])\n",
    "            #good = good + (new_par[1]**2+new_par[2]**2)/b_jet_1_newcoord[1]**2\n",
    "            good = good + (1-new_par[1]**4/b_jet_1_newcoord[1]**4)*np.e**(-0.6*(new_par[1]**2+new_par[2]**2)/b_jet_1_newcoord[1]**2)\n",
    "            num = num + 1\n",
    "        #new_var.append(good)\n",
    "        new_var.append(good/num)\n",
    "    return new_var\n",
    "\n",
    "def find_new_var_beta_3(reclustered,clustered,pt_cut = 1):\n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        tau_1 = np.zeros(3)\n",
    "        tau_2 = np.zeros(2)\n",
    "        d_0 = 0\n",
    "        jet = clustered[sqn][0]\n",
    "        sub_1 = reclustered[sqn][0]\n",
    "        try:\n",
    "            sub_2 = reclustered[sqn][1]\n",
    "        except:\n",
    "            sub_2 = sub_1\n",
    "        for k in jet:\n",
    "            if k.pt < pt_cut:\n",
    "                continue\n",
    "            d_0 = d_0 + k.pt*0.8\n",
    "            tau_1 = tau_1 + k.pt*np.array([R(k,jet)**0.5,R(k,jet)**1,R(k,jet)**2])\n",
    "            d_2 = min(R(sub_1,k),R(sub_2,k))\n",
    "            tau_2 = tau_2 + k.pt*np.array([d_2**1,d_2**2])\n",
    "        tau_1 = tau_1/d_0\n",
    "        tau_2 = tau_2/d_0\n",
    "        c = 0; d = 0.5; e = -1; a = 0; b = 0;\n",
    "        #new_var.append(tau_1[0]**a*tau_1[1]**b*tau_1[2]**c*tau_2[0]**d*tau_2[1]**e)\n",
    "        new_var.append(tau_1[0]**2*tau_2[0]**0.5*tau_2[1]**-1)\n",
    "    return(new_var)\n",
    "\n",
    "def find_new_var_beta_3_kT(reclustered,clustered,pt_cut = 1):\n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        tau_1 = np.zeros(3)\n",
    "        tau_2 = np.zeros(2)\n",
    "        d_0 = 0\n",
    "        jet = clustered[sqn][0]\n",
    "        \n",
    "        #Now, we need to recluster jet into two kT subjets\n",
    "        sequence_Cluster = cluster(jet, R=0.2,p=1)\n",
    "        jets_Cluster = sequence_Cluster.inclusive_jets()\n",
    "        \n",
    "        sub_1 = jets_Cluster[0]\n",
    "        try:\n",
    "            sub_2 = jets_Cluster[1]\n",
    "        except:\n",
    "            sub_2 = sub_1\n",
    "        for k in jet:\n",
    "            if k.pt < pt_cut:\n",
    "                continue\n",
    "            d_0 = d_0 + k.pt*0.8\n",
    "            tau_1 = tau_1 + k.pt*np.array([R(k,jet)**0.5,R(k,jet)**1,R(k,jet)**2])\n",
    "            d_2 = min(R(sub_1,k),R(sub_2,k))\n",
    "            tau_2 = tau_2 + k.pt*np.array([d_2**1,d_2**2])\n",
    "        tau_1 = tau_1/d_0\n",
    "        tau_2 = tau_2/d_0\n",
    "        c = 0; d = 0.5; e = -1; a = 0; b = 0;\n",
    "        #new_var.append(tau_1[0]**a*tau_1[1]**b*tau_1[2]**c*tau_2[0]**d*tau_2[1]**e)\n",
    "        new_var.append(tau_1[0]**2*tau_2[0]**0.5*tau_2[1]**-1)\n",
    "    return(new_var)\n",
    "    \n",
    "def find_new_var_beta_rb(reclustered,clustered,pt_cut=1):\n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        tau_1 = np.zeros(3)\n",
    "        tau_2 = np.zeros(2)\n",
    "        d_0 = 0\n",
    "        jet = clustered[sqn][0]\n",
    "        sub_1 = reclustered[sqn][0]\n",
    "        try:\n",
    "            sub_2 = reclustered[sqn][1]\n",
    "        except:\n",
    "            sub_2 = sub_1\n",
    "        for k in jet:\n",
    "            if k.pt < pt_cut:\n",
    "                continue\n",
    "            d_0 = d_0 + k.pt*0.8\n",
    "            tau_1 = tau_1 + k.pt*np.array([R(k,jet)**0.5,R(k,jet)**1,R(k,jet)**2])\n",
    "            d_2 = min(R(sub_1,k),R(sub_2,k))\n",
    "            tau_2 = tau_2 + k.pt*np.array([d_2**1,d_2**2])\n",
    "        tau_1 = tau_1/d_0\n",
    "        tau_2 = tau_2/d_0\n",
    "        c = 0; d = 0.5; e = -1; a = 0; b = 0;\n",
    "        #new_var.append(tau_1[0]**a*tau_1[1]**b*tau_1[2]**c*tau_2[0]**d*tau_2[1]**e)\n",
    "        #new_var.append(tau_1[0]**2*tau_2[0]**0.5*tau_2[1]**-1)\n",
    "        \n",
    "        net_R = 0\n",
    "        jet = clustered[sqn][0]\n",
    "        con = jet.constituents()\n",
    "        sub_1 = reclustered[sqn][0]\n",
    "        try:\n",
    "            sub_2 = reclustered[sqn][1]\n",
    "        except:\n",
    "            #print('ah')\n",
    "            sub_2 = sub_1\n",
    "        for i in range(len(con)):\n",
    "            if con[i].pt < pt_cut:\n",
    "                continue\n",
    "            #net_R = net_R + (con[i].pt/jet.pt)**(-1)*(R(sub_1,con[i])*R(sub_2,con[i]))\n",
    "            net_R = net_R + (R(sub_1,con[i])*R(sub_2,con[i]))\n",
    "        new_var.append(np.log(tau_1[0]**2*tau_2[0]**0.5*tau_2[1]**-1*20-1*net_R))\n",
    "    return(new_var)\n",
    "        \n",
    "def find_new_var_kmeans(reclustered,clustered):\n",
    "    #We use k_means with no pT dependence to cluster, and see how many clusteres\n",
    "    new_var = []\n",
    "    for sqn in range(len(clustered)):\n",
    "        data = np.array([[x.eta,x.phi] for x in clustered[sqn][0].constituents()])\n",
    "        mean = np.average(data,axis=0)\n",
    "        badness_1 = np.sum([(x[0]-mean[0])**2+(x[1]-mean[1])**2 for x in data])\n",
    "        try:\n",
    "            mean = np.array([[reclustered[sqn][0].eta,reclustered[sqn][0].phi],\n",
    "                    [reclustered[sqn][1].eta,reclustered[sqn][1].phi]])\n",
    "        except:\n",
    "            mean = np.array([[reclustered[sqn][0].eta,reclustered[sqn][0].phi],\n",
    "                    [reclustered[sqn][0].eta+0.4,reclustered[sqn][0].phi]])\n",
    "        badness = 0\n",
    "        old_badness = -1\n",
    "        for i in range(100):\n",
    "            cluster = [[],[]]\n",
    "            badness_c1 = 0; badness_c2 = 0;\n",
    "            for k in data:\n",
    "                if np.linalg.norm(k-mean[0],2) < np.linalg.norm(k-mean[1],2):\n",
    "                    cluster[0].append(k)\n",
    "                    badness_c1 = badness_c1 + np.linalg.norm(k - mean[0],2)\n",
    "                else:\n",
    "                    cluster[1].append(k)\n",
    "                    badness_c2 = badness_c2 + np.linalg.norm(k - mean[1],2)\n",
    "            if len(cluster[0]) != 0:\n",
    "                badness_c1 = badness_c1/len(cluster[0])\n",
    "            if len(cluster[1]) != 0:\n",
    "                badness_c2 = badness_c2/len(cluster[1])\n",
    "            badness = np.average([badness_c1,badness_c2])\n",
    "            mean[0] = np.average(cluster[0])\n",
    "            mean[1] = np.average(cluster[1])\n",
    "            if old_badness == badness:\n",
    "                break\n",
    "            old_badness = badness\n",
    "        new_var.append(badness_1/badness)\n",
    "    return(new_var)\n",
    "\n",
    "def find_IRC_kmeans(reclustered,clustered):\n",
    "    class angle:\n",
    "        def __init__(self,eta,phi):\n",
    "            self.eta = eta\n",
    "            self.phi = phi\n",
    "        def __repr__(self):\n",
    "            return \"(\" + str(self.eta) + \",\" + str(self.phi)+\")\"\n",
    "    def average(sqn):\n",
    "        #For a sequence of particles, finds the pT weighted average in eta,phi space\n",
    "        pt_sum = np.sum([x.pt for x in sqn])\n",
    "        avg_eta = np.sum([x.eta*(x.pt/pt_sum) for x in sqn])\n",
    "        #print(str(avg_eta) + \" avg_eta\")\n",
    "        #avg_phi = np.min(map(lambda phi : np.sum([dphi(x.phi, phi)**2*x.pt for x in sqn]),\n",
    "        #                     np.arange(0,2*math.pi,0.01)))\n",
    "        #Average phi is really annoying actually. Convention is -pi < phi < pi. To do this, \n",
    "        #let us split the points into phi > 0, phi < 0. Take the averages. If the two averages are\n",
    "        #closer (say closer than pi) than take the average of the two with no shenanigans. If not, wrap.\n",
    "        #print([x.phi < 0 for x in sqn])\n",
    "        #print(np.array([x.phi < 0 for x in sqn]).shape)\n",
    "        #print(sqn)\n",
    "        try:\n",
    "            under_0 = np.array(sqn)[np.array([x.phi < 0 for x in sqn])];\n",
    "        except:\n",
    "            under_0 = []\n",
    "        #print([x.phi for x in under_0])\n",
    "        try:\n",
    "            over_0 = np.array(sqn)[np.array([x.phi >= 0 for x in sqn])];\n",
    "        except:\n",
    "            over_0 = []\n",
    "        under_pt = np.sum([x.pt for x in under_0]); over_pt = np.sum([x.pt for x in over_0]);\n",
    "        if under_pt == 0:\n",
    "            return angle(avg_eta,np.sum([x.phi*x.pt for x in over_0])/over_pt)\n",
    "        elif over_pt == 0:\n",
    "            return angle(avg_eta,np.sum([x.phi*x.pt for x in under_0])/under_pt)\n",
    "        under_avg = np.sum([x.phi*x.pt for x in under_0])/under_pt; \n",
    "        over_avg = np.sum([x.phi*x.pt for x in over_0])/over_pt;\n",
    "        #print(\"avgs \" + str(under_avg) + \" \" + str(over_avg))\n",
    "        if abs(over_avg - under_avg) < 3.1415:\n",
    "            avg_phi = (over_avg*over_pt+under_avg*under_pt)/(over_pt + under_pt)\n",
    "        else:\n",
    "            avg_phi = (over_avg*over_pt + (under_avg+math.pi*2)*under_pt)/(over_pt + under_pt)\n",
    "            if avg_phi > math.pi:\n",
    "                avg_phi = avg_phi - math.pi*2\n",
    "        \n",
    "        return angle(avg_eta,avg_phi)\n",
    "    new_var = []\n",
    "    for sqn in range(len(clustered)):\n",
    "        #print(\"hmm\")\n",
    "        parts = clustered[sqn][0].constituents()\n",
    "        mean = average(parts)\n",
    "        badness_1 = np.sum([R(x,mean)**2*x.pt for x in parts])/np.sum([x.pt for x in parts])\n",
    "        try:\n",
    "            mean = [reclustered[sqn][0],reclustered[sqn][1]]\n",
    "        except:\n",
    "            mean = [reclustered[sqn][0],angle(reclustered[sqn][0].eta+0.1,reclustered[sqn][0].phi+0.1)]\n",
    "        badness = 0\n",
    "        old_badness = -1\n",
    "        #print(\"hah\")\n",
    "        for i in range(100):\n",
    "            #print(mean)\n",
    "            #print(\"hoh\")\n",
    "            cluster = [[],[]]\n",
    "            badness_c1 = 0; badness_c2 = 0;\n",
    "            for x in parts:\n",
    "                if R(x,mean[0]) < R(x,mean[1]):\n",
    "                    cluster[0].append(x)\n",
    "                else:\n",
    "                    cluster[1].append(x)\n",
    "            pT_1 = np.sum([x.pt for x in cluster[0]])\n",
    "            pT_2 = np.sum([x.pt for x in cluster[1]])\n",
    "            badness_c1 = np.sum([R(x,mean[0])**2*(x.pt/pT_1) for x in cluster[0]])\n",
    "            badness_c2 = np.sum([R(x,mean[1])**2*(x.pt/pT_1) for x in cluster[1]])\n",
    "            badness = np.average([badness_c1,badness_c2])\n",
    "            #print(\"huh\")\n",
    "            mean[0] = average(cluster[0])\n",
    "            mean[1] = average(cluster[1])\n",
    "            #print(\"hih\")\n",
    "            if old_badness == badness:\n",
    "                break\n",
    "            old_badness = badness\n",
    "        new_var.append(badness_1/badness)\n",
    "    return(new_var)\n",
    "\n",
    "def find_jet_pull(reclustered,clustered):\n",
    "    \n",
    "    \"\"\"\n",
    "        Returns variables related to the jet pull vector\n",
    "        Reclustered : Contains the 0.2 R anti-kT b-tagged subjets\n",
    "        Clustered   : Contains the 0.8 R anti-kT 'higgs' jet (which includes the 0.2R subjets)\n",
    "    \"\"\"\n",
    "    \n",
    "    def y(p):\n",
    "        \"\"\" Returns the rapidity\"\"\"\n",
    "        return ((1/2)*math.log((p.e+p.pz)/(p.e-p.pz)))\n",
    "    \n",
    "    new_var = []\n",
    "    for sqn in range(len(reclustered)):\n",
    "        con_1 = reclustered[sqn][0].constituents()\n",
    "        J1 = reclustered[sqn][0]\n",
    "        try:\n",
    "            con_2 = reclustered[sqn][1].constituents()\n",
    "            J2 = reclustered[sqn][1]\n",
    "        except:\n",
    "            new_var.append([0,0,0])\n",
    "            continue\n",
    "        \n",
    "        r_phi_1  = np.array([dphi(x.phi,J1.phi) for x in con_1])  \n",
    "        r_y_1    = np.array([y(x)-y(J1) for x in con_1])\n",
    "        r_norm_1 = (r_phi_1**2+r_y_1**2)**(1/2)\n",
    "        pt_1     = np.array([x.pt for x in con_1])\n",
    "        \n",
    "        \"\"\"This is the pull vector of the second b-jet acting on the first b-jet (sorted by pT)\"\"\"\n",
    "        pull_1  = np.array([np.sum([pt_1*r_y_1*r_norm_1]),\n",
    "                            np.sum([pt_1*r_phi_1*r_norm_1])])/J1.pt\n",
    "        pull_1_norm = (pull_1[0]**2+pull_1[1]**2)**(1/2)\n",
    "        \n",
    "        r_phi_2  = np.array([dphi(x.phi,J2.phi) for x in con_2])  \n",
    "        r_y_2    = np.array([y(x)-y(J2) for x in con_2])\n",
    "        r_norm_2 = (r_phi_2**2+r_y_2**2)**(1/2)\n",
    "        pt_2     = np.array([x.pt for x in con_2])\n",
    "        \n",
    "        \"\"\"Similarly the pull vector of the first on the second\"\"\"\n",
    "        pull_2  = np.array([np.sum([pt_2*r_y_2*r_norm_2]),\n",
    "                            np.sum([pt_2*r_phi_2*r_norm_2])])/J2.pt\n",
    "        pull_2_norm = (pull_2[0]**2+pull_2[1]**2)**(1/2)\n",
    "        \n",
    "        \"\"\"The vector from one b-subjet to another\"\"\"\n",
    "        J1_J2 = [y(J2)-y(J1),dphi(J2.phi,J1.phi)]\n",
    "        J2_J1 = [y(J1)-y(J2),dphi(J1.phi,J2.phi)]\n",
    "        J_norm = (J1_J2[0]**2+J1_J2[1]**2)**(1/2)\n",
    "        \n",
    "        \"\"\"The angles the pull vectors make with the vector from one subjet to another\"\"\"\n",
    "        A_1 = (pull_1[0]*J1_J2[0]+pull_1[1]*J1_J2[1])/(pull_1_norm*J_norm)\n",
    "        A_2 = (pull_2[0]*J2_J1[0]+pull_2[1]*J2_J1[1])/(pull_2_norm*J_norm)\n",
    "        \n",
    "        \"\"\"The angle between the pull vectors (just a test)\"\"\"\n",
    "        pA =  (pull_1[0]*pull_2[0]+pull_1[1]*pull_2[1])/(pull_1_norm*pull_2_norm)\n",
    "        new_var.append([math.acos(A_1),math.acos(A_2),math.acos(pA)])\n",
    "        \n",
    "    return new_var\n",
    "\n",
    "def find_connexion(reclustered,clustered):\n",
    "    #The idea is that given a metric between particles (e.g. the anti_kT metric),\n",
    "    #we want to find the minimum epsilon such that if each particle is connected\n",
    "    #to all other particles closer than epsilon away, then the resultant graph\n",
    "    #is fully connected\n",
    "    new_var = []\n",
    "    class node():\n",
    "        def add_connection(self,node):\n",
    "            self.connected.append(node)\n",
    "        def __init__(self,pt,eta,phi,e,pz,index):\n",
    "            self.connected = []\n",
    "            self.pt = pt\n",
    "            self.eta = eta\n",
    "            self.phi = phi\n",
    "            self.e = e\n",
    "            self.pz = pz\n",
    "            self.to_look = 0\n",
    "            self.index = index\n",
    "        def distance(self,node):\n",
    "            #return R_y(self,node)**2\n",
    "            return max(self.pt**(-2),node.pt**(-2))*R_y(self,node)**2 #The R^2 factor don't matter\n",
    "    for x in range(len(clustered)):\n",
    "        parts = clustered[x][0].constituents()\n",
    "        node_list = []\n",
    "        for y in range(len(parts)):\n",
    "            node_list.append(node(parts[y].pt,parts[y].eta,parts[y].phi,parts[y].e,parts[y].pz,y))\n",
    "        alive_nodes = [node_list[0]]\n",
    "        seen_nodes = [node_list[0]]\n",
    "        epsilon = 0\n",
    "        ghost_edges = []\n",
    "        while True:\n",
    "            #time.sleep(1)\n",
    "            #print(ghost_edges)\n",
    "            while len(alive_nodes) > 0:\n",
    "                while alive_nodes[0].to_look < len(node_list):\n",
    "                    i_temp = alive_nodes[0].index\n",
    "                    i_look = alive_nodes[0].to_look\n",
    "                    if i_look == i_temp:\n",
    "                        alive_nodes[0].to_look = i_look + 1\n",
    "                        continue\n",
    "                    d_temp = alive_nodes[0].distance(node_list[i_look])\n",
    "                    if d_temp < epsilon:\n",
    "                        if node_list[i_temp] not in seen_nodes:\n",
    "                            alive_nodes.append(node_list[i_temp])\n",
    "                            seen_nodes.append(node_list[i_temp])\n",
    "                    else:\n",
    "                        where_in = 0\n",
    "                        for k in range(len(ghost_edges)):\n",
    "                            if d_temp > ghost_edges[k][2]:\n",
    "                                where_in = where_in + 1\n",
    "                            else:\n",
    "                                break\n",
    "                        ghost_edges.insert(where_in,[i_temp,i_look,d_temp])\n",
    "                    alive_nodes[0].to_look = i_look + 1\n",
    "                alive_nodes.pop(0)\n",
    "            if len(seen_nodes) == len(node_list):\n",
    "                break\n",
    "            while True:\n",
    "                new_edge = ghost_edges.pop(0)\n",
    "                epsilon = new_edge[2]\n",
    "                if node_list[new_edge[1]] not in seen_nodes:\n",
    "                    alive_nodes.append(node_list[i_temp])\n",
    "                    seen_nodes.append(node_list[i_temp])\n",
    "                    break\n",
    "            if len(seen_nodes) == len(node_list):\n",
    "                break\n",
    "        new_var.append(epsilon)\n",
    "    return(new_var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     38,
     49,
     89,
     103,
     117
    ]
   },
   "outputs": [],
   "source": [
    "def generate_real_SIC(expect,predict,masses,quality=1,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten(),masses)).transpose(), key=lambda x: x[1])),0)\n",
    "    background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "    signal_weights = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "    efficiency = []; signal_eff = [];\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort),quality*10):\n",
    "        background_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==0],bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(to_sort[:i+1,2][to_sort[:i+1,0]==1],bins=np.arange(50,197,7))\n",
    "        log_likelihood = 0; baseLL = 0; \n",
    "        signal_eff.append(np.sum(to_sort[:i+1,0])/total_signal)\n",
    "        test_LL_vs_SS = []\n",
    "        for signal_strength in np.arange(1,5,quality/1000):\n",
    "            log_likelihood = 0;\n",
    "            for k in range(len(bins)-1):\n",
    "                expected = background_weights*background_mass_binned[k]+signal_weights*signal_strength*signal_mass_binned[k]\n",
    "                observed = background_weights*background_mass_binned[k]+signal_weights*signal_mass_binned[k]\n",
    "                if (expected <= 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "            if signal_strength == 1:\n",
    "                baseLL = log_likelihood\n",
    "            test_LL_vs_SS.append(log_likelihood)\n",
    "            if log_likelihood < baseLL-1/2:\n",
    "                efficiency.append(1/(signal_strength-1))\n",
    "                break\n",
    "            if signal_strength > 3:\n",
    "                efficiency.append(1/2)\n",
    "                break\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"base efficiency : \" + str(float(efficiency[-1])))\n",
    "    efficiency = np.array(efficiency)/float(efficiency[-1])\n",
    "    max_eff = np.max(efficiency)\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def find_highest_SIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)): # generate a int range scanning over 95% of all samples??\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    return(max_eff)\n",
    "\n",
    "def find_highest_SIC_binned(expect,predict,masses):\n",
    "    bins = np.arange(50,197,7)\n",
    "    efficiency = []\n",
    "    def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "        log_likelihood = 0\n",
    "        background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "        signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "        background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "        signal_weights = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "        for i in range(len(bins)-1):\n",
    "            expected = background_weights*background_mass_binned[i]+signal_weights*signal_strength*signal_mass_binned[i]\n",
    "            observed = background_weights*background_mass_binned[i]+signal_weights*signal_mass_binned[i]\n",
    "            if (expected <= 0):\n",
    "                return float(\"inf\")\n",
    "            log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "        return -log_likelihood\n",
    "    \n",
    "    sigmas = []\n",
    "    for i in np.arange(0,0.9,0.05):\n",
    "        #res = least_squares(lambda x : log_like(x,masses[np.logical_and(predict.flatten() >= i,expect == 0)],\n",
    "        #                                        masses[np.logical_and(predict.flatten() >= i,expect == 1)]),x0=1)\n",
    "\n",
    "        #i is the cut on the machine learning\n",
    "        kept_back = masses[np.logical_and(predict.flatten() >= i,expect == 0)]\n",
    "        kept_signal = masses[np.logical_and(predict.flatten() >= i,expect == 1)]\n",
    "        j_array = []\n",
    "        for j in np.arange(1,25,0.5):\n",
    "            #print log_like(j,kept_back,kept_signal)\n",
    "            if log_like(j,kept_back,kept_signal) > log_like(1,kept_back,kept_signal)+0.5:\n",
    "                j_array.append(j-1)\n",
    "                break\n",
    "            if j >20:\n",
    "                j_array.append(20)\n",
    "                break\n",
    "        print(j_array,i)\n",
    "        sigmas.append(1/np.min(j_array))\n",
    "        \n",
    "    max_eff = np.max(sigmas)\n",
    "    return(max_eff)\n",
    "\n",
    "def generateSIC(expect,predict,quality=100,verbose=False):\n",
    "    to_sort = np.flip(np.array(sorted(np.vstack((expect,predict.flatten())).transpose(), key=lambda x: x[1])),0)\n",
    "    total_signal = np.sum(to_sort[:,0])\n",
    "    efficiency = []; signal_eff = []\n",
    "    for i in range(int(0.05*len(to_sort)),len(to_sort)):\n",
    "        signal_eff_temp = np.sum(to_sort[:i+1,0])/total_signal\n",
    "        background_eff_temp = (i+1-np.sum(to_sort[:i+1,0]))/(len(to_sort)-total_signal)\n",
    "        signal_eff.append(signal_eff_temp)\n",
    "        efficiency.append((signal_eff_temp)/((background_eff_temp)**(1/2)))\n",
    "    max_eff = np.max(efficiency)\n",
    "    max_cut = to_sort[efficiency.index(max_eff),1]\n",
    "    print(\"Max SI of \" + str(max_eff) + \" at cut \" + str(max_cut))\n",
    "    return(efficiency,signal_eff)\n",
    "\n",
    "def log_like(signal_strength,background_mass_list,signal_mass_list):\n",
    "    log_likelihood = 0\n",
    "    background_mass_binned,bins = np.histogram(background_mass_list,bins=np.arange(50,197,7))\n",
    "    signal_mass_binned,bins = np.histogram(signal_mass_list,bins=np.arange(50,197,7))\n",
    "    background_weights = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "    signal_weights = 3.15*actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weights*background_mass_binned[i]+signal_weights*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weights*background_mass_binned[i]+signal_weights*signal_mass_binned[i]\n",
    "        if (expected <= 0):\n",
    "            return float(\"inf\")\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return -log_likelihood\n",
    "\n",
    "def log_like(signal_strength):\n",
    "    log_likelihood = 0\n",
    "    for i in range(len(bins)-1):\n",
    "        expected = background_weights*background_mass_binned[i]+signal_weights*signal_strength*signal_mass_binned[i]\n",
    "        observed = background_weights*background_mass_binned[i]+signal_weights*signal_mass_binned[i]\n",
    "        #print(expected)\n",
    "        log_likelihood = log_likelihood + observed*math.log(expected) - expected\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONV NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     2,
     7,
     10,
     24
    ]
   },
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR ML\n",
    "\n",
    "#FRANK# returns a function that generates a padding layer\n",
    "#FRANK# x is the detector image, of following format:\n",
    "#FRANK# [index, image#(charged and neutral pt and multiplicity), 40, 40]\n",
    "def return_pad_me(padding):\n",
    "    def pad_me(x):\n",
    "        #FRANK# x[:,:,:y,:] slice x off from y at the given axis.\n",
    "        return(tf.concat((x,x[:,:,:padding,:]),2))\n",
    "    return(pad_me)\n",
    "\n",
    "def pad_out(padding,input_shape):\n",
    "    return input_shape\n",
    "\n",
    "class gen_call(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, test_data):\n",
    "        self.x, self.y = test_data\n",
    "    \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.highest_SIC_train = []\n",
    "        self.highest_SIC_test = []\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        self.highest_SIC_test.append(find_highest_SIC(self.y,y_pred))\n",
    "        print(str(self.highest_SIC_test[-1]) + \" is how good\")\n",
    "\n",
    "def show_outputs(output):\n",
    "    #Assumes the output is in shape like (32,41,36)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for i in range(1,1+output.shape[0]):\n",
    "        fig.add_subplot(4,output.shape[0]/4,i)\n",
    "        plt.imshow(10*output[i-1,:,:])\n",
    "        plt.axis('off')\n",
    "    #plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AggregationMethod',\n",
       " 'Assert',\n",
       " 'CriticalSection',\n",
       " 'DType',\n",
       " 'DeviceSpec',\n",
       " 'GradientTape',\n",
       " 'Graph',\n",
       " 'IndexedSlices',\n",
       " 'IndexedSlicesSpec',\n",
       " 'Module',\n",
       " 'Operation',\n",
       " 'OptionalSpec',\n",
       " 'RaggedTensor',\n",
       " 'RaggedTensorSpec',\n",
       " 'RegisterGradient',\n",
       " 'SparseTensor',\n",
       " 'SparseTensorSpec',\n",
       " 'Tensor',\n",
       " 'TensorArray',\n",
       " 'TensorArraySpec',\n",
       " 'TensorShape',\n",
       " 'TensorSpec',\n",
       " 'TypeSpec',\n",
       " 'UnconnectedGradients',\n",
       " 'Variable',\n",
       " 'VariableAggregation',\n",
       " 'VariableSynchronization',\n",
       " '_LazyLoader',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__compiler_version__',\n",
       " '__cxx11_abi_flag__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__git_version__',\n",
       " '__loader__',\n",
       " '__monolithic_build__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_absolute_import',\n",
       " '_api',\n",
       " '_division',\n",
       " '_forward_module',\n",
       " '_importlib',\n",
       " '_m',\n",
       " '_print_function',\n",
       " '_root_estimator',\n",
       " '_sys',\n",
       " '_top_level_modules',\n",
       " '_types',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add',\n",
       " 'add_n',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_dtype',\n",
       " 'as_string',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_equal',\n",
       " 'assert_greater',\n",
       " 'assert_less',\n",
       " 'assert_rank',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'audio',\n",
       " 'autograph',\n",
       " 'batch_to_space',\n",
       " 'bfloat16',\n",
       " 'bitcast',\n",
       " 'bitwise',\n",
       " 'bool',\n",
       " 'boolean_mask',\n",
       " 'broadcast_dynamic_shape',\n",
       " 'broadcast_static_shape',\n",
       " 'broadcast_to',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'clip_by_global_norm',\n",
       " 'clip_by_norm',\n",
       " 'clip_by_value',\n",
       " 'compat',\n",
       " 'complex',\n",
       " 'complex128',\n",
       " 'complex64',\n",
       " 'concat',\n",
       " 'cond',\n",
       " 'config',\n",
       " 'constant',\n",
       " 'constant_initializer',\n",
       " 'control_dependencies',\n",
       " 'convert_to_tensor',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'cumsum',\n",
       " 'custom_gradient',\n",
       " 'data',\n",
       " 'debugging',\n",
       " 'device',\n",
       " 'distribute',\n",
       " 'divide',\n",
       " 'double',\n",
       " 'dtypes',\n",
       " 'dynamic_partition',\n",
       " 'dynamic_stitch',\n",
       " 'edit_distance',\n",
       " 'einsum',\n",
       " 'ensure_shape',\n",
       " 'equal',\n",
       " 'errors',\n",
       " 'estimator',\n",
       " 'examples',\n",
       " 'executing_eagerly',\n",
       " 'exp',\n",
       " 'expand_dims',\n",
       " 'experimental',\n",
       " 'extract_volume_patches',\n",
       " 'eye',\n",
       " 'feature_column',\n",
       " 'fill',\n",
       " 'fingerprint',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'floor',\n",
       " 'foldl',\n",
       " 'foldr',\n",
       " 'function',\n",
       " 'gather',\n",
       " 'gather_nd',\n",
       " 'get_logger',\n",
       " 'get_static_value',\n",
       " 'grad_pass_through',\n",
       " 'gradients',\n",
       " 'graph_util',\n",
       " 'greater',\n",
       " 'greater_equal',\n",
       " 'group',\n",
       " 'guarantee_const',\n",
       " 'half',\n",
       " 'hessians',\n",
       " 'histogram_fixed_width',\n",
       " 'histogram_fixed_width_bins',\n",
       " 'identity',\n",
       " 'identity_n',\n",
       " 'image',\n",
       " 'import_graph_def',\n",
       " 'init_scope',\n",
       " 'initializers',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'io',\n",
       " 'is_tensor',\n",
       " 'keras',\n",
       " 'less',\n",
       " 'less_equal',\n",
       " 'linalg',\n",
       " 'linspace',\n",
       " 'lite',\n",
       " 'load_library',\n",
       " 'load_op_library',\n",
       " 'logical_and',\n",
       " 'logical_not',\n",
       " 'logical_or',\n",
       " 'lookup',\n",
       " 'losses',\n",
       " 'make_ndarray',\n",
       " 'make_tensor_proto',\n",
       " 'map_fn',\n",
       " 'math',\n",
       " 'matmul',\n",
       " 'matrix_square_root',\n",
       " 'maximum',\n",
       " 'meshgrid',\n",
       " 'metrics',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'name_scope',\n",
       " 'negative',\n",
       " 'nest',\n",
       " 'newaxis',\n",
       " 'nn',\n",
       " 'no_gradient',\n",
       " 'no_op',\n",
       " 'nondifferentiable_batch_function',\n",
       " 'norm',\n",
       " 'not_equal',\n",
       " 'numpy_function',\n",
       " 'one_hot',\n",
       " 'ones',\n",
       " 'ones_initializer',\n",
       " 'ones_like',\n",
       " 'optimizers',\n",
       " 'pad',\n",
       " 'parallel_stack',\n",
       " 'pow',\n",
       " 'print',\n",
       " 'py_function',\n",
       " 'qint16',\n",
       " 'qint32',\n",
       " 'qint8',\n",
       " 'quantization',\n",
       " 'queue',\n",
       " 'quint16',\n",
       " 'quint8',\n",
       " 'ragged',\n",
       " 'random',\n",
       " 'random_normal_initializer',\n",
       " 'random_uniform_initializer',\n",
       " 'range',\n",
       " 'rank',\n",
       " 'raw_ops',\n",
       " 'realdiv',\n",
       " 'recompute_grad',\n",
       " 'reduce_all',\n",
       " 'reduce_any',\n",
       " 'reduce_logsumexp',\n",
       " 'reduce_max',\n",
       " 'reduce_mean',\n",
       " 'reduce_min',\n",
       " 'reduce_prod',\n",
       " 'reduce_sum',\n",
       " 'register_tensor_conversion_function',\n",
       " 'required_space_to_batch_paddings',\n",
       " 'reshape',\n",
       " 'resource',\n",
       " 'reverse',\n",
       " 'reverse_sequence',\n",
       " 'roll',\n",
       " 'round',\n",
       " 'saturate_cast',\n",
       " 'saved_model',\n",
       " 'scalar_mul',\n",
       " 'scan',\n",
       " 'scatter_nd',\n",
       " 'searchsorted',\n",
       " 'sequence_mask',\n",
       " 'sets',\n",
       " 'shape',\n",
       " 'shape_n',\n",
       " 'sigmoid',\n",
       " 'sign',\n",
       " 'signal',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'slice',\n",
       " 'sort',\n",
       " 'space_to_batch',\n",
       " 'space_to_batch_nd',\n",
       " 'sparse',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'square',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'stop_gradient',\n",
       " 'strided_slice',\n",
       " 'string',\n",
       " 'strings',\n",
       " 'subtract',\n",
       " 'summary',\n",
       " 'switch_case',\n",
       " 'sysconfig',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tensor_scatter_nd_add',\n",
       " 'tensor_scatter_nd_sub',\n",
       " 'tensor_scatter_nd_update',\n",
       " 'tensordot',\n",
       " 'test',\n",
       " 'tile',\n",
       " 'timestamp',\n",
       " 'tpu',\n",
       " 'train',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncatediv',\n",
       " 'truncatemod',\n",
       " 'tuple',\n",
       " 'uint16',\n",
       " 'uint32',\n",
       " 'uint64',\n",
       " 'uint8',\n",
       " 'unique',\n",
       " 'unique_with_counts',\n",
       " 'unravel_index',\n",
       " 'unstack',\n",
       " 'variable_creator_scope',\n",
       " 'variant',\n",
       " 'vectorized_map',\n",
       " 'version',\n",
       " 'where',\n",
       " 'while_loop',\n",
       " 'xla',\n",
       " 'zeros',\n",
       " 'zeros_initializer',\n",
       " 'zeros_like']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "#FRANK# this method is the exact replica of find_highest_SIC() but operates on tensors.\n",
    "#FRANK# for some reason, pred has different format as expect. \n",
    "print(tf.__version__)\n",
    "def highest_SIC_metric(y_true,y_pred):\n",
    "    print('highest_SIC_metric() is called')\n",
    "    y_true = tf.keras.backend.flatten(y_true) #FRANK# flattens to 1D\n",
    "    y_pred = tf.keras.backend.flatten(y_pred) #FRANK# flattens to 1D\n",
    "    \n",
    "    #stacked = tf.transpose(tf.stack((expect,predict)))\n",
    "    #to_sort = tf.reverse(sorted(stacked, key=lambda x: x[1]),0) #FRANK# sorted is wrong!!\n",
    "    total_sample = tf.cast(tf.size(y_pred), tf.float32)  # count total nums, then cast to float32 to avoid issue\n",
    "    total_signal = tf.cast(tf.reduce_sum(y_true), tf.float32)  # summing across all predicted (where sigs are 1's and bkgs are 0's) to get total num of signal. \n",
    "    total_background = tf.cast(tf.subtract(total_sample, total_signal), tf.float32)  # subtracting signal countss from total size to get background counts\n",
    "    \n",
    "    \n",
    "    # original mechanism:\n",
    "    # 1. sort by ML score\n",
    "    # 2. sum all actuals before an index\n",
    "    # 3. count all 0's before an index \n",
    "    \n",
    "    sorted_indices = tf.argsort(y_pred,axis=-1,direction='ASCENDING')\n",
    "    sorted_sigs = tf.gather(y_pred,sorted_indices)\n",
    "    # return the indices of prediction in ascending order. By reading these indices, you can access corresponding expected y's.\n",
    "    ones = tf.fill(tf.shape(sorted_sigs), 1.0)\n",
    "    sorted_bkgs = tf.subtract(ones, sorted_sigs)\n",
    "    # make a sorted tensor where 1's are bkgs and 0's are sigs\n",
    "    \n",
    "    sig_cum_sums = tf.cast(tf.cumsum(sorted_sigs), tf.float32) # return the integrated signal number from 0 to each index\n",
    "    bkg_cum_sums = tf.cast(tf.cumsum(sorted_bkgs), tf.float32) # return the integrated bkg number from 0 to each index\n",
    "\n",
    "    sig_effs = tf.divide(sig_cum_sums, total_signal)\n",
    "    bkg_effs = tf.divide(bkg_cum_sums, total_background) #FRANK# total_background might be 0, causing bkg_effs=inf\n",
    "    effs = tf.divide(sig_effs, tf.sqrt(bkg_effs))\n",
    "        \n",
    "    return tf.reduce_max(effs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_81 (Lambda)           (None, 3, 44, 40)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 32, 40, 36)        2432      \n",
      "_________________________________________________________________\n",
      "lambda_82 (Lambda)           (None, 32, 41, 36)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 32, 20, 18)        0         \n",
      "_________________________________________________________________\n",
      "lambda_83 (Lambda)           (None, 32, 24, 18)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 64, 20, 14)        51264     \n",
      "_________________________________________________________________\n",
      "lambda_84 (Lambda)           (None, 64, 21, 14)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 64, 10, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1000)              4481000   \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 4,535,697\n",
      "Trainable params: 4,535,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "highest_SIC_metric() is called\n"
     ]
    }
   ],
   "source": [
    "#FINAL - This is the convolutional net for the event image\n",
    "model = Sequential()\n",
    "model.add(Lambda(return_pad_me(4),\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first'))\n",
    "#model_nopad.add(keras.layers.LeakyReLU(alpha=0.1))\n",
    "model.add(Lambda(return_pad_me(1),\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "model.add(Lambda(return_pad_me(4),\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (5, 5), \n",
    "                 activation='relu',\n",
    "                 data_format='channels_first'))\n",
    "#model_nopad.add(keras.layers.LeakyReLU(alpha=0.1))\n",
    "model.add(Lambda(return_pad_me(1),\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "#model_nopad.add(keras.layers.LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#model_opt = keras.optimizers.Adadelta(lr=2.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model_opt = keras.optimizers.Adadelta() #FRANK# decrease learning rate\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=model_opt,\n",
    "              metrics=['accuracy', highest_SIC_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-deafa8713c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#FRANK# testing metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m history_event_only = model.fit(x_train, y_train,\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "#FRANK# testing metric\n",
    "epochs = 1000\n",
    "print(y_train)\n",
    "history_event_only = model.fit(x_train, y_train,\n",
    "        batch_size=50,#batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val,y_val),\n",
    "        verbose=1, shuffle=True, callbacks=[EarlyStopping(monitor='highest_SIC_test', patience=2), gen_call((x_val_cut,y_val_cut))])\n",
    "#FRANK# for event image SIC: predicting\n",
    "#y_pred = model.predict(x_test)\n",
    "#print(y_pred)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5752981  0.5256461  0.55304366 ... 0.52291924 0.5759589  0.51929736]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "highest_SIC_metric() is called\n",
      "Tensor(\"truediv_2:0\", shape=(23051,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#FRANK# testing metric\n",
    "print(np.array(y_pred.flatten()))\n",
    "print(np.array(y_test))\n",
    "print(highest_SIC_metric(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_event_only' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-c5b5a661b6e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#FRANK# training plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_event_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_event_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_event_only' is not defined"
     ]
    }
   ],
   "source": [
    "#FRANK# training plots\n",
    "\n",
    "print(history_event_only.history.keys())\n",
    "plt.plot(history_event_only.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#FINAL this is the model for the jet image\n",
    "model_fine = Sequential()\n",
    "model_fine.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first',input_shape=input_shape))\n",
    "#model_fine.add(Dropout(0.5))\n",
    "model_fine.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),data_format='channels_first'))\n",
    "model_fine.add(Conv2D(64, (5, 5), activation='relu',data_format='channels_first'))\n",
    "#model_fine.add(Dropout(0.5))\n",
    "model_fine.add(MaxPooling2D(pool_size=(2, 2),data_format='channels_first'))\n",
    "model_fine.add(Flatten())\n",
    "model_fine.add(Dense(1000, activation='relu'))\n",
    "#model_fine.add(Dropout(0.2))\n",
    "model_fine.add(Dense(1, activation='sigmoid'))\n",
    "#model_fine.summary()\n",
    "\n",
    "model_opt = keras.optimizers.Adadelta()\n",
    "\n",
    "model_fine.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=model_opt,\n",
    "              metrics=['accuracy', 'highest_SIC_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "model_fine.fit(x_train_r,y_train_r,\n",
    "               batch_size = batch_size,\n",
    "               epochs = epochs,\n",
    "               verbose = 1, \n",
    "               shuffle = True, \n",
    "               validation_data=(x_val_r,y_val_r),\n",
    "               callbacks=[EarlyStopping(monitor='highest_SIC_test', patience=2), gen_call((x_val_cut_r,y_val_cut_r))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT NOTE DON'T DELETE\n",
    "#The 1e-3 factor in my weights comes from the fact that Pythia reports cross-sections in millibarns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# calculating true positive rates and false positive rates. \n",
    "#FRANK# note that bkgds have lower beta in general, so the cut should be \n",
    "#FRANK# beta3 > threshold.\n",
    "#FRANK# 'where' usage referred from \n",
    "#FRANK# https://stackoverflow.com/questions/12995937/count-all-values-in-a-matrix-greater-than-a-value\n",
    "def simple_ROC(sig, bkgd, search_range=(0,70), step = 1):\n",
    "    beta3_min = min([min(sig),min(bkgd)])\n",
    "    beta3_max = max([max(sig),max(bkgd)])\n",
    "    search_range=(beta3_min, beta3_max)\n",
    "    sig_count = len(sig)\n",
    "    bkgd_count = len(bkgd)\n",
    "    sig_rates = []\n",
    "    bkgd_rates = []\n",
    "    thres_list = np.arange(search_range[0], search_range[1], step)\n",
    "    for thres in thres_list:\n",
    "        sig_selected = np.greater(sig, thres)\n",
    "        bkgd_selected = np.greater(bkgd, thres)\n",
    "        sig_rates.append(np.sum(sig_selected)/sig_count)\n",
    "        bkgd_rates.append(np.sum(bkgd_selected)/bkgd_count)\n",
    "    return sig_rates, bkgd_rates, thres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# visualizing event image model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# splitting predictions for events and backgrounds\n",
    "#FRANK# the split location is \n",
    "split_location =int(np.argwhere(y_test==1.)[0])\n",
    "#FRANK# signal predictions with event image model\n",
    "sig_event_image = predicted[split_location:]\n",
    "bkgd_event_image = predicted[:split_location]\n",
    "\n",
    "print(sig_event_image)\n",
    "print(bkgd_event_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# ROC curve of event images\n",
    "event_sig_rates, event_bkgd_rates, event_thres_list = simple_ROC(sig = sig_event_image, bkgd = bkgd_event_image, step=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# plot roc and sic for event image model\n",
    "\n",
    "#plt.plot(thres_list,sig_rates,label='sig',linewidth=1)\n",
    "#plt.plot(thres_list,bkgd_rates,label='bkgd',linewidth=1)\n",
    "plt.rcParams['figure.figsize'] = [6, 6]\n",
    "plt.plot(event_sig_rates, event_bkgd_rates,label='bkgd',linewidth=1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"signal rate\")\n",
    "plt.ylabel(\"background rate\")\n",
    "plt.show()\n",
    "\n",
    "#FRANK# e/eb \n",
    "event_significances = event_sig_rates/np.sqrt(event_bkgd_rates)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 4]\n",
    "plt.plot(event_sig_rates,event_significances,label='beta3',linewidth=1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"signal rate\")\n",
    "plt.ylabel(\"significance improvement\")\n",
    "plt.ylim(top=2.5)\n",
    "plt.ylim(bottom=1)\n",
    "plt.xlim(right=1)  # adjust the right leaving left unchanged\n",
    "plt.xlim(left=0.1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here evaluate ML performance by e.g.\n",
    "#r_combine_y,r_combine_x = generate_real_SIC(y_test,new_model_combine.predict([x_test,x_test_r]),mass_test,quality=1)\n",
    "\n",
    "\n",
    "                          \n",
    "\n",
    "r_full_y,r_full_x = generateSIC(y_test,predicted, quality=100)\n",
    "#r_fine_y,r_fine_x = generate_real_SIC(y_test,model_fine.predict(x_test_r),mass_test,quality=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print groomed mass histogram\n",
    "plt.rcParams['figure.figsize'] = [7.5,6]\n",
    "background_weight = actual_background_cross*35.9*1e15/(average_number_accepted*num_background_files)\n",
    "signal_weight = actual_signal_cross*35.9*1e15/(signal_accepted*num_signal_files)\n",
    "background_weights = np.full(21, background_weight, dtype=None, order='C')\n",
    "signal_weights = np.full(21, signal_weight, dtype=None, order='C')\n",
    "\n",
    "hist_bkg, bin_edges_bkg = np.histogram(background_mass_list, bins=21, range = (50, 197))\n",
    "bin_centers_bkg = (bin_edges_bkg[:-1] + bin_edges_bkg[1:]) / 2\n",
    "hist_sig, bin_edges_bkg = np.histogram(signal_mass_list, bins=21, range = (50, 197))\n",
    "\n",
    "plt.step(bin_centers_bkg, hist_bkg*background_weight, label='bkgd')\n",
    "plt.step(bin_centers_bkg, hist_sig*signal_weight, label='sig')\n",
    "plt.legend()\n",
    "plt.ylim(top=10000)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(right=197)  # adjust the right leaving left unchanged\n",
    "plt.xlim(left=50) \n",
    "plt.show()\n",
    "#log_like(1,background_mass_list=background_mass_list,signal_mass_list=signal_mass_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# calculating beta3\n",
    "\n",
    "background_beta3 = find_new_var_beta_3(background_reclustered,background_event_list_clustered,pt_cut = 1)\n",
    "signal_beta3 = find_new_var_beta_3(signal_reclustered,signal_event_list_clustered,pt_cut = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# beta-3 histogram\n",
    "plt.rcParams['figure.figsize'] = [5, 4]\n",
    "beta3_min = min([min(background_beta3),min(signal_beta3)])\n",
    "beta3_max = max([max(background_beta3),max(signal_beta3)])\n",
    "print('beta3 range is '+str((beta3_min, beta3_max)))\n",
    "plt.hist(background_beta3, label='bkgd', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_beta3, label='sig', bins = 40, range = (beta3_min, 70), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Beta-3\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(background_mass_list, label='bkgd', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.hist(signal_mass_list, label='sig', bins = 21, range = (50, 197), stacked=True, density=True,histtype ='step')\n",
    "plt.xlabel(\"Trimmed Mass\")\n",
    "plt.ylabel(\"Normalized Shape\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# calculating significances wrt beta3 cuts\n",
    "sig_rates, bkgd_rates, thres_list = simple_ROC(sig = signal_beta3, bkgd = background_beta3, step=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(thres_list,sig_rates,label='sig',linewidth=1)\n",
    "#plt.plot(thres_list,bkgd_rates,label='bkgd',linewidth=1)\n",
    "plt.rcParams['figure.figsize'] = [6, 6]\n",
    "plt.plot(sig_rates, bkgd_rates,label='bkgd',linewidth=1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"signal rate\")\n",
    "plt.ylabel(\"background rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRANK# e/eb \n",
    "significances = sig_rates/np.sqrt(bkgd_rates)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 4]\n",
    "plt.plot(sig_rates,significances,label='beta3',linewidth=1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"signal rate\")\n",
    "plt.ylabel(\"significance improvement\")\n",
    "plt.ylim(top=2.5)\n",
    "plt.ylim(bottom=1)\n",
    "plt.xlim(right=1)  # adjust the right leaving left unchanged\n",
    "plt.xlim(left=0.1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.title(\"Binned Likelihood Significance Improvements for various Architectures\")\n",
    "#plt.plot(r_combine_x,r_combine_y,color=\"red\",label=\"Full CNN Architecture\",linewidth=1)\n",
    "#plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "#plt.plot(r_fine_x,r_fine_y,color=\"red\",alpha=0.6,label=\"Jet image only\",linewidth=1,linestyle=\"dotted\")\n",
    "#FRANK# event image SIC\n",
    "plt.plot(r_full_x,r_full_y,color=\"red\",alpha=0.6,label=\"Event image only\",linewidth=1,linestyle=\"dashed\")\n",
    "plt.plot(sig_rates,significances,label='Beta3',linewidth=1)\n",
    "#FRANK# beta-3 SIC\n",
    "#plt.plot(beta_x,beta_y,color=\"blue\",alpha=0.6,label=r\"$\\beta_3$\",linewidth=1)#,linestyle=\"dashed\")\n",
    "#plt.plot(three_x,three_y,color=\"blue\",alpha=0.6,label=r\"$Rb_2$\",linewidth=1,linestyle=\"dotted\")\n",
    "#plt.plot(x_02,y_02,color=\"gray\",label=\"Jet image, no neutral layer\",linewidth=1)\n",
    "#plt.plot(vars_y,vars_x,color=\"blue\",label=r\"$\\beta_3 + Rb_2$\",linewidth=1)\n",
    "plt.xlim(0.1,1)\n",
    "plt.ylim(1,2.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.savefig('all_SIC.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Total model, with the two different branches\n",
    "#FRANK# input shape is calculated during splitting:\n",
    "#FRANK# splitData(background_image_list,signal_image_list)\n",
    "#FRANK# how is jet image taken?\n",
    "new_branch_total = Sequential()\n",
    "\n",
    "#FRANK# Lambda is from Keras.\n",
    "new_branch_total.add(Lambda(return_pad_me(5), #FRANK# tf.concat((x,x[:,:,:5,:]),2). Concatenate means pasting together.\n",
    "                            #FRANK# this layer slices off an edge of the tensor and paste it on the other side.\n",
    "                 input_shape=input_shape))\n",
    "new_branch_total.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                data_format='channels_first'))\n",
    "new_branch_total.add(Lambda(return_pad_me(2)))\n",
    "new_branch_total.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "new_branch_total.add(Lambda(return_pad_me(5),\n",
    "                 input_shape=input_shape))\n",
    "new_branch_total.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#new_branch_total.add(Dropout(0.5))\n",
    "new_branch_total.add(Lambda(return_pad_me(2)))\n",
    "new_branch_total.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_branch_total.add(Flatten())\n",
    "new_branch_total.add(Dense(300,activation='relu'))\n",
    "#new_branch_total.add(Dropout(0.5))\n",
    "\n",
    "new_branch_jet = Sequential()\n",
    "new_branch_jet.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                      kernel_initializer='random_uniform',input_shape=input_shape_r, data_format=\"channels_first\")) #FRANK# problemmatic. see debug note #1\n",
    "#new_branch_jet.add(Dropout(0.5))\n",
    "new_branch_jet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "new_branch_jet.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#new_branch_jet.add(Dropout(0.5))\n",
    "new_branch_jet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_branch_jet.add(Flatten())\n",
    "new_branch_jet.add(Dense(300, activation='relu'))\n",
    "#new_branch_jet.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "#FRANK# Merge layer is no longer supported. Another \n",
    "#new_model_combine = Sequential()\n",
    "#new_model_combine.add(Merge([new_branch_total, new_branch_jet], mode = 'concat'))\n",
    "#new_model_combine.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#FRANK# these are not models, but tensors. Model.input or Model.output are tensor pointers I think.\n",
    "new_model_combine_tensor = Concatenate(axis=-1)([new_branch_total.output, new_branch_jet.output])   \n",
    "new_model_combine_tensor = Dense(1,activation='sigmoid')(new_model_combine_tensor)\n",
    "#FRANK# what you need to do is merging output of 300 and 300 into one.\n",
    "\n",
    "#new_model_combine.summary()\n",
    "new_model_combine = Model([new_branch_total.input,new_branch_jet.input], new_model_combine_tensor)\n",
    "new_model_combine.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', highest_SIC_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model_combine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-62c9f1ec52a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m new_model_combine.fit([x_train,x_train_r], y_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model_combine' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "new_model_combine.fit([x_train,x_train_r], y_train,\n",
    "          batch_size=50, #batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1, shuffle=True, validation_data=([x_val,x_val_r],y_val),\n",
    "                  callbacks=[EarlyStopping(monitor='highest_SIC_metric', patience=2), gen_call(([x_val_cut,x_val_cut_r],y_val_cut_r))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Keras",
   "language": "python",
   "name": "python_3_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
